{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformer_lens, sae_lens\n",
    "from transformer_lens import HookedTransformer\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def sparsegpt(W, X, λ=0.01, p=0.5, B=128, Bs=32, device='cuda'):\n",
    "    '''\n",
    "    W: weight matrix\n",
    "    X: input matrix\n",
    "    λ: regularization parameter\n",
    "    p: sparsity parameter\n",
    "    B: batch size\n",
    "    Bs: block size\n",
    "    '''\n",
    "\n",
    "    # print(W.shape, X.shape)\n",
    "    # Initialize mask and block quantization errors\n",
    "    M = torch.ones_like(W) # Binary pruning mask\n",
    "    E = torch.zeros_like(W) # Block quantization errors\n",
    "\n",
    "    # Compute Hessian inverse\n",
    "    H = X.T @ X + λ * torch.eye(X.shape[1], device=device)\n",
    "    H_inv = torch.cholesky(torch.inverse(H)).T\n",
    "\n",
    "    # Blockwise pruning\n",
    "    for i in range(0, W.shape[1], B):\n",
    "        for j in range(i, min(i + B, W.shape[1])):\n",
    "            if j % Bs == 0:\n",
    "\n",
    "                # Select (1-p) fraction of weights based on Hessian information\n",
    "                W_squared = W[:, j:j + Bs] ** 2\n",
    "                H_diag = torch.diag(H_inv)[j : j+Bs].unsqueeze(0)\n",
    "                score = W_squared / H_diag ** 2\n",
    "                thr = torch.quantile(score, p) # Pruning threshold\n",
    "                M[:, j:j + Bs] = (score > thr).float() # Binary mask\n",
    "\n",
    "            # Prune and update quantization error\n",
    "            E[:, j] += (1 - M[:, j]) * W[:, j]\n",
    "            W[:, j] = M[:, j] * W[:, j] # Apply mask\n",
    "\n",
    "            # Update weights using Hessian inverse\n",
    "            W[:, i:(i + B)] -= E[:, j].unsqueeze(1) * H_inv[j, i:(i + B)]\n",
    "\n",
    "    return W * M\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def prune_sgpt(model, tokens):\n",
    "    wts_act = {\n",
    "    'attn.W_Q': 'attn.hook_q',\n",
    "    'attn.W_K': 'attn.hook_k',\n",
    "    'attn.W_V': 'attn.hook_v',\n",
    "    'attn.W_O': 'hook_attn_out',\n",
    "    'mlp.W_in': 'mlp.hook_pre',\n",
    "    'mlp.W_out': 'hook_mlp_out'\n",
    "    }\n",
    "    for layer in range(model.cfg.n_layers):\n",
    "        logits, cache = model.run_with_cache(tokens, remove_batch_dim=True)\n",
    "        for wt, act in wts_act.items():\n",
    "            W = model.get_parameter(f'blocks.{layer}.{wt}')\n",
    "            X = cache[f'blocks.{layer}.{act}']\n",
    "\n",
    "            if W.dim() == 3:\n",
    "                if 'W_O' in wt:\n",
    "                    X_norm = X\n",
    "                    for head in range(W.shape[0]):\n",
    "                        W[head] = sparsegpt(W[head], X_norm)\n",
    "                        \n",
    "                else:\n",
    "                    for head in range(W.shape[0]):\n",
    "                        X_norm = X[:, head, :]\n",
    "                        W[head] = sparsegpt(W[head], X_norm)\n",
    "            else:\n",
    "                X_norm = X\n",
    "                W = sparsegpt(W, X_norm)\n",
    "            \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "gpt2: sae_lens.HookedSAETransformer = sae_lens.HookedSAETransformer.from_pretrained(\"gpt2-small\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized prompt: ['<|endoftext|>', 'Mit', 'igating', ' the', ' risk', ' of', ' extinction', ' from', ' AI', ' should', ' be', ' a', ' global']\n",
      "Tokenized answer: [' priority']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">        Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">19.46</span><span style=\"font-weight: bold\"> Prob: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">52.99</span><span style=\"font-weight: bold\">% Token: | priority|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m0\u001b[0m\u001b[1m        Logit: \u001b[0m\u001b[1;36m19.46\u001b[0m\u001b[1m Prob: \u001b[0m\u001b[1;36m52.99\u001b[0m\u001b[1m% Token: | priority|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 19.46 Prob: 52.99% Token: | priority|\n",
      "Top 1th token. Logit: 17.44 Prob:  7.02% Token: | effort|\n",
      "Top 2th token. Logit: 16.94 Prob:  4.26% Token: | issue|\n",
      "Top 3th token. Logit: 16.63 Prob:  3.14% Token: | challenge|\n",
      "Top 4th token. Logit: 16.37 Prob:  2.42% Token: | goal|\n",
      "Top 5th token. Logit: 16.06 Prob:  1.78% Token: | concern|\n",
      "Top 6th token. Logit: 15.88 Prob:  1.47% Token: | focus|\n",
      "Top 7th token. Logit: 15.61 Prob:  1.13% Token: | approach|\n",
      "Top 8th token. Logit: 15.53 Prob:  1.04% Token: | policy|\n",
      "Top 9th token. Logit: 15.42 Prob:  0.93% Token: | initiative|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">' priority'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">)]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m' priority'\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"Mitigating the risk of extinction from AI should be a global\"\n",
    "answer = \" priority\"\n",
    "\n",
    "# First see how the model does without SAEs\n",
    "transformer_lens.utils.test_prompt(prompt, answer, gpt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = transformer_lens.utils.get_dataset('openwebtext')\n",
    "\n",
    "class OpenWebText(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset, max_length=1024):\n",
    "        self.dataset = dataset\n",
    "        self.max_length = 1024\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.dataset[idx]['text']\n",
    "        tokens = gpt2.to_tokens(text)\n",
    "        tokens = tokens[:self.max_length]\n",
    "        return tokens\n",
    "    \n",
    "openwebtext = OpenWebText(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([64, 768]) torch.Size([1024, 768])\n",
      "torch.Size([64, 768]) torch.Size([1024, 768])\n",
      "torch.Size([64, 768]) torch.Size([1024, 768])\n",
      "torch.Size([64, 768]) torch.Size([1024, 768])\n",
      "torch.Size([64, 768]) torch.Size([1024, 768])\n",
      "torch.Size([64, 768]) torch.Size([1024, 768])\n",
      "torch.Size([64, 768]) torch.Size([1024, 768])\n",
      "torch.Size([64, 768]) torch.Size([1024, 768])\n",
      "torch.Size([64, 768]) torch.Size([1024, 768])\n",
      "torch.Size([64, 768]) torch.Size([1024, 768])\n",
      "torch.Size([64, 768]) torch.Size([1024, 768])\n",
      "torch.Size([64, 768]) torch.Size([1024, 768])\n",
      "torch.Size([768, 3072]) torch.Size([1024, 3072])\n",
      "torch.Size([3072, 768]) torch.Size([1024, 768])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([64, 768]) torch.Size([1024, 768])\n",
      "torch.Size([64, 768]) torch.Size([1024, 768])\n",
      "torch.Size([64, 768]) torch.Size([1024, 768])\n",
      "torch.Size([64, 768]) torch.Size([1024, 768])\n",
      "torch.Size([64, 768]) torch.Size([1024, 768])\n",
      "torch.Size([64, 768]) torch.Size([1024, 768])\n",
      "torch.Size([64, 768]) torch.Size([1024, 768])\n",
      "torch.Size([64, 768]) torch.Size([1024, 768])\n",
      "torch.Size([64, 768]) torch.Size([1024, 768])\n",
      "torch.Size([64, 768]) torch.Size([1024, 768])\n",
      "torch.Size([64, 768]) torch.Size([1024, 768])\n",
      "torch.Size([64, 768]) torch.Size([1024, 768])\n",
      "torch.Size([768, 3072]) torch.Size([1024, 3072])\n",
      "torch.Size([3072, 768]) torch.Size([1024, 768])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([64, 768]) torch.Size([1024, 768])\n",
      "torch.Size([64, 768]) torch.Size([1024, 768])\n",
      "torch.Size([64, 768]) torch.Size([1024, 768])\n",
      "torch.Size([64, 768]) torch.Size([1024, 768])\n",
      "torch.Size([64, 768]) torch.Size([1024, 768])\n",
      "torch.Size([64, 768]) torch.Size([1024, 768])\n",
      "torch.Size([64, 768]) torch.Size([1024, 768])\n",
      "torch.Size([64, 768]) torch.Size([1024, 768])\n",
      "torch.Size([64, 768]) torch.Size([1024, 768])\n",
      "torch.Size([64, 768]) torch.Size([1024, 768])\n",
      "torch.Size([64, 768]) torch.Size([1024, 768])\n",
      "torch.Size([64, 768]) torch.Size([1024, 768])\n",
      "torch.Size([768, 3072]) torch.Size([1024, 3072])\n",
      "torch.Size([3072, 768]) torch.Size([1024, 768])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([64, 768]) torch.Size([1024, 768])\n",
      "torch.Size([64, 768]) torch.Size([1024, 768])\n",
      "torch.Size([64, 768]) torch.Size([1024, 768])\n",
      "torch.Size([64, 768]) torch.Size([1024, 768])\n",
      "torch.Size([64, 768]) torch.Size([1024, 768])\n",
      "torch.Size([64, 768]) torch.Size([1024, 768])\n",
      "torch.Size([64, 768]) torch.Size([1024, 768])\n",
      "torch.Size([64, 768]) torch.Size([1024, 768])\n",
      "torch.Size([64, 768]) torch.Size([1024, 768])\n",
      "torch.Size([64, 768]) torch.Size([1024, 768])\n",
      "torch.Size([64, 768]) torch.Size([1024, 768])\n",
      "torch.Size([64, 768]) torch.Size([1024, 768])\n",
      "torch.Size([768, 3072]) torch.Size([1024, 3072])\n",
      "torch.Size([3072, 768]) torch.Size([1024, 768])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([64, 768]) torch.Size([1024, 768])\n",
      "torch.Size([64, 768]) torch.Size([1024, 768])\n",
      "torch.Size([64, 768]) torch.Size([1024, 768])\n",
      "torch.Size([64, 768]) torch.Size([1024, 768])\n",
      "torch.Size([64, 768]) torch.Size([1024, 768])\n",
      "torch.Size([64, 768]) torch.Size([1024, 768])\n",
      "torch.Size([64, 768]) torch.Size([1024, 768])\n",
      "torch.Size([64, 768]) torch.Size([1024, 768])\n",
      "torch.Size([64, 768]) torch.Size([1024, 768])\n",
      "torch.Size([64, 768]) torch.Size([1024, 768])\n",
      "torch.Size([64, 768]) torch.Size([1024, 768])\n",
      "torch.Size([64, 768]) torch.Size([1024, 768])\n",
      "torch.Size([768, 3072]) torch.Size([1024, 3072])\n",
      "torch.Size([3072, 768]) torch.Size([1024, 768])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([64, 768]) torch.Size([1024, 768])\n",
      "torch.Size([64, 768]) torch.Size([1024, 768])\n",
      "torch.Size([64, 768]) torch.Size([1024, 768])\n",
      "torch.Size([64, 768]) torch.Size([1024, 768])\n",
      "torch.Size([64, 768]) torch.Size([1024, 768])\n",
      "torch.Size([64, 768]) torch.Size([1024, 768])\n",
      "torch.Size([64, 768]) torch.Size([1024, 768])\n",
      "torch.Size([64, 768]) torch.Size([1024, 768])\n",
      "torch.Size([64, 768]) torch.Size([1024, 768])\n",
      "torch.Size([64, 768]) torch.Size([1024, 768])\n",
      "torch.Size([64, 768]) torch.Size([1024, 768])\n",
      "torch.Size([64, 768]) torch.Size([1024, 768])\n",
      "torch.Size([768, 3072]) torch.Size([1024, 3072])\n",
      "torch.Size([3072, 768]) torch.Size([1024, 768])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([64, 768]) torch.Size([1024, 768])\n",
      "torch.Size([64, 768]) torch.Size([1024, 768])\n",
      "torch.Size([64, 768]) torch.Size([1024, 768])\n",
      "torch.Size([64, 768]) torch.Size([1024, 768])\n",
      "torch.Size([64, 768]) torch.Size([1024, 768])\n",
      "torch.Size([64, 768]) torch.Size([1024, 768])\n",
      "torch.Size([64, 768]) torch.Size([1024, 768])\n",
      "torch.Size([64, 768]) torch.Size([1024, 768])\n",
      "torch.Size([64, 768]) torch.Size([1024, 768])\n",
      "torch.Size([64, 768]) torch.Size([1024, 768])\n",
      "torch.Size([64, 768]) torch.Size([1024, 768])\n",
      "torch.Size([64, 768]) torch.Size([1024, 768])\n",
      "torch.Size([768, 3072]) torch.Size([1024, 3072])\n",
      "torch.Size([3072, 768]) torch.Size([1024, 768])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([64, 768]) torch.Size([1024, 768])\n",
      "torch.Size([64, 768]) torch.Size([1024, 768])\n",
      "torch.Size([64, 768]) torch.Size([1024, 768])\n",
      "torch.Size([64, 768]) torch.Size([1024, 768])\n",
      "torch.Size([64, 768]) torch.Size([1024, 768])\n",
      "torch.Size([64, 768]) torch.Size([1024, 768])\n",
      "torch.Size([64, 768]) torch.Size([1024, 768])\n",
      "torch.Size([64, 768]) torch.Size([1024, 768])\n",
      "torch.Size([64, 768]) torch.Size([1024, 768])\n",
      "torch.Size([64, 768]) torch.Size([1024, 768])\n",
      "torch.Size([64, 768]) torch.Size([1024, 768])\n",
      "torch.Size([64, 768]) torch.Size([1024, 768])\n",
      "torch.Size([768, 3072]) torch.Size([1024, 3072])\n",
      "torch.Size([3072, 768]) torch.Size([1024, 768])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([64, 768]) torch.Size([1024, 768])\n",
      "torch.Size([64, 768]) torch.Size([1024, 768])\n",
      "torch.Size([64, 768]) torch.Size([1024, 768])\n",
      "torch.Size([64, 768]) torch.Size([1024, 768])\n",
      "torch.Size([64, 768]) torch.Size([1024, 768])\n",
      "torch.Size([64, 768]) torch.Size([1024, 768])\n",
      "torch.Size([64, 768]) torch.Size([1024, 768])\n",
      "torch.Size([64, 768]) torch.Size([1024, 768])\n",
      "torch.Size([64, 768]) torch.Size([1024, 768])\n",
      "torch.Size([64, 768]) torch.Size([1024, 768])\n",
      "torch.Size([64, 768]) torch.Size([1024, 768])\n",
      "torch.Size([64, 768]) torch.Size([1024, 768])\n",
      "torch.Size([768, 3072]) torch.Size([1024, 3072])\n",
      "torch.Size([3072, 768]) torch.Size([1024, 768])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([64, 768]) torch.Size([1024, 768])\n",
      "torch.Size([64, 768]) torch.Size([1024, 768])\n",
      "torch.Size([64, 768]) torch.Size([1024, 768])\n",
      "torch.Size([64, 768]) torch.Size([1024, 768])\n",
      "torch.Size([64, 768]) torch.Size([1024, 768])\n",
      "torch.Size([64, 768]) torch.Size([1024, 768])\n",
      "torch.Size([64, 768]) torch.Size([1024, 768])\n",
      "torch.Size([64, 768]) torch.Size([1024, 768])\n",
      "torch.Size([64, 768]) torch.Size([1024, 768])\n",
      "torch.Size([64, 768]) torch.Size([1024, 768])\n",
      "torch.Size([64, 768]) torch.Size([1024, 768])\n",
      "torch.Size([64, 768]) torch.Size([1024, 768])\n",
      "torch.Size([768, 3072]) torch.Size([1024, 3072])\n",
      "torch.Size([3072, 768]) torch.Size([1024, 768])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([64, 768]) torch.Size([1024, 768])\n",
      "torch.Size([64, 768]) torch.Size([1024, 768])\n",
      "torch.Size([64, 768]) torch.Size([1024, 768])\n",
      "torch.Size([64, 768]) torch.Size([1024, 768])\n",
      "torch.Size([64, 768]) torch.Size([1024, 768])\n",
      "torch.Size([64, 768]) torch.Size([1024, 768])\n",
      "torch.Size([64, 768]) torch.Size([1024, 768])\n",
      "torch.Size([64, 768]) torch.Size([1024, 768])\n",
      "torch.Size([64, 768]) torch.Size([1024, 768])\n",
      "torch.Size([64, 768]) torch.Size([1024, 768])\n",
      "torch.Size([64, 768]) torch.Size([1024, 768])\n",
      "torch.Size([64, 768]) torch.Size([1024, 768])\n",
      "torch.Size([768, 3072]) torch.Size([1024, 3072])\n",
      "torch.Size([3072, 768]) torch.Size([1024, 768])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([768, 64]) torch.Size([1024, 64])\n",
      "torch.Size([64, 768]) torch.Size([1024, 768])\n",
      "torch.Size([64, 768]) torch.Size([1024, 768])\n",
      "torch.Size([64, 768]) torch.Size([1024, 768])\n",
      "torch.Size([64, 768]) torch.Size([1024, 768])\n",
      "torch.Size([64, 768]) torch.Size([1024, 768])\n",
      "torch.Size([64, 768]) torch.Size([1024, 768])\n",
      "torch.Size([64, 768]) torch.Size([1024, 768])\n",
      "torch.Size([64, 768]) torch.Size([1024, 768])\n",
      "torch.Size([64, 768]) torch.Size([1024, 768])\n",
      "torch.Size([64, 768]) torch.Size([1024, 768])\n",
      "torch.Size([64, 768]) torch.Size([1024, 768])\n",
      "torch.Size([64, 768]) torch.Size([1024, 768])\n",
      "torch.Size([768, 3072]) torch.Size([1024, 3072])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/10000 [00:21<59:19:38, 21.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3072, 768]) torch.Size([1024, 768])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([64, 768]) torch.Size([727, 768])\n",
      "torch.Size([64, 768]) torch.Size([727, 768])\n",
      "torch.Size([64, 768]) torch.Size([727, 768])\n",
      "torch.Size([64, 768]) torch.Size([727, 768])\n",
      "torch.Size([64, 768]) torch.Size([727, 768])\n",
      "torch.Size([64, 768]) torch.Size([727, 768])\n",
      "torch.Size([64, 768]) torch.Size([727, 768])\n",
      "torch.Size([64, 768]) torch.Size([727, 768])\n",
      "torch.Size([64, 768]) torch.Size([727, 768])\n",
      "torch.Size([64, 768]) torch.Size([727, 768])\n",
      "torch.Size([64, 768]) torch.Size([727, 768])\n",
      "torch.Size([64, 768]) torch.Size([727, 768])\n",
      "torch.Size([768, 3072]) torch.Size([727, 3072])\n",
      "torch.Size([3072, 768]) torch.Size([727, 768])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([64, 768]) torch.Size([727, 768])\n",
      "torch.Size([64, 768]) torch.Size([727, 768])\n",
      "torch.Size([64, 768]) torch.Size([727, 768])\n",
      "torch.Size([64, 768]) torch.Size([727, 768])\n",
      "torch.Size([64, 768]) torch.Size([727, 768])\n",
      "torch.Size([64, 768]) torch.Size([727, 768])\n",
      "torch.Size([64, 768]) torch.Size([727, 768])\n",
      "torch.Size([64, 768]) torch.Size([727, 768])\n",
      "torch.Size([64, 768]) torch.Size([727, 768])\n",
      "torch.Size([64, 768]) torch.Size([727, 768])\n",
      "torch.Size([64, 768]) torch.Size([727, 768])\n",
      "torch.Size([64, 768]) torch.Size([727, 768])\n",
      "torch.Size([768, 3072]) torch.Size([727, 3072])\n",
      "torch.Size([3072, 768]) torch.Size([727, 768])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([64, 768]) torch.Size([727, 768])\n",
      "torch.Size([64, 768]) torch.Size([727, 768])\n",
      "torch.Size([64, 768]) torch.Size([727, 768])\n",
      "torch.Size([64, 768]) torch.Size([727, 768])\n",
      "torch.Size([64, 768]) torch.Size([727, 768])\n",
      "torch.Size([64, 768]) torch.Size([727, 768])\n",
      "torch.Size([64, 768]) torch.Size([727, 768])\n",
      "torch.Size([64, 768]) torch.Size([727, 768])\n",
      "torch.Size([64, 768]) torch.Size([727, 768])\n",
      "torch.Size([64, 768]) torch.Size([727, 768])\n",
      "torch.Size([64, 768]) torch.Size([727, 768])\n",
      "torch.Size([64, 768]) torch.Size([727, 768])\n",
      "torch.Size([768, 3072]) torch.Size([727, 3072])\n",
      "torch.Size([3072, 768]) torch.Size([727, 768])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([64, 768]) torch.Size([727, 768])\n",
      "torch.Size([64, 768]) torch.Size([727, 768])\n",
      "torch.Size([64, 768]) torch.Size([727, 768])\n",
      "torch.Size([64, 768]) torch.Size([727, 768])\n",
      "torch.Size([64, 768]) torch.Size([727, 768])\n",
      "torch.Size([64, 768]) torch.Size([727, 768])\n",
      "torch.Size([64, 768]) torch.Size([727, 768])\n",
      "torch.Size([64, 768]) torch.Size([727, 768])\n",
      "torch.Size([64, 768]) torch.Size([727, 768])\n",
      "torch.Size([64, 768]) torch.Size([727, 768])\n",
      "torch.Size([64, 768]) torch.Size([727, 768])\n",
      "torch.Size([64, 768]) torch.Size([727, 768])\n",
      "torch.Size([768, 3072]) torch.Size([727, 3072])\n",
      "torch.Size([3072, 768]) torch.Size([727, 768])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([64, 768]) torch.Size([727, 768])\n",
      "torch.Size([64, 768]) torch.Size([727, 768])\n",
      "torch.Size([64, 768]) torch.Size([727, 768])\n",
      "torch.Size([64, 768]) torch.Size([727, 768])\n",
      "torch.Size([64, 768]) torch.Size([727, 768])\n",
      "torch.Size([64, 768]) torch.Size([727, 768])\n",
      "torch.Size([64, 768]) torch.Size([727, 768])\n",
      "torch.Size([64, 768]) torch.Size([727, 768])\n",
      "torch.Size([64, 768]) torch.Size([727, 768])\n",
      "torch.Size([64, 768]) torch.Size([727, 768])\n",
      "torch.Size([64, 768]) torch.Size([727, 768])\n",
      "torch.Size([64, 768]) torch.Size([727, 768])\n",
      "torch.Size([768, 3072]) torch.Size([727, 3072])\n",
      "torch.Size([3072, 768]) torch.Size([727, 768])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([64, 768]) torch.Size([727, 768])\n",
      "torch.Size([64, 768]) torch.Size([727, 768])\n",
      "torch.Size([64, 768]) torch.Size([727, 768])\n",
      "torch.Size([64, 768]) torch.Size([727, 768])\n",
      "torch.Size([64, 768]) torch.Size([727, 768])\n",
      "torch.Size([64, 768]) torch.Size([727, 768])\n",
      "torch.Size([64, 768]) torch.Size([727, 768])\n",
      "torch.Size([64, 768]) torch.Size([727, 768])\n",
      "torch.Size([64, 768]) torch.Size([727, 768])\n",
      "torch.Size([64, 768]) torch.Size([727, 768])\n",
      "torch.Size([64, 768]) torch.Size([727, 768])\n",
      "torch.Size([64, 768]) torch.Size([727, 768])\n",
      "torch.Size([768, 3072]) torch.Size([727, 3072])\n",
      "torch.Size([3072, 768]) torch.Size([727, 768])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([64, 768]) torch.Size([727, 768])\n",
      "torch.Size([64, 768]) torch.Size([727, 768])\n",
      "torch.Size([64, 768]) torch.Size([727, 768])\n",
      "torch.Size([64, 768]) torch.Size([727, 768])\n",
      "torch.Size([64, 768]) torch.Size([727, 768])\n",
      "torch.Size([64, 768]) torch.Size([727, 768])\n",
      "torch.Size([64, 768]) torch.Size([727, 768])\n",
      "torch.Size([64, 768]) torch.Size([727, 768])\n",
      "torch.Size([64, 768]) torch.Size([727, 768])\n",
      "torch.Size([64, 768]) torch.Size([727, 768])\n",
      "torch.Size([64, 768]) torch.Size([727, 768])\n",
      "torch.Size([64, 768]) torch.Size([727, 768])\n",
      "torch.Size([768, 3072]) torch.Size([727, 3072])\n",
      "torch.Size([3072, 768]) torch.Size([727, 768])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([64, 768]) torch.Size([727, 768])\n",
      "torch.Size([64, 768]) torch.Size([727, 768])\n",
      "torch.Size([64, 768]) torch.Size([727, 768])\n",
      "torch.Size([64, 768]) torch.Size([727, 768])\n",
      "torch.Size([64, 768]) torch.Size([727, 768])\n",
      "torch.Size([64, 768]) torch.Size([727, 768])\n",
      "torch.Size([64, 768]) torch.Size([727, 768])\n",
      "torch.Size([64, 768]) torch.Size([727, 768])\n",
      "torch.Size([64, 768]) torch.Size([727, 768])\n",
      "torch.Size([64, 768]) torch.Size([727, 768])\n",
      "torch.Size([64, 768]) torch.Size([727, 768])\n",
      "torch.Size([64, 768]) torch.Size([727, 768])\n",
      "torch.Size([768, 3072]) torch.Size([727, 3072])\n",
      "torch.Size([3072, 768]) torch.Size([727, 768])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([64, 768]) torch.Size([727, 768])\n",
      "torch.Size([64, 768]) torch.Size([727, 768])\n",
      "torch.Size([64, 768]) torch.Size([727, 768])\n",
      "torch.Size([64, 768]) torch.Size([727, 768])\n",
      "torch.Size([64, 768]) torch.Size([727, 768])\n",
      "torch.Size([64, 768]) torch.Size([727, 768])\n",
      "torch.Size([64, 768]) torch.Size([727, 768])\n",
      "torch.Size([64, 768]) torch.Size([727, 768])\n",
      "torch.Size([64, 768]) torch.Size([727, 768])\n",
      "torch.Size([64, 768]) torch.Size([727, 768])\n",
      "torch.Size([64, 768]) torch.Size([727, 768])\n",
      "torch.Size([64, 768]) torch.Size([727, 768])\n",
      "torch.Size([768, 3072]) torch.Size([727, 3072])\n",
      "torch.Size([3072, 768]) torch.Size([727, 768])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([64, 768]) torch.Size([727, 768])\n",
      "torch.Size([64, 768]) torch.Size([727, 768])\n",
      "torch.Size([64, 768]) torch.Size([727, 768])\n",
      "torch.Size([64, 768]) torch.Size([727, 768])\n",
      "torch.Size([64, 768]) torch.Size([727, 768])\n",
      "torch.Size([64, 768]) torch.Size([727, 768])\n",
      "torch.Size([64, 768]) torch.Size([727, 768])\n",
      "torch.Size([64, 768]) torch.Size([727, 768])\n",
      "torch.Size([64, 768]) torch.Size([727, 768])\n",
      "torch.Size([64, 768]) torch.Size([727, 768])\n",
      "torch.Size([64, 768]) torch.Size([727, 768])\n",
      "torch.Size([64, 768]) torch.Size([727, 768])\n",
      "torch.Size([768, 3072]) torch.Size([727, 3072])\n",
      "torch.Size([3072, 768]) torch.Size([727, 768])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([64, 768]) torch.Size([727, 768])\n",
      "torch.Size([64, 768]) torch.Size([727, 768])\n",
      "torch.Size([64, 768]) torch.Size([727, 768])\n",
      "torch.Size([64, 768]) torch.Size([727, 768])\n",
      "torch.Size([64, 768]) torch.Size([727, 768])\n",
      "torch.Size([64, 768]) torch.Size([727, 768])\n",
      "torch.Size([64, 768]) torch.Size([727, 768])\n",
      "torch.Size([64, 768]) torch.Size([727, 768])\n",
      "torch.Size([64, 768]) torch.Size([727, 768])\n",
      "torch.Size([64, 768]) torch.Size([727, 768])\n",
      "torch.Size([64, 768]) torch.Size([727, 768])\n",
      "torch.Size([64, 768]) torch.Size([727, 768])\n",
      "torch.Size([768, 3072]) torch.Size([727, 3072])\n",
      "torch.Size([3072, 768]) torch.Size([727, 768])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([768, 64]) torch.Size([727, 64])\n",
      "torch.Size([64, 768]) torch.Size([727, 768])\n",
      "torch.Size([64, 768]) torch.Size([727, 768])\n",
      "torch.Size([64, 768]) torch.Size([727, 768])\n",
      "torch.Size([64, 768]) torch.Size([727, 768])\n",
      "torch.Size([64, 768]) torch.Size([727, 768])\n",
      "torch.Size([64, 768]) torch.Size([727, 768])\n",
      "torch.Size([64, 768]) torch.Size([727, 768])\n",
      "torch.Size([64, 768]) torch.Size([727, 768])\n",
      "torch.Size([64, 768]) torch.Size([727, 768])\n",
      "torch.Size([64, 768]) torch.Size([727, 768])\n",
      "torch.Size([64, 768]) torch.Size([727, 768])\n",
      "torch.Size([64, 768]) torch.Size([727, 768])\n",
      "torch.Size([768, 3072]) torch.Size([727, 3072])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/10000 [00:42<58:34:42, 21.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3072, 768]) torch.Size([727, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "pruned_gpt2 = sae_lens.HookedSAETransformer.from_pretrained(\"gpt2-small\", device=device)\n",
    "i = 0\n",
    "for batch in tqdm(openwebtext):\n",
    "    if i == 2:\n",
    "        break\n",
    "    pruned_gpt2 = prune_sgpt(pruned_gpt2, batch)\n",
    "    i += 1\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized prompt: ['<|endoftext|>', 'Mit', 'igating', ' the', ' risk', ' of', ' extinction', ' from', ' AI', ' should', ' be', ' a', ' global']\n",
      "Tokenized answer: [' priority']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">        Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">19.46</span><span style=\"font-weight: bold\"> Prob: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">52.99</span><span style=\"font-weight: bold\">% Token: | priority|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m0\u001b[0m\u001b[1m        Logit: \u001b[0m\u001b[1;36m19.46\u001b[0m\u001b[1m Prob: \u001b[0m\u001b[1;36m52.99\u001b[0m\u001b[1m% Token: | priority|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 19.46 Prob: 52.99% Token: | priority|\n",
      "Top 1th token. Logit: 17.44 Prob:  7.02% Token: | effort|\n",
      "Top 2th token. Logit: 16.94 Prob:  4.26% Token: | issue|\n",
      "Top 3th token. Logit: 16.63 Prob:  3.14% Token: | challenge|\n",
      "Top 4th token. Logit: 16.37 Prob:  2.42% Token: | goal|\n",
      "Top 5th token. Logit: 16.06 Prob:  1.78% Token: | concern|\n",
      "Top 6th token. Logit: 15.88 Prob:  1.47% Token: | focus|\n",
      "Top 7th token. Logit: 15.61 Prob:  1.13% Token: | approach|\n",
      "Top 8th token. Logit: 15.53 Prob:  1.04% Token: | policy|\n",
      "Top 9th token. Logit: 15.42 Prob:  0.93% Token: | initiative|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">' priority'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">)]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m' priority'\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"Mitigating the risk of extinction from AI should be a global\"\n",
    "answer = \" priority\"\n",
    "\n",
    "\n",
    "# First see how the model does without SAEs\n",
    "transformer_lens.utils.test_prompt(prompt, answer, pruned_gpt2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:34<00:00, 290.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3383 torch.Size([3383, 1024])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformer_lens, sae_lens\n",
    "from transformer_lens import HookedTransformer\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "dataset = transformer_lens.utils.get_dataset('openwebtext')\n",
    "gpt2: sae_lens.HookedSAETransformer = sae_lens.HookedSAETransformer.from_pretrained_no_processing(\"gpt2-small\", device=device, n_devices=3)\n",
    "\n",
    "class OpenWebText(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset, max_length=1024):\n",
    "        self.dataset = dataset\n",
    "        self.max_length = 1024\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.dataset[idx]['text']\n",
    "        tokens = gpt2.to_tokens(text)\n",
    "        tokens = tokens[:self.max_length]\n",
    "        return tokens\n",
    "    \n",
    "openwebtext = OpenWebText(dataset)\n",
    "\n",
    "from tqdm import tqdm\n",
    "count = 0\n",
    "data = []\n",
    "for batch in tqdm(openwebtext):\n",
    "    if batch.shape[1] == 1024:\n",
    "        data.append(batch)\n",
    "        count += 1\n",
    "data_tensor = torch.cat(data, dim=0)\n",
    "print(count, data_tensor.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits, cache = gpt2.run_with_cache(data_tensor[:8, :], remove_batch_dim=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Please check if the implementation is right:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:34<00:00, 286.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting pruning ...\n"
     ]
    }
   ],
   "source": [
    "import math, time, einops, sae_lens, transformer_lens, torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "class SparseGPT:\n",
    "    def __init__(self, W):\n",
    "        self.dev = W.device\n",
    "        self.W = W.clone()\n",
    "        self.rows = W.shape[0]\n",
    "        self.cols = W.shape[1]\n",
    "        self.H = torch.zeros((self.cols, self.cols), device=self.dev)\n",
    "        self.n_samples = 0\n",
    "\n",
    "    def add_batch(self, X): \n",
    "        tmp = X.shape[0]\n",
    "        self.H *= self.n_samples/(self.n_samples + tmp)\n",
    "        self.n_samples += tmp\n",
    "        X = math.sqrt(2/self.n_samples) * X.float()\n",
    "        self.H += X.T @ X \n",
    "\n",
    "    \n",
    "    def faster_prune(self, W, sparsity=0.25, blocksize=128, percdamp=0.01):\n",
    "        W = W.clone()\n",
    "        W = W.float()\n",
    "        tick = time.time()\n",
    "\n",
    "        H = self.H\n",
    "        del self.H\n",
    "        dead = torch.diag(H) == 0\n",
    "        H[dead, dead] = 1\n",
    "        W[:, dead] = 0\n",
    "\n",
    "        Losses = torch.zeros(self.rows, device=self.dev)\n",
    "\n",
    "        damp = percdamp * torch.mean(torch.diag(H))\n",
    "        diag = torch.arange(self.cols, device=self.dev)\n",
    "        H[diag, diag] += damp\n",
    "        H = torch.linalg.cholesky(H)\n",
    "        H = torch.cholesky_inverse(H)\n",
    "        H = torch.linalg.cholesky(H, upper=True)\n",
    "        Hinv = H\n",
    "\n",
    "        mask = None\n",
    "\n",
    "        for i1 in range(0, self.cols, blocksize):\n",
    "            i2 = min(i1 + blocksize, self.cols)\n",
    "            count = i2 - i1\n",
    "\n",
    "            W1 = W[:, i1:i2].clone()\n",
    "            Q1 = torch.zeros_like(W1)\n",
    "            Err1 = torch.zeros_like(W1)\n",
    "            Losses1 = torch.zeros_like(W1)\n",
    "            Hinv1 = Hinv[i1:i2, i1:i2]\n",
    "\n",
    "            if mask is not None:\n",
    "                mask1 = mask[:, i1:i2]\n",
    "            else:  \n",
    "                tmp = W1 ** 2 / (torch.diag(Hinv1).reshape((1, -1))) ** 2\n",
    "                thresh = torch.sort(tmp.flatten())[0][int(tmp.numel() * sparsity)]\n",
    "                mask1 = tmp <= thresh\n",
    "\n",
    "            for i in range(count):\n",
    "                w = W1[:, i]\n",
    "                d = Hinv1[i, i]\n",
    "\n",
    "                q = w.clone()\n",
    "                q[mask1[:, i]] = 0\n",
    "\n",
    "                Q1[:, i] = q\n",
    "                Losses1[:, i] = (w - q) ** 2 / d ** 2\n",
    "\n",
    "                err1 = (w - q) / d \n",
    "                W1[:, i:] -= err1.unsqueeze(1).matmul(Hinv1[i, i:].unsqueeze(0))\n",
    "                Err1[:, i] = err1\n",
    "\n",
    "            W[:, i1:i2] = Q1\n",
    "            Losses += torch.sum(Losses1, 1) / 2\n",
    "\n",
    "            W[:, i2:] -= Err1.matmul(Hinv[i1:i2, i2:])\n",
    "\n",
    "        torch.cuda.synchronize()\n",
    "        self.W = W.reshape((self.rows, self.cols)).to(self.dev)\n",
    "        \n",
    "    def free(self):\n",
    "        self.H = None\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def prune_sparsegpt(model, tokens):\n",
    "    print(\"Starting pruning ...\")\n",
    "    wts_act = {\n",
    "    'attn.W_Q': 'attn.hook_q',\n",
    "    'attn.W_K': 'attn.hook_k',\n",
    "    'attn.W_V': 'attn.hook_v',\n",
    "    'attn.W_O': 'hook_attn_out',\n",
    "    'mlp.W_in': 'mlp.hook_pre',\n",
    "    'mlp.W_out': 'hook_mlp_out'\n",
    "    }\n",
    "\n",
    "    logits, cache = model.run_with_cache(tokens, remove_batch_dim=True)\n",
    "    for layer in range(model.cfg.n_layers):\n",
    "        layer_cache = {k: v for k, v in cache.items() if f'blocks.{layer}.' in k}\n",
    "\n",
    "        for wt, act in wts_act.items():\n",
    "            W = model.get_parameter(f'blocks.{layer}.{wt}')\n",
    "            X = layer_cache[f'blocks.{layer}.{act}']\n",
    "\n",
    "\n",
    "            if W.dim() == 2:\n",
    "                sparsegpt_object = SparseGPT(W)\n",
    "                sparsegpt_object.add_batch(X)\n",
    "                sparsegpt_object.faster_prune(W)\n",
    "                W.copy_(sparsegpt_object.W)\n",
    "                sparsegpt_object.free()\n",
    "\n",
    "            else:\n",
    "                if 'W_O' in wt:\n",
    "                    for head in range(W.shape[0]):\n",
    "                        sparsegpt_object = SparseGPT(W[head])\n",
    "                        sparsegpt_object.add_batch(X)\n",
    "                        sparsegpt_object.faster_prune(W[head])\n",
    "                        W[head].copy_(sparsegpt_object.W)\n",
    "                        sparsegpt_object.free()\n",
    "\n",
    "\n",
    "                else:\n",
    "                    for head in range(W.shape[0]):\n",
    "                        sparsegpt_object = SparseGPT(W[head])\n",
    "                        sparsegpt_object.add_batch(X[:, head, :])\n",
    "                        sparsegpt_object.faster_prune(W[head])\n",
    "                        W[head].copy_(sparsegpt_object.W)\n",
    "                        sparsegpt_object.free()\n",
    "\n",
    "            del sparsegpt_object\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    del layer_cache\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "\n",
    "\n",
    "pruned_gpt2 = sae_lens.HookedSAETransformer.from_pretrained(\"gpt2-small\", device=\"cuda\")\n",
    "dataset = transformer_lens.utils.get_dataset('openwebtext')\n",
    "\n",
    "class OpenWebText(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset, max_length=1024):\n",
    "        self.dataset = dataset\n",
    "        self.max_length = 1024\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.dataset[idx]['text']\n",
    "        tokens = pruned_gpt2.to_tokens(text)\n",
    "        tokens = tokens[:self.max_length]\n",
    "        return tokens\n",
    "    \n",
    "openwebtext = OpenWebText(dataset)\n",
    "\n",
    "data = []\n",
    "for batch in tqdm(openwebtext):\n",
    "    if batch.shape[1] == 1024:\n",
    "        data.append(batch)\n",
    "\n",
    "data_tensor = torch.cat(data, dim=0)[:8, :]\n",
    "prune_sparsegpt(pruned_gpt2, data_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.003115949220955372"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt2: sae_lens.HookedSAETransformer = sae_lens.HookedSAETransformer.from_pretrained(\"gpt2-small\", device=\"cuda\")\n",
    "torch.abs(pruned_gpt2.W_Q - gpt2.W_Q).mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized prompt: ['<|endoftext|>', 'Mit', 'igating', ' the', ' risk', ' of', ' extinction', ' from', ' AI', ' should', ' be', ' a', ' global']\n",
      "Tokenized answer: [' priority']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"font-weight: bold\">        Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">14.63</span><span style=\"font-weight: bold\"> Prob:  </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3.66</span><span style=\"font-weight: bold\">% Token: | priority|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m3\u001b[0m\u001b[1m        Logit: \u001b[0m\u001b[1;36m14.63\u001b[0m\u001b[1m Prob:  \u001b[0m\u001b[1;36m3.66\u001b[0m\u001b[1m% Token: | priority|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 15.88 Prob: 12.87% Token: | issue|\n",
      "Top 1th token. Logit: 15.45 Prob:  8.34% Token: | concern|\n",
      "Top 2th token. Logit: 14.68 Prob:  3.85% Token: | problem|\n",
      "Top 3th token. Logit: 14.63 Prob:  3.66% Token: | priority|\n",
      "Top 4th token. Logit: 13.89 Prob:  1.75% Token: | cause|\n",
      "Top 5th token. Logit: 13.85 Prob:  1.68% Token: | one|\n",
      "Top 6th token. Logit: 13.82 Prob:  1.63% Token: | phenomenon|\n",
      "Top 7th token. Logit: 13.65 Prob:  1.37% Token: | trend|\n",
      "Top 8th token. Logit: 13.64 Prob:  1.36% Token: | challenge|\n",
      "Top 9th token. Logit: 13.39 Prob:  1.06% Token: | policy|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">' priority'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"font-weight: bold\">)]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m' priority'\u001b[0m, \u001b[1;36m3\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"Mitigating the risk of extinction from AI should be a global\"\n",
    "answer = \" priority\"\n",
    "\n",
    "\n",
    "# First see how the model does without SAEs\n",
    "transformer_lens.utils.test_prompt(prompt, answer, pruned_gpt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized prompt: ['<|endoftext|>', 'Mary', ' and', ' John', ' went', ' to', ' the', ' park', ' to', ' play', '.', ' Mary', ' gave', ' the', ' ball', ' to']\n",
      "Tokenized answer: [' John']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">        Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">17.57</span><span style=\"font-weight: bold\"> Prob: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">56.01</span><span style=\"font-weight: bold\">% Token: | John|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m0\u001b[0m\u001b[1m        Logit: \u001b[0m\u001b[1;36m17.57\u001b[0m\u001b[1m Prob: \u001b[0m\u001b[1;36m56.01\u001b[0m\u001b[1m% Token: | John|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 17.57 Prob: 56.01% Token: | John|\n",
      "Top 1th token. Logit: 15.73 Prob:  8.86% Token: | Mary|\n",
      "Top 2th token. Logit: 15.53 Prob:  7.25% Token: | the|\n",
      "Top 3th token. Logit: 15.41 Prob:  6.43% Token: | her|\n",
      "Top 4th token. Logit: 14.15 Prob:  1.82% Token: | a|\n",
      "Top 5th token. Logit: 14.01 Prob:  1.60% Token: | them|\n",
      "Top 6th token. Logit: 13.39 Prob:  0.86% Token: | his|\n",
      "Top 7th token. Logit: 13.25 Prob:  0.74% Token: | Jesus|\n",
      "Top 8th token. Logit: 12.88 Prob:  0.51% Token: | Joseph|\n",
      "Top 9th token. Logit: 12.85 Prob:  0.50% Token: | me|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">' John'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">)]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m' John'\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized prompt: ['<|endoftext|>', 'Mary', ' and', ' John', ' went', ' to', ' the', ' park', ' to', ' play', '.', ' Mary', ' gave', ' the', ' ball', ' to']\n",
      "Tokenized answer: [' John']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"font-weight: bold\">        Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10.64</span><span style=\"font-weight: bold\"> Prob:  </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.37</span><span style=\"font-weight: bold\">% Token: | John|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m3\u001b[0m\u001b[1m        Logit: \u001b[0m\u001b[1;36m10.64\u001b[0m\u001b[1m Prob:  \u001b[0m\u001b[1;36m1.37\u001b[0m\u001b[1m% Token: | John|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 13.59 Prob: 26.24% Token: | the|\n",
      "Top 1th token. Logit: 12.10 Prob:  5.88% Token: | a|\n",
      "Top 2th token. Logit: 11.29 Prob:  2.63% Token: | his|\n",
      "Top 3th token. Logit: 10.64 Prob:  1.37% Token: | John|\n",
      "Top 4th token. Logit: 10.53 Prob:  1.23% Token: | be|\n",
      "Top 5th token. Logit: 10.34 Prob:  1.01% Token: | him|\n",
      "Top 6th token. Logit: 10.29 Prob:  0.97% Token: | me|\n",
      "Top 7th token. Logit: 10.09 Prob:  0.79% Token: | an|\n",
      "Top 8th token. Logit: 10.04 Prob:  0.75% Token: | some|\n",
      "Top 9th token. Logit:  9.86 Prob:  0.63% Token: | her|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">' John'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"font-weight: bold\">)]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m' John'\u001b[0m, \u001b[1;36m3\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"Mary and John went to the park to play. Mary gave the ball to\"\n",
    "answer = \" John\"\n",
    "\n",
    "transformer_lens.utils.test_prompt(prompt, answer, gpt2)\n",
    "transformer_lens.utils.test_prompt(prompt, answer, pruned_gpt2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SAE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import itertools\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "from collections import Counter, defaultdict\n",
    "from copy import deepcopy\n",
    "from dataclasses import dataclass\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "from typing import Any, Callable, Literal, TypeAlias\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch as t\n",
    "from datasets import load_dataset\n",
    "import transformer_lens\n",
    "import sae_lens\n",
    "\n",
    "import einops\n",
    "import circuitsvis as cv\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from IPython.display import HTML, IFrame, clear_output, display\n",
    "from jaxtyping import Float, Int\n",
    "from openai import OpenAI\n",
    "from rich import print as rprint\n",
    "from rich.table import Table\n",
    "from tabulate import tabulate\n",
    "\n",
    "\n",
    "# Hugging face: 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\n",
    "# Wandb: 6b549d940e7a29c79c184f27f25606e94a48a966"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = t.device(\"cuda\" if t.cuda.is_available() else \"cpu\")\n",
    "t.set_grad_enabled(False)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "gpt2: sae_lens.HookedSAETransformer = sae_lens.HookedSAETransformer.from_pretrained(\"gpt2-small\", device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attention SAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9c376168d144dd2a66ce89f52b0ebcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)-06_rsanthropic_rie25000_nr4_v9_cfg.json:   0%|          | 0.00/1.01k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42d68d40f9af4b46a318cd5c3d8076b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)c1.00e-06_rsanthropic_rie25000_nr4_v9.pt:   0%|          | 0.00/151M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e97449dd52141f6be96b905c15252a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)-06_rsanthropic_rie25000_nr4_v5_cfg.json:   0%|          | 0.00/1.02k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9cf1b87c313484e97c086386fd63a22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)c1.00e-06_rsanthropic_rie25000_nr4_v5.pt:   0%|          | 0.00/151M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "702494c611814cdc92a1bfeccfdbb503",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)-06_rsanthropic_rie25000_nr4_v4_cfg.json:   0%|          | 0.00/1.02k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "808ede7b9d6e4fdd9f015ba113ae8ce1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)c1.00e-06_rsanthropic_rie25000_nr4_v4.pt:   0%|          | 0.00/151M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9bf62c642c5436d9afca4507a47f15d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)-06_rsanthropic_rie25000_nr4_v9_cfg.json:   0%|          | 0.00/1.01k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d10d047494b44a4789f5383f8460f645",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)c1.00e-06_rsanthropic_rie25000_nr4_v9.pt:   0%|          | 0.00/151M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6821808b8ca4b5ba49de4e9e2f1f430",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)-06_rsanthropic_rie25000_nr4_v7_cfg.json:   0%|          | 0.00/1.01k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e0061cbdb14435496e9b8c9e4fb3bd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)c1.00e-06_rsanthropic_rie25000_nr4_v7.pt:   0%|          | 0.00/151M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2f366e1c4c44c13b52cf0249699be19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)-06_rsanthropic_rie25000_nr4_v9_cfg.json:   0%|          | 0.00/1.02k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35fe80cd4cdd4c80acae239d5dfca16e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)c1.00e-06_rsanthropic_rie25000_nr4_v9.pt:   0%|          | 0.00/302M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f614d87176e4d6da817699e5726bfb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)-06_rsanthropic_rie25000_nr4_v9_cfg.json:   0%|          | 0.00/1.01k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0256fa7f236340a08177b2493b549abf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)c1.00e-06_rsanthropic_rie25000_nr4_v9.pt:   0%|          | 0.00/151M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ef7abf2015249d8bf1e0241e65d4ae4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)-06_rsanthropic_rie25000_nr4_v9_cfg.json:   0%|          | 0.00/1.01k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e5639bc3e8641e9a746ca6fa09b349e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)c1.00e-06_rsanthropic_rie25000_nr4_v9.pt:   0%|          | 0.00/302M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64bf1f088e384feaa0c5078e1399a76c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)-05_rsanthropic_rie25000_nr4_v6_cfg.json:   0%|          | 0.00/1.02k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f8bc030aede474e945372ba03890c59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)c1.00e-05_rsanthropic_rie25000_nr4_v6.pt:   0%|          | 0.00/151M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0917c45b0f8a4acb8c0674116c633006",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)-06_rsanthropic_rie25000_nr4_v9_cfg.json:   0%|          | 0.00/1.01k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "357a64ee28ed45d4ac2934efb947429b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)c1.00e-06_rsanthropic_rie25000_nr4_v9.pt:   0%|          | 0.00/151M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5655517736e54bbaa4aef090695e8474",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)-05_rsanthropic_rie25000_nr4_v9_cfg.json:   0%|          | 0.00/1.02k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcf699835bc94188be87abd319ab5597",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)c1.00e-05_rsanthropic_rie25000_nr4_v9.pt:   0%|          | 0.00/151M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb58f319904a4482ad2e1e2d283ffae1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)-06_rsanthropic_rie25000_nr4_v9_cfg.json:   0%|          | 0.00/1.03k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80ac1abd619b44979a4c87935778cd40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)c3.16e-06_rsanthropic_rie25000_nr4_v9.pt:   0%|          | 0.00/151M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "attn_saes = {\n",
    "    layer: sae_lens.SAE.from_pretrained(\n",
    "        \"gpt2-small-hook-z-kk\",\n",
    "        f\"blocks.{layer}.hook_z\",\n",
    "        device=device,\n",
    "    )[0]\n",
    "    for layer in range(gpt2.cfg.n_layers)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: SAE(\n",
      "  (activation_fn): ReLU()\n",
      "  (hook_sae_input): HookPoint()\n",
      "  (hook_sae_acts_pre): HookPoint()\n",
      "  (hook_sae_acts_post): HookPoint()\n",
      "  (hook_sae_output): HookPoint()\n",
      "  (hook_sae_recons): HookPoint()\n",
      "  (hook_sae_error): HookPoint()\n",
      "), 1: SAE(\n",
      "  (activation_fn): ReLU()\n",
      "  (hook_sae_input): HookPoint()\n",
      "  (hook_sae_acts_pre): HookPoint()\n",
      "  (hook_sae_acts_post): HookPoint()\n",
      "  (hook_sae_output): HookPoint()\n",
      "  (hook_sae_recons): HookPoint()\n",
      "  (hook_sae_error): HookPoint()\n",
      "), 2: SAE(\n",
      "  (activation_fn): ReLU()\n",
      "  (hook_sae_input): HookPoint()\n",
      "  (hook_sae_acts_pre): HookPoint()\n",
      "  (hook_sae_acts_post): HookPoint()\n",
      "  (hook_sae_output): HookPoint()\n",
      "  (hook_sae_recons): HookPoint()\n",
      "  (hook_sae_error): HookPoint()\n",
      "), 3: SAE(\n",
      "  (activation_fn): ReLU()\n",
      "  (hook_sae_input): HookPoint()\n",
      "  (hook_sae_acts_pre): HookPoint()\n",
      "  (hook_sae_acts_post): HookPoint()\n",
      "  (hook_sae_output): HookPoint()\n",
      "  (hook_sae_recons): HookPoint()\n",
      "  (hook_sae_error): HookPoint()\n",
      "), 4: SAE(\n",
      "  (activation_fn): ReLU()\n",
      "  (hook_sae_input): HookPoint()\n",
      "  (hook_sae_acts_pre): HookPoint()\n",
      "  (hook_sae_acts_post): HookPoint()\n",
      "  (hook_sae_output): HookPoint()\n",
      "  (hook_sae_recons): HookPoint()\n",
      "  (hook_sae_error): HookPoint()\n",
      "), 5: SAE(\n",
      "  (activation_fn): ReLU()\n",
      "  (hook_sae_input): HookPoint()\n",
      "  (hook_sae_acts_pre): HookPoint()\n",
      "  (hook_sae_acts_post): HookPoint()\n",
      "  (hook_sae_output): HookPoint()\n",
      "  (hook_sae_recons): HookPoint()\n",
      "  (hook_sae_error): HookPoint()\n",
      "), 6: SAE(\n",
      "  (activation_fn): ReLU()\n",
      "  (hook_sae_input): HookPoint()\n",
      "  (hook_sae_acts_pre): HookPoint()\n",
      "  (hook_sae_acts_post): HookPoint()\n",
      "  (hook_sae_output): HookPoint()\n",
      "  (hook_sae_recons): HookPoint()\n",
      "  (hook_sae_error): HookPoint()\n",
      "), 7: SAE(\n",
      "  (activation_fn): ReLU()\n",
      "  (hook_sae_input): HookPoint()\n",
      "  (hook_sae_acts_pre): HookPoint()\n",
      "  (hook_sae_acts_post): HookPoint()\n",
      "  (hook_sae_output): HookPoint()\n",
      "  (hook_sae_recons): HookPoint()\n",
      "  (hook_sae_error): HookPoint()\n",
      "), 8: SAE(\n",
      "  (activation_fn): ReLU()\n",
      "  (hook_sae_input): HookPoint()\n",
      "  (hook_sae_acts_pre): HookPoint()\n",
      "  (hook_sae_acts_post): HookPoint()\n",
      "  (hook_sae_output): HookPoint()\n",
      "  (hook_sae_recons): HookPoint()\n",
      "  (hook_sae_error): HookPoint()\n",
      "), 9: SAE(\n",
      "  (activation_fn): ReLU()\n",
      "  (hook_sae_input): HookPoint()\n",
      "  (hook_sae_acts_pre): HookPoint()\n",
      "  (hook_sae_acts_post): HookPoint()\n",
      "  (hook_sae_output): HookPoint()\n",
      "  (hook_sae_recons): HookPoint()\n",
      "  (hook_sae_error): HookPoint()\n",
      "), 10: SAE(\n",
      "  (activation_fn): ReLU()\n",
      "  (hook_sae_input): HookPoint()\n",
      "  (hook_sae_acts_pre): HookPoint()\n",
      "  (hook_sae_acts_post): HookPoint()\n",
      "  (hook_sae_output): HookPoint()\n",
      "  (hook_sae_recons): HookPoint()\n",
      "  (hook_sae_error): HookPoint()\n",
      "), 11: SAE(\n",
      "  (activation_fn): ReLU()\n",
      "  (hook_sae_input): HookPoint()\n",
      "  (hook_sae_acts_pre): HookPoint()\n",
      "  (hook_sae_acts_post): HookPoint()\n",
      "  (hook_sae_output): HookPoint()\n",
      "  (hook_sae_recons): HookPoint()\n",
      "  (hook_sae_error): HookPoint()\n",
      ")}\n"
     ]
    }
   ],
   "source": [
    "print(attn_saes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://neuronpedia.org/gpt2-small/9-att-kk/2?embed=true&embedexplanation=true&embedplots=true&embedtest=true&height=300\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"800\"\n",
       "            height=\"600\"\n",
       "            src=\"https://neuronpedia.org/gpt2-small/9-att-kk/2?embed=true&embedexplanation=true&embedplots=true&embedtest=true&height=300\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7ef9b05566e0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def display_dashboard(\n",
    "    sae_release=\"gpt2-small-res-jb\",\n",
    "    sae_id=\"blocks.7.hook_resid_pre\",\n",
    "    latent_idx=0,\n",
    "    width=800,\n",
    "    height=600,\n",
    "):\n",
    "    release = sae_lens.toolkit.pretrained_saes_directory.get_pretrained_saes_directory()[sae_release]\n",
    "    neuronpedia_id = release.neuronpedia_id[sae_id]\n",
    "\n",
    "    url = f\"https://neuronpedia.org/{neuronpedia_id}/{latent_idx}?embed=true&embedexplanation=true&embedplots=true&embedtest=true&height=300\"\n",
    "\n",
    "    print(url)\n",
    "    display(IFrame(url, width=width, height=height))\n",
    "\n",
    "\n",
    "layer = 9\n",
    "\n",
    "display_dashboard(\n",
    "    sae_release=\"gpt2-small-hook-z-kk\",\n",
    "    sae_id=f\"blocks.{layer}.hook_z\",\n",
    "    latent_idx=2,  # or you can try `random.randint(0, attn_saes[layer].cfg.d_sae)`\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HookedTransformerConfig:\n",
       "{'NTK_by_parts_factor': 8.0,\n",
       " 'NTK_by_parts_high_freq_factor': 4.0,\n",
       " 'NTK_by_parts_low_freq_factor': 1.0,\n",
       " 'act_fn': 'gelu_new',\n",
       " 'attention_dir': 'causal',\n",
       " 'attn_only': False,\n",
       " 'attn_scale': 8.0,\n",
       " 'attn_scores_soft_cap': -1.0,\n",
       " 'attn_types': None,\n",
       " 'checkpoint_index': None,\n",
       " 'checkpoint_label_type': None,\n",
       " 'checkpoint_value': None,\n",
       " 'd_head': 64,\n",
       " 'd_mlp': 3072,\n",
       " 'd_model': 768,\n",
       " 'd_vocab': 50257,\n",
       " 'd_vocab_out': 50257,\n",
       " 'decoder_start_token_id': None,\n",
       " 'default_prepend_bos': True,\n",
       " 'device': device(type='cuda'),\n",
       " 'dtype': torch.float32,\n",
       " 'eps': 1e-05,\n",
       " 'experts_per_token': None,\n",
       " 'final_rms': False,\n",
       " 'from_checkpoint': False,\n",
       " 'gated_mlp': False,\n",
       " 'init_mode': 'gpt2',\n",
       " 'init_weights': False,\n",
       " 'initializer_range': 0.02886751345948129,\n",
       " 'load_in_4bit': False,\n",
       " 'model_name': 'gpt2',\n",
       " 'n_ctx': 1024,\n",
       " 'n_devices': 1,\n",
       " 'n_heads': 12,\n",
       " 'n_key_value_heads': None,\n",
       " 'n_layers': 12,\n",
       " 'n_params': 84934656,\n",
       " 'normalization_type': 'LNPre',\n",
       " 'num_experts': None,\n",
       " 'original_architecture': 'GPT2LMHeadModel',\n",
       " 'output_logits_soft_cap': -1.0,\n",
       " 'parallel_attn_mlp': False,\n",
       " 'positional_embedding_type': 'standard',\n",
       " 'post_embedding_ln': False,\n",
       " 'relative_attention_max_distance': None,\n",
       " 'relative_attention_num_buckets': None,\n",
       " 'rotary_adjacent_pairs': False,\n",
       " 'rotary_base': 10000,\n",
       " 'rotary_dim': None,\n",
       " 'scale_attn_by_inverse_layer_idx': False,\n",
       " 'seed': None,\n",
       " 'tie_word_embeddings': False,\n",
       " 'tokenizer_name': 'gpt2',\n",
       " 'tokenizer_prepends_bos': False,\n",
       " 'trust_remote_code': False,\n",
       " 'ungroup_grouped_query_attention': False,\n",
       " 'use_NTK_by_parts_rope': False,\n",
       " 'use_attn_in': False,\n",
       " 'use_attn_result': False,\n",
       " 'use_attn_scale': True,\n",
       " 'use_hook_mlp_in': False,\n",
       " 'use_hook_tokens': False,\n",
       " 'use_local_attn': False,\n",
       " 'use_normalization_before_and_after': False,\n",
       " 'use_split_qkv_input': False,\n",
       " 'window_size': None}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt2.cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Magnitude pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n",
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "def prune_magnitude(W, sparse_ratio=0.5):\n",
    "    W_abs = W.abs()\n",
    "    k = int(W_abs.numel() * sparse_ratio)\n",
    "    _, indices = W_abs.view(-1).topk(k)\n",
    "    mask = t.zeros_like(W_abs)\n",
    "    mask.view(-1)[indices] = 1\n",
    "    return mask*W\n",
    "\n",
    "def prune_model(model):\n",
    "    wts = ['W_Q', 'W_K', 'W_V', 'W_O', 'W_in', 'W_out']\n",
    "    for name, param in gpt2.named_parameters():\n",
    "        # print(f\"Layer: {name}, Shape: {param.shape}\")\n",
    "        if name.split('.')[-1] in wts:\n",
    "            if param.dim() == 3:    \n",
    "                for i in range(param.shape[0]):\n",
    "                    param[i] = prune_magnitude(param[i])\n",
    "            else:\n",
    "                param = prune_magnitude(param)\n",
    "    \n",
    "    return model\n",
    "\n",
    "gpt2: sae_lens.HookedSAETransformer = sae_lens.HookedSAETransformer.from_pretrained(\"gpt2-small\", device=device)\n",
    "pruned_gpt2 = prune_model(gpt2)\n",
    "gpt2: sae_lens.HookedSAETransformer = sae_lens.HookedSAETransformer.from_pretrained(\"gpt2-small\", device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized prompt: ['<|endoftext|>', 'I']\n",
      "Tokenized answer: [' am']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"font-weight: bold\">        Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">13.88</span><span style=\"font-weight: bold\"> Prob:  </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4.74</span><span style=\"font-weight: bold\">% Token: | am|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m3\u001b[0m\u001b[1m        Logit: \u001b[0m\u001b[1;36m13.88\u001b[0m\u001b[1m Prob:  \u001b[0m\u001b[1;36m4.74\u001b[0m\u001b[1m% Token: | am|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 14.56 Prob:  9.34% Token: |'m|\n",
      "Top 1th token. Logit: 14.51 Prob:  8.81% Token: |'ve|\n",
      "Top 2th token. Logit: 14.17 Prob:  6.31% Token: | have|\n",
      "Top 3th token. Logit: 13.88 Prob:  4.74% Token: | am|\n",
      "Top 4th token. Logit: 13.88 Prob:  4.69% Token: | was|\n",
      "Top 5th token. Logit: 13.39 Prob:  2.88% Token: | don|\n",
      "Top 6th token. Logit: 13.29 Prob:  2.61% Token: | think|\n",
      "Top 7th token. Logit: 13.15 Prob:  2.27% Token: | had|\n",
      "Top 8th token. Logit: 13.09 Prob:  2.14% Token: | know|\n",
      "Top 9th token. Logit: 12.82 Prob:  1.64% Token: | can|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">' am'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"font-weight: bold\">)]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m' am'\u001b[0m, \u001b[1;36m3\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"I\"\n",
    "answer = \" am\"\n",
    "transformer_lens.utils.test_prompt(prompt, answer, gpt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized prompt: ['<|endoftext|>', 'I']\n",
      "Tokenized answer: [' am']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"font-weight: bold\">        Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">13.83</span><span style=\"font-weight: bold\"> Prob:  </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3.99</span><span style=\"font-weight: bold\">% Token: | am|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m4\u001b[0m\u001b[1m        Logit: \u001b[0m\u001b[1;36m13.83\u001b[0m\u001b[1m Prob:  \u001b[0m\u001b[1;36m3.99\u001b[0m\u001b[1m% Token: | am|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 14.82 Prob: 10.75% Token: | have|\n",
      "Top 1th token. Logit: 14.21 Prob:  5.82% Token: | was|\n",
      "Top 2th token. Logit: 14.16 Prob:  5.53% Token: |'m|\n",
      "Top 3th token. Logit: 14.06 Prob:  4.99% Token: |'ve|\n",
      "Top 4th token. Logit: 13.83 Prob:  3.99% Token: | am|\n",
      "Top 5th token. Logit: 13.71 Prob:  3.53% Token: | would|\n",
      "Top 6th token. Logit: 13.57 Prob:  3.08% Token: |.|\n",
      "Top 7th token. Logit: 13.47 Prob:  2.78% Token: | will|\n",
      "Top 8th token. Logit: 13.41 Prob:  2.61% Token: | just|\n",
      "Top 9th token. Logit: 13.36 Prob:  2.49% Token: | think|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">' am'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"font-weight: bold\">)]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m' am'\u001b[0m, \u001b[1;36m4\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"I\"\n",
    "answer = \" am\"\n",
    "transformer_lens.utils.test_prompt(prompt, answer, pruned_gpt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.3225, device='cuda:0')"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer_lens.evals.sanity_check(gpt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.8446, device='cuda:0')"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer_lens.evals.sanity_check(pruned_gpt2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wanda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HookedTransformerConfig:\n",
       "{'NTK_by_parts_factor': 8.0,\n",
       " 'NTK_by_parts_high_freq_factor': 4.0,\n",
       " 'NTK_by_parts_low_freq_factor': 1.0,\n",
       " 'act_fn': 'gelu_new',\n",
       " 'attention_dir': 'causal',\n",
       " 'attn_only': False,\n",
       " 'attn_scale': 8.0,\n",
       " 'attn_scores_soft_cap': -1.0,\n",
       " 'attn_types': None,\n",
       " 'checkpoint_index': None,\n",
       " 'checkpoint_label_type': None,\n",
       " 'checkpoint_value': None,\n",
       " 'd_head': 64,\n",
       " 'd_mlp': 3072,\n",
       " 'd_model': 768,\n",
       " 'd_vocab': 50257,\n",
       " 'd_vocab_out': 50257,\n",
       " 'decoder_start_token_id': None,\n",
       " 'default_prepend_bos': True,\n",
       " 'device': device(type='cuda'),\n",
       " 'dtype': torch.float32,\n",
       " 'eps': 1e-05,\n",
       " 'experts_per_token': None,\n",
       " 'final_rms': False,\n",
       " 'from_checkpoint': False,\n",
       " 'gated_mlp': False,\n",
       " 'init_mode': 'gpt2',\n",
       " 'init_weights': False,\n",
       " 'initializer_range': 0.02886751345948129,\n",
       " 'load_in_4bit': False,\n",
       " 'model_name': 'gpt2',\n",
       " 'n_ctx': 1024,\n",
       " 'n_devices': 1,\n",
       " 'n_heads': 12,\n",
       " 'n_key_value_heads': None,\n",
       " 'n_layers': 12,\n",
       " 'n_params': 84934656,\n",
       " 'normalization_type': 'LNPre',\n",
       " 'num_experts': None,\n",
       " 'original_architecture': 'GPT2LMHeadModel',\n",
       " 'output_logits_soft_cap': -1.0,\n",
       " 'parallel_attn_mlp': False,\n",
       " 'positional_embedding_type': 'standard',\n",
       " 'post_embedding_ln': False,\n",
       " 'relative_attention_max_distance': None,\n",
       " 'relative_attention_num_buckets': None,\n",
       " 'rotary_adjacent_pairs': False,\n",
       " 'rotary_base': 10000,\n",
       " 'rotary_dim': None,\n",
       " 'scale_attn_by_inverse_layer_idx': False,\n",
       " 'seed': None,\n",
       " 'tie_word_embeddings': False,\n",
       " 'tokenizer_name': 'gpt2',\n",
       " 'tokenizer_prepends_bos': False,\n",
       " 'trust_remote_code': False,\n",
       " 'ungroup_grouped_query_attention': False,\n",
       " 'use_NTK_by_parts_rope': False,\n",
       " 'use_attn_in': False,\n",
       " 'use_attn_result': False,\n",
       " 'use_attn_scale': True,\n",
       " 'use_hook_mlp_in': False,\n",
       " 'use_hook_tokens': False,\n",
       " 'use_local_attn': False,\n",
       " 'use_normalization_before_and_after': False,\n",
       " 'use_split_qkv_input': False,\n",
       " 'window_size': None}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt2.cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 4,  0,  1,  0],\n",
       "        [ 0,  0, -1, -3],\n",
       "        [-3,  0,  0,  2]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def prune_wanda(W, X_norm, sparse_ratio=0.5):\n",
    "    W_metric = W.abs() * X_norm\n",
    "    _, sorted_idx = W_metric.sort(dim=1)\n",
    "    pruned_idx = sorted_idx[:, :int(W.shape[1] * sparse_ratio)]\n",
    "    \n",
    "    W_clone = W.detach().clone()    \n",
    "    W_clone.scatter_(dim=1, index=pruned_idx, src=t.zeros_like(pruned_idx, dtype=W.dtype))\n",
    "    return W_clone\n",
    "\n",
    "W = t.tensor([\n",
    "    [4, 0, 1, -1],\n",
    "    [3, -2, -1, -3],\n",
    "    [-3, 1, 0, 2]\n",
    "])\n",
    "X = t.tensor([\n",
    "    [1, 2, 8, 3]\n",
    "])\n",
    "\n",
    "prune_wanda(W, X, sparse_ratio=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_model(model, hooks):\n",
    "    text = \"A quick brown fox jumps over the lazy dog.\"\n",
    "    tokens = model.to_tokens(text)\n",
    "    logits, cache = model.run_with_cache(tokens, remove_batch_dim=True)\n",
    "\n",
    "    for hook in hooks:\n",
    "        logits, cache = model.run_with_cache(tokens, remove_batch_dim=True)\n",
    "        layer = int(hook.split('.')[1])\n",
    "\n",
    "        if 'mlp' in hook:\n",
    "            if 'out' in hook:\n",
    "                X = cache[hook].norm(p=2, dim=0)\n",
    "                model.W_out[layer] = t.nn.Parameter(prune_wanda(model.W_out[layer], X, sparse_ratio=0.5))\n",
    "            else:\n",
    "                X = cache[hook]\n",
    "                X = X.norm(p=2, dim=0)\n",
    "                model.W_in[layer] = t.nn.Parameter(prune_wanda(model.W_in[layer], X, sparse_ratio=0.5))\n",
    "        else:\n",
    "            for head in range(model.cfg.n_heads):\n",
    "                X = cache[hook][:, head, :]  \n",
    "                X = X.norm(p=2, dim=0)\n",
    "\n",
    "                if 'q' in hook:\n",
    "                    model.W_Q[layer, head] = t.nn.Parameter(prune_wanda(model.W_Q[layer, head], X, sparse_ratio=0.5)\n",
    ")\n",
    "                if 'k' in hook:\n",
    "                    model.W_K[layer, head] = t.nn.Parameter(prune_wanda(model.W_K[layer, head], X, sparse_ratio=0.5))\n",
    "                \n",
    "                if 'v' in hook:\n",
    "                    model.W_V[layer, head] = t.nn.Parameter(prune_wanda(model.W_V[layer, head], X, sparse_ratio=0.5))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "gpt2_text = \"A quick brown fox jumps over the lazy dog.\"\n",
    "gpt2_tokens = gpt2.to_tokens(gpt2_text)\n",
    "gpt2_logits, gpt2_cache = gpt2.run_with_cache(gpt2_tokens, remove_batch_dim=True)\n",
    "\n",
    "hooks = []\n",
    "for k, v in gpt2_cache.cache_dict.items():\n",
    "    if 'block' in k and (v.dim() == 3 or 'mlp' in k):\n",
    "        if 'score' not in k and 'pattern' not in k:\n",
    "            hooks.append(k)\n",
    "\n",
    "len(hooks)\n",
    "pruned_gpt2 = prune_model(gpt2, hooks)\n",
    "gpt2: sae_lens.HookedSAETransformer = sae_lens.HookedSAETransformer.from_pretrained(\"gpt2-small\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0., device='cuda:0')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.norm(pruned_gpt2.W_Q - gpt2.W_Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized prompt: ['<|endoftext|>', 'I']\n",
      "Tokenized answer: [' am']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"font-weight: bold\">        Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">13.88</span><span style=\"font-weight: bold\"> Prob:  </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4.74</span><span style=\"font-weight: bold\">% Token: | am|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m3\u001b[0m\u001b[1m        Logit: \u001b[0m\u001b[1;36m13.88\u001b[0m\u001b[1m Prob:  \u001b[0m\u001b[1;36m4.74\u001b[0m\u001b[1m% Token: | am|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 14.56 Prob:  9.34% Token: |'m|\n",
      "Top 1th token. Logit: 14.51 Prob:  8.81% Token: |'ve|\n",
      "Top 2th token. Logit: 14.17 Prob:  6.31% Token: | have|\n",
      "Top 3th token. Logit: 13.88 Prob:  4.74% Token: | am|\n",
      "Top 4th token. Logit: 13.88 Prob:  4.69% Token: | was|\n",
      "Top 5th token. Logit: 13.39 Prob:  2.88% Token: | don|\n",
      "Top 6th token. Logit: 13.29 Prob:  2.61% Token: | think|\n",
      "Top 7th token. Logit: 13.15 Prob:  2.27% Token: | had|\n",
      "Top 8th token. Logit: 13.09 Prob:  2.14% Token: | know|\n",
      "Top 9th token. Logit: 12.82 Prob:  1.64% Token: | can|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">' am'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"font-weight: bold\">)]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m' am'\u001b[0m, \u001b[1;36m3\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"I\"\n",
    "answer = \" am\"\n",
    "transformer_lens.utils.test_prompt(prompt, answer, gpt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized prompt: ['<|endoftext|>', 'I']\n",
      "Tokenized answer: [' am']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"font-weight: bold\">        Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">13.88</span><span style=\"font-weight: bold\"> Prob:  </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4.74</span><span style=\"font-weight: bold\">% Token: | am|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m3\u001b[0m\u001b[1m        Logit: \u001b[0m\u001b[1;36m13.88\u001b[0m\u001b[1m Prob:  \u001b[0m\u001b[1;36m4.74\u001b[0m\u001b[1m% Token: | am|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 14.56 Prob:  9.34% Token: |'m|\n",
      "Top 1th token. Logit: 14.51 Prob:  8.81% Token: |'ve|\n",
      "Top 2th token. Logit: 14.17 Prob:  6.31% Token: | have|\n",
      "Top 3th token. Logit: 13.88 Prob:  4.74% Token: | am|\n",
      "Top 4th token. Logit: 13.88 Prob:  4.69% Token: | was|\n",
      "Top 5th token. Logit: 13.39 Prob:  2.88% Token: | don|\n",
      "Top 6th token. Logit: 13.29 Prob:  2.61% Token: | think|\n",
      "Top 7th token. Logit: 13.15 Prob:  2.27% Token: | had|\n",
      "Top 8th token. Logit: 13.09 Prob:  2.14% Token: | know|\n",
      "Top 9th token. Logit: 12.82 Prob:  1.64% Token: | can|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">' am'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"font-weight: bold\">)]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m' am'\u001b[0m, \u001b[1;36m3\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"I\"\n",
    "answer = \" am\"\n",
    "transformer_lens.utils.test_prompt(prompt, answer, pruned_gpt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([11, 12, 64])\n"
     ]
    }
   ],
   "source": [
    "X = gpt2_cache['blocks.0.attn.hook_q']\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embed.W_E torch.Size([50257, 768])\n"
     ]
    }
   ],
   "source": [
    "X = gpt2_cache['blocks.0.attn.hook_q']\n",
    "for name, param in gpt2.named_parameters():\n",
    "    print(name, param.shape)\n",
    "    if name == 'blocks.0.attn.W_Q':\n",
    "        W = param\n",
    "        param = prune_wanda(W, X, sparse_ratio=0.5)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wanda new try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n",
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "def prune_wanda(W, X_norm, sparse_ratio=0.5):\n",
    "    W_metric = W.abs() * X_norm\n",
    "    _, sorted_idx = W_metric.sort(dim=1)\n",
    "    pruned_idx = sorted_idx[:, :int(W.shape[1] * sparse_ratio)]\n",
    "    \n",
    "    W_clone = W.detach().clone()    \n",
    "    W_clone.scatter_(dim=1, index=pruned_idx, src=t.zeros_like(pruned_idx, dtype=W.dtype))\n",
    "    return W_clone\n",
    "\n",
    "def prune_model(model, tokens):\n",
    "    wts_act = {\n",
    "    'attn.W_Q': 'attn.hook_q',\n",
    "    'attn.W_K': 'attn.hook_k',\n",
    "    'attn.W_V': 'attn.hook_v',\n",
    "    'attn.W_O': 'hook_attn_out',\n",
    "    'mlp.W_in': 'mlp.hook_pre',\n",
    "    'mlp.W_out': 'hook_mlp_out'\n",
    "    }\n",
    "    for layer in range(model.cfg.n_layers):\n",
    "        logits, cache = model.run_with_cache(tokens, remove_batch_dim=True)\n",
    "        for wt, act in wts_act.items():\n",
    "            W = model.get_parameter(f'blocks.{layer}.{wt}')\n",
    "            X = cache[f'blocks.{layer}.{act}']\n",
    "\n",
    "            if W.dim() == 3:\n",
    "                if 'W_O' in wt:\n",
    "                    X_norm = X.norm(p=2, dim=0)\n",
    "                    for head in range(W.shape[0]):\n",
    "                        W[head] = prune_wanda(W[head], X_norm, sparse_ratio=0.5)\n",
    "                        \n",
    "                else:\n",
    "                    for head in range(W.shape[0]):\n",
    "                        X_norm = X[:, head, :].norm(p=2, dim=0)\n",
    "                        W[head] = prune_wanda(W[head], X_norm, sparse_ratio=0.5)\n",
    "            else:\n",
    "                X_norm = X.norm(p=2, dim=0)\n",
    "                W = prune_wanda(W, X_norm, sparse_ratio=0.5)\n",
    "            \n",
    "\n",
    "    return model\n",
    "\n",
    "gpt2: sae_lens.HookedSAETransformer = sae_lens.HookedSAETransformer.from_pretrained(\"gpt2-small\", device=device)\n",
    "gpt2_text = \"A quick brown fox jumps over the lazy dog.\"\n",
    "gpt2_tokens = gpt2.to_tokens(gpt2_text)\n",
    "pruned_gpt2 = prune_model(gpt2, gpt2_tokens)\n",
    "gpt2: sae_lens.HookedSAETransformer = sae_lens.HookedSAETransformer.from_pretrained(\"gpt2-small\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized prompt: ['<|endoftext|>', 'I']\n",
      "Tokenized answer: [' am']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"font-weight: bold\">        Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">13.88</span><span style=\"font-weight: bold\"> Prob:  </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4.74</span><span style=\"font-weight: bold\">% Token: | am|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m3\u001b[0m\u001b[1m        Logit: \u001b[0m\u001b[1;36m13.88\u001b[0m\u001b[1m Prob:  \u001b[0m\u001b[1;36m4.74\u001b[0m\u001b[1m% Token: | am|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 14.56 Prob:  9.34% Token: |'m|\n",
      "Top 1th token. Logit: 14.51 Prob:  8.81% Token: |'ve|\n",
      "Top 2th token. Logit: 14.17 Prob:  6.31% Token: | have|\n",
      "Top 3th token. Logit: 13.88 Prob:  4.74% Token: | am|\n",
      "Top 4th token. Logit: 13.88 Prob:  4.69% Token: | was|\n",
      "Top 5th token. Logit: 13.39 Prob:  2.88% Token: | don|\n",
      "Top 6th token. Logit: 13.29 Prob:  2.61% Token: | think|\n",
      "Top 7th token. Logit: 13.15 Prob:  2.27% Token: | had|\n",
      "Top 8th token. Logit: 13.09 Prob:  2.14% Token: | know|\n",
      "Top 9th token. Logit: 12.82 Prob:  1.64% Token: | can|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">' am'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"font-weight: bold\">)]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m' am'\u001b[0m, \u001b[1;36m3\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"I\"\n",
    "answer = \" am\"\n",
    "transformer_lens.utils.test_prompt(prompt, answer, gpt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized prompt: ['<|endoftext|>', 'I']\n",
      "Tokenized answer: [' am']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"font-weight: bold\">        Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">14.87</span><span style=\"font-weight: bold\"> Prob:  </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4.20</span><span style=\"font-weight: bold\">% Token: | am|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m4\u001b[0m\u001b[1m        Logit: \u001b[0m\u001b[1;36m14.87\u001b[0m\u001b[1m Prob:  \u001b[0m\u001b[1;36m4.20\u001b[0m\u001b[1m% Token: | am|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 15.69 Prob:  9.48% Token: | have|\n",
      "Top 1th token. Logit: 15.55 Prob:  8.25% Token: | was|\n",
      "Top 2th token. Logit: 15.42 Prob:  7.22% Token: |'m|\n",
      "Top 3th token. Logit: 15.28 Prob:  6.32% Token: |'ve|\n",
      "Top 4th token. Logit: 14.87 Prob:  4.20% Token: | am|\n",
      "Top 5th token. Logit: 14.68 Prob:  3.46% Token: | think|\n",
      "Top 6th token. Logit: 14.64 Prob:  3.33% Token: | don|\n",
      "Top 7th token. Logit: 14.57 Prob:  3.11% Token: | had|\n",
      "Top 8th token. Logit: 14.46 Prob:  2.78% Token: | know|\n",
      "Top 9th token. Logit: 14.32 Prob:  2.42% Token: | can|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">' am'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"font-weight: bold\">)]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m' am'\u001b[0m, \u001b[1;36m4\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"I\"\n",
    "answer = \" am\"\n",
    "transformer_lens.utils.test_prompt(prompt, answer, pruned_gpt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.3225, device='cuda:0')"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer_lens.evals.sanity_check(gpt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.6407, device='cuda:0')"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer_lens.evals.sanity_check(pruned_gpt2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gemma2\n",
    "\n",
    "Issue: Pretrained SAE is not available for the model - for attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You tried to specify center_unembed=True for a model using logit softcap, but this can't be done! Softcapping is not invariant upon adding a constant Setting center_unembed=False instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d43d88923b19449098cb3f88e93afc93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gemma-2-2b into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "gemma2b: sae_lens.HookedSAETransformer = sae_lens.HookedSAETransformer.from_pretrained(\"gemma-2-2b\", device=device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SAE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import itertools\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "from collections import Counter, defaultdict\n",
    "from copy import deepcopy\n",
    "from dataclasses import dataclass\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "from typing import Any, Callable, Literal, TypeAlias\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch as t\n",
    "from datasets import load_dataset\n",
    "import transformer_lens\n",
    "import sae_lens\n",
    "\n",
    "import einops\n",
    "import circuitsvis as cv\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from IPython.display import HTML, IFrame, clear_output, display\n",
    "from jaxtyping import Float, Int\n",
    "from openai import OpenAI\n",
    "from rich import print as rprint\n",
    "from rich.table import Table\n",
    "from tabulate import tabulate\n",
    "\n",
    "from tqdm import tqdm\n",
    "device = t.device(\"cuda\" if t.cuda.is_available() else \"cpu\")\n",
    "t.set_grad_enabled(False)\n",
    "print(device)\n",
    "\n",
    "# Hugging face: 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\n",
    "# Wandb: 6b549d940e7a29c79c184f27f25606e94a48a966"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n",
      "Tokenized prompt: ['<|endoftext|>', 'I']\n",
      "Tokenized answer: [' am']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"font-weight: bold\">        Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">13.83</span><span style=\"font-weight: bold\"> Prob:  </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3.99</span><span style=\"font-weight: bold\">% Token: | am|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m4\u001b[0m\u001b[1m        Logit: \u001b[0m\u001b[1;36m13.83\u001b[0m\u001b[1m Prob:  \u001b[0m\u001b[1;36m3.99\u001b[0m\u001b[1m% Token: | am|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 14.82 Prob: 10.75% Token: | have|\n",
      "Top 1th token. Logit: 14.21 Prob:  5.82% Token: | was|\n",
      "Top 2th token. Logit: 14.16 Prob:  5.53% Token: |'m|\n",
      "Top 3th token. Logit: 14.06 Prob:  4.99% Token: |'ve|\n",
      "Top 4th token. Logit: 13.83 Prob:  3.99% Token: | am|\n",
      "Top 5th token. Logit: 13.71 Prob:  3.53% Token: | would|\n",
      "Top 6th token. Logit: 13.57 Prob:  3.08% Token: |.|\n",
      "Top 7th token. Logit: 13.47 Prob:  2.78% Token: | will|\n",
      "Top 8th token. Logit: 13.41 Prob:  2.61% Token: | just|\n",
      "Top 9th token. Logit: 13.36 Prob:  2.49% Token: | think|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">' am'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"font-weight: bold\">)]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m' am'\u001b[0m, \u001b[1;36m4\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = sae_lens.HookedSAETransformer.from_pretrained(\"gpt2-small\", device=device)\n",
    "model.load_state_dict(t.load('pruned_gpt2_magnitude.pth'))\n",
    "prompt = \"I\"\n",
    "answer = \" am\"\n",
    "transformer_lens.utils.test_prompt(prompt, answer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n",
      "Tokenized prompt: ['<|endoftext|>', 'I']\n",
      "Tokenized answer: [' am']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"font-weight: bold\">        Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">14.18</span><span style=\"font-weight: bold\"> Prob:  </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4.09</span><span style=\"font-weight: bold\">% Token: | am|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m4\u001b[0m\u001b[1m        Logit: \u001b[0m\u001b[1;36m14.18\u001b[0m\u001b[1m Prob:  \u001b[0m\u001b[1;36m4.09\u001b[0m\u001b[1m% Token: | am|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 14.94 Prob:  8.72% Token: |'m|\n",
      "Top 1th token. Logit: 14.91 Prob:  8.47% Token: | have|\n",
      "Top 2th token. Logit: 14.75 Prob:  7.18% Token: |'ve|\n",
      "Top 3th token. Logit: 14.65 Prob:  6.49% Token: | was|\n",
      "Top 4th token. Logit: 14.18 Prob:  4.09% Token: | am|\n",
      "Top 5th token. Logit: 14.01 Prob:  3.45% Token: | don|\n",
      "Top 6th token. Logit: 13.81 Prob:  2.83% Token: | think|\n",
      "Top 7th token. Logit: 13.80 Prob:  2.78% Token: | know|\n",
      "Top 8th token. Logit: 13.76 Prob:  2.68% Token: | had|\n",
      "Top 9th token. Logit: 13.68 Prob:  2.48% Token: | can|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">' am'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"font-weight: bold\">)]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m' am'\u001b[0m, \u001b[1;36m4\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = sae_lens.HookedSAETransformer.from_pretrained(\"gpt2-small\", device=device)\n",
    "model.load_state_dict(t.load('pruned_gpt2_wanda.pth'))\n",
    "prompt = \"I\"\n",
    "answer = \" am\"\n",
    "transformer_lens.utils.test_prompt(prompt, answer, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HookedSAETransformer(\n",
       "  (embed): Embed()\n",
       "  (hook_embed): HookPoint()\n",
       "  (pos_embed): PosEmbed()\n",
       "  (hook_pos_embed): HookPoint()\n",
       "  (blocks): ModuleList(\n",
       "    (0-11): 12 x TransformerBlock(\n",
       "      (ln1): LayerNormPre(\n",
       "        (hook_scale): HookPoint()\n",
       "        (hook_normalized): HookPoint()\n",
       "      )\n",
       "      (ln2): LayerNormPre(\n",
       "        (hook_scale): HookPoint()\n",
       "        (hook_normalized): HookPoint()\n",
       "      )\n",
       "      (attn): Attention(\n",
       "        (hook_k): HookPoint()\n",
       "        (hook_q): HookPoint()\n",
       "        (hook_v): HookPoint()\n",
       "        (hook_z): HookPoint()\n",
       "        (hook_attn_scores): HookPoint()\n",
       "        (hook_pattern): HookPoint()\n",
       "        (hook_result): HookPoint()\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (hook_pre): HookPoint()\n",
       "        (hook_post): HookPoint()\n",
       "      )\n",
       "      (hook_attn_in): HookPoint()\n",
       "      (hook_q_input): HookPoint()\n",
       "      (hook_k_input): HookPoint()\n",
       "      (hook_v_input): HookPoint()\n",
       "      (hook_mlp_in): HookPoint()\n",
       "      (hook_attn_out): HookPoint()\n",
       "      (hook_mlp_out): HookPoint()\n",
       "      (hook_resid_pre): HookPoint()\n",
       "      (hook_resid_mid): HookPoint()\n",
       "      (hook_resid_post): HookPoint()\n",
       "    )\n",
       "  )\n",
       "  (ln_final): LayerNormPre(\n",
       "    (hook_scale): HookPoint()\n",
       "    (hook_normalized): HookPoint()\n",
       "  )\n",
       "  (unembed): Unembed()\n",
       ")"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt2: sae_lens.HookedSAETransformer = sae_lens.HookedSAETransformer.from_pretrained(\"gpt2-small\", device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attention SAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9c376168d144dd2a66ce89f52b0ebcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)-06_rsanthropic_rie25000_nr4_v9_cfg.json:   0%|          | 0.00/1.01k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42d68d40f9af4b46a318cd5c3d8076b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)c1.00e-06_rsanthropic_rie25000_nr4_v9.pt:   0%|          | 0.00/151M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e97449dd52141f6be96b905c15252a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)-06_rsanthropic_rie25000_nr4_v5_cfg.json:   0%|          | 0.00/1.02k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9cf1b87c313484e97c086386fd63a22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)c1.00e-06_rsanthropic_rie25000_nr4_v5.pt:   0%|          | 0.00/151M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "702494c611814cdc92a1bfeccfdbb503",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)-06_rsanthropic_rie25000_nr4_v4_cfg.json:   0%|          | 0.00/1.02k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "808ede7b9d6e4fdd9f015ba113ae8ce1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)c1.00e-06_rsanthropic_rie25000_nr4_v4.pt:   0%|          | 0.00/151M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9bf62c642c5436d9afca4507a47f15d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)-06_rsanthropic_rie25000_nr4_v9_cfg.json:   0%|          | 0.00/1.01k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d10d047494b44a4789f5383f8460f645",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)c1.00e-06_rsanthropic_rie25000_nr4_v9.pt:   0%|          | 0.00/151M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6821808b8ca4b5ba49de4e9e2f1f430",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)-06_rsanthropic_rie25000_nr4_v7_cfg.json:   0%|          | 0.00/1.01k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e0061cbdb14435496e9b8c9e4fb3bd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)c1.00e-06_rsanthropic_rie25000_nr4_v7.pt:   0%|          | 0.00/151M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2f366e1c4c44c13b52cf0249699be19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)-06_rsanthropic_rie25000_nr4_v9_cfg.json:   0%|          | 0.00/1.02k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35fe80cd4cdd4c80acae239d5dfca16e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)c1.00e-06_rsanthropic_rie25000_nr4_v9.pt:   0%|          | 0.00/302M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f614d87176e4d6da817699e5726bfb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)-06_rsanthropic_rie25000_nr4_v9_cfg.json:   0%|          | 0.00/1.01k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0256fa7f236340a08177b2493b549abf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)c1.00e-06_rsanthropic_rie25000_nr4_v9.pt:   0%|          | 0.00/151M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ef7abf2015249d8bf1e0241e65d4ae4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)-06_rsanthropic_rie25000_nr4_v9_cfg.json:   0%|          | 0.00/1.01k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e5639bc3e8641e9a746ca6fa09b349e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)c1.00e-06_rsanthropic_rie25000_nr4_v9.pt:   0%|          | 0.00/302M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64bf1f088e384feaa0c5078e1399a76c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)-05_rsanthropic_rie25000_nr4_v6_cfg.json:   0%|          | 0.00/1.02k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f8bc030aede474e945372ba03890c59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)c1.00e-05_rsanthropic_rie25000_nr4_v6.pt:   0%|          | 0.00/151M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0917c45b0f8a4acb8c0674116c633006",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)-06_rsanthropic_rie25000_nr4_v9_cfg.json:   0%|          | 0.00/1.01k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "357a64ee28ed45d4ac2934efb947429b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)c1.00e-06_rsanthropic_rie25000_nr4_v9.pt:   0%|          | 0.00/151M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5655517736e54bbaa4aef090695e8474",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)-05_rsanthropic_rie25000_nr4_v9_cfg.json:   0%|          | 0.00/1.02k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcf699835bc94188be87abd319ab5597",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)c1.00e-05_rsanthropic_rie25000_nr4_v9.pt:   0%|          | 0.00/151M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb58f319904a4482ad2e1e2d283ffae1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)-06_rsanthropic_rie25000_nr4_v9_cfg.json:   0%|          | 0.00/1.03k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80ac1abd619b44979a4c87935778cd40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)c3.16e-06_rsanthropic_rie25000_nr4_v9.pt:   0%|          | 0.00/151M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "attn_saes = {\n",
    "    layer: sae_lens.SAE.from_pretrained(\n",
    "        \"gpt2-small-hook-z-kk\",\n",
    "        f\"blocks.{layer}.hook_z\",\n",
    "        device=device,\n",
    "    )[0]\n",
    "    for layer in range(gpt2.cfg.n_layers)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: SAE(\n",
      "  (activation_fn): ReLU()\n",
      "  (hook_sae_input): HookPoint()\n",
      "  (hook_sae_acts_pre): HookPoint()\n",
      "  (hook_sae_acts_post): HookPoint()\n",
      "  (hook_sae_output): HookPoint()\n",
      "  (hook_sae_recons): HookPoint()\n",
      "  (hook_sae_error): HookPoint()\n",
      "), 1: SAE(\n",
      "  (activation_fn): ReLU()\n",
      "  (hook_sae_input): HookPoint()\n",
      "  (hook_sae_acts_pre): HookPoint()\n",
      "  (hook_sae_acts_post): HookPoint()\n",
      "  (hook_sae_output): HookPoint()\n",
      "  (hook_sae_recons): HookPoint()\n",
      "  (hook_sae_error): HookPoint()\n",
      "), 2: SAE(\n",
      "  (activation_fn): ReLU()\n",
      "  (hook_sae_input): HookPoint()\n",
      "  (hook_sae_acts_pre): HookPoint()\n",
      "  (hook_sae_acts_post): HookPoint()\n",
      "  (hook_sae_output): HookPoint()\n",
      "  (hook_sae_recons): HookPoint()\n",
      "  (hook_sae_error): HookPoint()\n",
      "), 3: SAE(\n",
      "  (activation_fn): ReLU()\n",
      "  (hook_sae_input): HookPoint()\n",
      "  (hook_sae_acts_pre): HookPoint()\n",
      "  (hook_sae_acts_post): HookPoint()\n",
      "  (hook_sae_output): HookPoint()\n",
      "  (hook_sae_recons): HookPoint()\n",
      "  (hook_sae_error): HookPoint()\n",
      "), 4: SAE(\n",
      "  (activation_fn): ReLU()\n",
      "  (hook_sae_input): HookPoint()\n",
      "  (hook_sae_acts_pre): HookPoint()\n",
      "  (hook_sae_acts_post): HookPoint()\n",
      "  (hook_sae_output): HookPoint()\n",
      "  (hook_sae_recons): HookPoint()\n",
      "  (hook_sae_error): HookPoint()\n",
      "), 5: SAE(\n",
      "  (activation_fn): ReLU()\n",
      "  (hook_sae_input): HookPoint()\n",
      "  (hook_sae_acts_pre): HookPoint()\n",
      "  (hook_sae_acts_post): HookPoint()\n",
      "  (hook_sae_output): HookPoint()\n",
      "  (hook_sae_recons): HookPoint()\n",
      "  (hook_sae_error): HookPoint()\n",
      "), 6: SAE(\n",
      "  (activation_fn): ReLU()\n",
      "  (hook_sae_input): HookPoint()\n",
      "  (hook_sae_acts_pre): HookPoint()\n",
      "  (hook_sae_acts_post): HookPoint()\n",
      "  (hook_sae_output): HookPoint()\n",
      "  (hook_sae_recons): HookPoint()\n",
      "  (hook_sae_error): HookPoint()\n",
      "), 7: SAE(\n",
      "  (activation_fn): ReLU()\n",
      "  (hook_sae_input): HookPoint()\n",
      "  (hook_sae_acts_pre): HookPoint()\n",
      "  (hook_sae_acts_post): HookPoint()\n",
      "  (hook_sae_output): HookPoint()\n",
      "  (hook_sae_recons): HookPoint()\n",
      "  (hook_sae_error): HookPoint()\n",
      "), 8: SAE(\n",
      "  (activation_fn): ReLU()\n",
      "  (hook_sae_input): HookPoint()\n",
      "  (hook_sae_acts_pre): HookPoint()\n",
      "  (hook_sae_acts_post): HookPoint()\n",
      "  (hook_sae_output): HookPoint()\n",
      "  (hook_sae_recons): HookPoint()\n",
      "  (hook_sae_error): HookPoint()\n",
      "), 9: SAE(\n",
      "  (activation_fn): ReLU()\n",
      "  (hook_sae_input): HookPoint()\n",
      "  (hook_sae_acts_pre): HookPoint()\n",
      "  (hook_sae_acts_post): HookPoint()\n",
      "  (hook_sae_output): HookPoint()\n",
      "  (hook_sae_recons): HookPoint()\n",
      "  (hook_sae_error): HookPoint()\n",
      "), 10: SAE(\n",
      "  (activation_fn): ReLU()\n",
      "  (hook_sae_input): HookPoint()\n",
      "  (hook_sae_acts_pre): HookPoint()\n",
      "  (hook_sae_acts_post): HookPoint()\n",
      "  (hook_sae_output): HookPoint()\n",
      "  (hook_sae_recons): HookPoint()\n",
      "  (hook_sae_error): HookPoint()\n",
      "), 11: SAE(\n",
      "  (activation_fn): ReLU()\n",
      "  (hook_sae_input): HookPoint()\n",
      "  (hook_sae_acts_pre): HookPoint()\n",
      "  (hook_sae_acts_post): HookPoint()\n",
      "  (hook_sae_output): HookPoint()\n",
      "  (hook_sae_recons): HookPoint()\n",
      "  (hook_sae_error): HookPoint()\n",
      ")}\n"
     ]
    }
   ],
   "source": [
    "print(attn_saes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://neuronpedia.org/gpt2-small/9-att-kk/2?embed=true&embedexplanation=true&embedplots=true&embedtest=true&height=300\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"800\"\n",
       "            height=\"600\"\n",
       "            src=\"https://neuronpedia.org/gpt2-small/9-att-kk/2?embed=true&embedexplanation=true&embedplots=true&embedtest=true&height=300\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7ef9b05566e0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def display_dashboard(\n",
    "    sae_release=\"gpt2-small-res-jb\",\n",
    "    sae_id=\"blocks.7.hook_resid_pre\",\n",
    "    latent_idx=0,\n",
    "    width=800,\n",
    "    height=600,\n",
    "):\n",
    "    release = sae_lens.toolkit.pretrained_saes_directory.get_pretrained_saes_directory()[sae_release]\n",
    "    neuronpedia_id = release.neuronpedia_id[sae_id]\n",
    "\n",
    "    url = f\"https://neuronpedia.org/{neuronpedia_id}/{latent_idx}?embed=true&embedexplanation=true&embedplots=true&embedtest=true&height=300\"\n",
    "\n",
    "    print(url)\n",
    "    display(IFrame(url, width=width, height=height))\n",
    "\n",
    "\n",
    "layer = 9\n",
    "\n",
    "display_dashboard(\n",
    "    sae_release=\"gpt2-small-hook-z-kk\",\n",
    "    sae_id=f\"blocks.{layer}.hook_z\",\n",
    "    latent_idx=2,  # or you can try `random.randint(0, attn_saes[layer].cfg.d_sae)`\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HookedTransformerConfig:\n",
       "{'NTK_by_parts_factor': 8.0,\n",
       " 'NTK_by_parts_high_freq_factor': 4.0,\n",
       " 'NTK_by_parts_low_freq_factor': 1.0,\n",
       " 'act_fn': 'gelu_new',\n",
       " 'attention_dir': 'causal',\n",
       " 'attn_only': False,\n",
       " 'attn_scale': 8.0,\n",
       " 'attn_scores_soft_cap': -1.0,\n",
       " 'attn_types': None,\n",
       " 'checkpoint_index': None,\n",
       " 'checkpoint_label_type': None,\n",
       " 'checkpoint_value': None,\n",
       " 'd_head': 64,\n",
       " 'd_mlp': 3072,\n",
       " 'd_model': 768,\n",
       " 'd_vocab': 50257,\n",
       " 'd_vocab_out': 50257,\n",
       " 'decoder_start_token_id': None,\n",
       " 'default_prepend_bos': True,\n",
       " 'device': device(type='cuda'),\n",
       " 'dtype': torch.float32,\n",
       " 'eps': 1e-05,\n",
       " 'experts_per_token': None,\n",
       " 'final_rms': False,\n",
       " 'from_checkpoint': False,\n",
       " 'gated_mlp': False,\n",
       " 'init_mode': 'gpt2',\n",
       " 'init_weights': False,\n",
       " 'initializer_range': 0.02886751345948129,\n",
       " 'load_in_4bit': False,\n",
       " 'model_name': 'gpt2',\n",
       " 'n_ctx': 1024,\n",
       " 'n_devices': 1,\n",
       " 'n_heads': 12,\n",
       " 'n_key_value_heads': None,\n",
       " 'n_layers': 12,\n",
       " 'n_params': 84934656,\n",
       " 'normalization_type': 'LNPre',\n",
       " 'num_experts': None,\n",
       " 'original_architecture': 'GPT2LMHeadModel',\n",
       " 'output_logits_soft_cap': -1.0,\n",
       " 'parallel_attn_mlp': False,\n",
       " 'positional_embedding_type': 'standard',\n",
       " 'post_embedding_ln': False,\n",
       " 'relative_attention_max_distance': None,\n",
       " 'relative_attention_num_buckets': None,\n",
       " 'rotary_adjacent_pairs': False,\n",
       " 'rotary_base': 10000,\n",
       " 'rotary_dim': None,\n",
       " 'scale_attn_by_inverse_layer_idx': False,\n",
       " 'seed': None,\n",
       " 'tie_word_embeddings': False,\n",
       " 'tokenizer_name': 'gpt2',\n",
       " 'tokenizer_prepends_bos': False,\n",
       " 'trust_remote_code': False,\n",
       " 'ungroup_grouped_query_attention': False,\n",
       " 'use_NTK_by_parts_rope': False,\n",
       " 'use_attn_in': False,\n",
       " 'use_attn_result': False,\n",
       " 'use_attn_scale': True,\n",
       " 'use_hook_mlp_in': False,\n",
       " 'use_hook_tokens': False,\n",
       " 'use_local_attn': False,\n",
       " 'use_normalization_before_and_after': False,\n",
       " 'use_split_qkv_input': False,\n",
       " 'window_size': None}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt2.cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Magnitude pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n",
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "def prune_magnitude(W, sparse_ratio=0.5):\n",
    "    W_abs = W.abs()\n",
    "    k = int(W_abs.numel() * sparse_ratio)\n",
    "    _, indices = W_abs.view(-1).topk(k)\n",
    "    mask = t.zeros_like(W_abs)\n",
    "    mask.view(-1)[indices] = 1\n",
    "    return mask*W\n",
    "\n",
    "def prune_model(model):\n",
    "    wts = ['W_Q', 'W_K', 'W_V', 'W_O', 'W_in', 'W_out']\n",
    "    for name, param in gpt2.named_parameters():\n",
    "        # print(f\"Layer: {name}, Shape: {param.shape}\")\n",
    "        if name.split('.')[-1] in wts:\n",
    "            if param.dim() == 3:    \n",
    "                for i in range(param.shape[0]):\n",
    "                    param[i] = prune_magnitude(param[i])\n",
    "            else:\n",
    "                param = prune_magnitude(param)\n",
    "    \n",
    "    return model\n",
    "\n",
    "gpt2: sae_lens.HookedSAETransformer = sae_lens.HookedSAETransformer.from_pretrained(\"gpt2-small\", device=device)\n",
    "pruned_gpt2 = prune_model(gpt2)\n",
    "gpt2: sae_lens.HookedSAETransformer = sae_lens.HookedSAETransformer.from_pretrained(\"gpt2-small\", device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized prompt: ['<|endoftext|>', 'I']\n",
      "Tokenized answer: [' am']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"font-weight: bold\">        Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">13.88</span><span style=\"font-weight: bold\"> Prob:  </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4.74</span><span style=\"font-weight: bold\">% Token: | am|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m3\u001b[0m\u001b[1m        Logit: \u001b[0m\u001b[1;36m13.88\u001b[0m\u001b[1m Prob:  \u001b[0m\u001b[1;36m4.74\u001b[0m\u001b[1m% Token: | am|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 14.56 Prob:  9.34% Token: |'m|\n",
      "Top 1th token. Logit: 14.51 Prob:  8.81% Token: |'ve|\n",
      "Top 2th token. Logit: 14.17 Prob:  6.31% Token: | have|\n",
      "Top 3th token. Logit: 13.88 Prob:  4.74% Token: | am|\n",
      "Top 4th token. Logit: 13.88 Prob:  4.69% Token: | was|\n",
      "Top 5th token. Logit: 13.39 Prob:  2.88% Token: | don|\n",
      "Top 6th token. Logit: 13.29 Prob:  2.61% Token: | think|\n",
      "Top 7th token. Logit: 13.15 Prob:  2.27% Token: | had|\n",
      "Top 8th token. Logit: 13.09 Prob:  2.14% Token: | know|\n",
      "Top 9th token. Logit: 12.82 Prob:  1.64% Token: | can|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">' am'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"font-weight: bold\">)]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m' am'\u001b[0m, \u001b[1;36m3\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"I\"\n",
    "answer = \" am\"\n",
    "transformer_lens.utils.test_prompt(prompt, answer, gpt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized prompt: ['<|endoftext|>', 'I']\n",
      "Tokenized answer: [' am']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"font-weight: bold\">        Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">13.83</span><span style=\"font-weight: bold\"> Prob:  </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3.99</span><span style=\"font-weight: bold\">% Token: | am|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m4\u001b[0m\u001b[1m        Logit: \u001b[0m\u001b[1;36m13.83\u001b[0m\u001b[1m Prob:  \u001b[0m\u001b[1;36m3.99\u001b[0m\u001b[1m% Token: | am|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 14.82 Prob: 10.75% Token: | have|\n",
      "Top 1th token. Logit: 14.21 Prob:  5.82% Token: | was|\n",
      "Top 2th token. Logit: 14.16 Prob:  5.53% Token: |'m|\n",
      "Top 3th token. Logit: 14.06 Prob:  4.99% Token: |'ve|\n",
      "Top 4th token. Logit: 13.83 Prob:  3.99% Token: | am|\n",
      "Top 5th token. Logit: 13.71 Prob:  3.53% Token: | would|\n",
      "Top 6th token. Logit: 13.57 Prob:  3.08% Token: |.|\n",
      "Top 7th token. Logit: 13.47 Prob:  2.78% Token: | will|\n",
      "Top 8th token. Logit: 13.41 Prob:  2.61% Token: | just|\n",
      "Top 9th token. Logit: 13.36 Prob:  2.49% Token: | think|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">' am'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"font-weight: bold\">)]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m' am'\u001b[0m, \u001b[1;36m4\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"I\"\n",
    "answer = \" am\"\n",
    "transformer_lens.utils.test_prompt(prompt, answer, pruned_gpt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.3225, device='cuda:0')"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer_lens.evals.sanity_check(gpt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.8446, device='cuda:0')"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer_lens.evals.sanity_check(pruned_gpt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized prompt: ['<|endoftext|>', 'After', ' John', ' and', ' Mary', ' went', ' to', ' the', ' store', ',', ' John', ' gave', ' a', ' bottle', ' of', ' milk', ' to']\n",
      "Tokenized answer: [' Mary']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">        Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">18.09</span><span style=\"font-weight: bold\"> Prob: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">70.07</span><span style=\"font-weight: bold\">% Token: | Mary|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m0\u001b[0m\u001b[1m        Logit: \u001b[0m\u001b[1;36m18.09\u001b[0m\u001b[1m Prob: \u001b[0m\u001b[1;36m70.07\u001b[0m\u001b[1m% Token: | Mary|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 18.09 Prob: 70.07% Token: | Mary|\n",
      "Top 1th token. Logit: 15.38 Prob:  4.67% Token: | the|\n",
      "Top 2th token. Logit: 15.35 Prob:  4.54% Token: | John|\n",
      "Top 3th token. Logit: 15.25 Prob:  4.11% Token: | them|\n",
      "Top 4th token. Logit: 14.84 Prob:  2.73% Token: | his|\n",
      "Top 5th token. Logit: 14.06 Prob:  1.24% Token: | her|\n",
      "Top 6th token. Logit: 13.54 Prob:  0.74% Token: | a|\n",
      "Top 7th token. Logit: 13.52 Prob:  0.73% Token: | their|\n",
      "Top 8th token. Logit: 13.13 Prob:  0.49% Token: | Jesus|\n",
      "Top 9th token. Logit: 12.97 Prob:  0.42% Token: | him|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">' Mary'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">)]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m' Mary'\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "example_prompt = \"After John and Mary went to the store, John gave a bottle of milk to\"\n",
    "example_answer = \" Mary\"\n",
    "transformer_lens.utils.test_prompt(example_prompt, example_answer, gpt2, prepend_bos=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized prompt: ['<|endoftext|>', 'After', ' John', ' and', ' Mary', ' went', ' to', ' the', ' store', ',', ' John', ' gave', ' a', ' bottle', ' of', ' milk', ' to']\n",
      "Tokenized answer: [' Mary']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"font-weight: bold\">        Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">15.49</span><span style=\"font-weight: bold\"> Prob:  </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3.86</span><span style=\"font-weight: bold\">% Token: | Mary|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m3\u001b[0m\u001b[1m        Logit: \u001b[0m\u001b[1;36m15.49\u001b[0m\u001b[1m Prob:  \u001b[0m\u001b[1;36m3.86\u001b[0m\u001b[1m% Token: | Mary|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 17.59 Prob: 31.61% Token: | the|\n",
      "Top 1th token. Logit: 17.12 Prob: 19.72% Token: | a|\n",
      "Top 2th token. Logit: 16.68 Prob: 12.66% Token: | his|\n",
      "Top 3th token. Logit: 15.49 Prob:  3.86% Token: | Mary|\n",
      "Top 4th token. Logit: 15.43 Prob:  3.64% Token: | one|\n",
      "Top 5th token. Logit: 15.00 Prob:  2.36% Token: | her|\n",
      "Top 6th token. Logit: 14.71 Prob:  1.78% Token: | their|\n",
      "Top 7th token. Logit: 14.47 Prob:  1.40% Token: | an|\n",
      "Top 8th token. Logit: 14.23 Prob:  1.10% Token: | them|\n",
      "Top 9th token. Logit: 14.11 Prob:  0.97% Token: | another|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">' Mary'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"font-weight: bold\">)]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m' Mary'\u001b[0m, \u001b[1;36m3\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "example_prompt = \"After John and Mary went to the store, John gave a bottle of milk to\"\n",
    "example_answer = \" Mary\"\n",
    "transformer_lens.utils.test_prompt(example_prompt, example_answer, pruned_gpt2, prepend_bos=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wanda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HookedTransformerConfig:\n",
       "{'NTK_by_parts_factor': 8.0,\n",
       " 'NTK_by_parts_high_freq_factor': 4.0,\n",
       " 'NTK_by_parts_low_freq_factor': 1.0,\n",
       " 'act_fn': 'gelu_new',\n",
       " 'attention_dir': 'causal',\n",
       " 'attn_only': False,\n",
       " 'attn_scale': 8.0,\n",
       " 'attn_scores_soft_cap': -1.0,\n",
       " 'attn_types': None,\n",
       " 'checkpoint_index': None,\n",
       " 'checkpoint_label_type': None,\n",
       " 'checkpoint_value': None,\n",
       " 'd_head': 64,\n",
       " 'd_mlp': 3072,\n",
       " 'd_model': 768,\n",
       " 'd_vocab': 50257,\n",
       " 'd_vocab_out': 50257,\n",
       " 'decoder_start_token_id': None,\n",
       " 'default_prepend_bos': True,\n",
       " 'device': device(type='cuda'),\n",
       " 'dtype': torch.float32,\n",
       " 'eps': 1e-05,\n",
       " 'experts_per_token': None,\n",
       " 'final_rms': False,\n",
       " 'from_checkpoint': False,\n",
       " 'gated_mlp': False,\n",
       " 'init_mode': 'gpt2',\n",
       " 'init_weights': False,\n",
       " 'initializer_range': 0.02886751345948129,\n",
       " 'load_in_4bit': False,\n",
       " 'model_name': 'gpt2',\n",
       " 'n_ctx': 1024,\n",
       " 'n_devices': 1,\n",
       " 'n_heads': 12,\n",
       " 'n_key_value_heads': None,\n",
       " 'n_layers': 12,\n",
       " 'n_params': 84934656,\n",
       " 'normalization_type': 'LNPre',\n",
       " 'num_experts': None,\n",
       " 'original_architecture': 'GPT2LMHeadModel',\n",
       " 'output_logits_soft_cap': -1.0,\n",
       " 'parallel_attn_mlp': False,\n",
       " 'positional_embedding_type': 'standard',\n",
       " 'post_embedding_ln': False,\n",
       " 'relative_attention_max_distance': None,\n",
       " 'relative_attention_num_buckets': None,\n",
       " 'rotary_adjacent_pairs': False,\n",
       " 'rotary_base': 10000,\n",
       " 'rotary_dim': None,\n",
       " 'scale_attn_by_inverse_layer_idx': False,\n",
       " 'seed': None,\n",
       " 'tie_word_embeddings': False,\n",
       " 'tokenizer_name': 'gpt2',\n",
       " 'tokenizer_prepends_bos': False,\n",
       " 'trust_remote_code': False,\n",
       " 'ungroup_grouped_query_attention': False,\n",
       " 'use_NTK_by_parts_rope': False,\n",
       " 'use_attn_in': False,\n",
       " 'use_attn_result': False,\n",
       " 'use_attn_scale': True,\n",
       " 'use_hook_mlp_in': False,\n",
       " 'use_hook_tokens': False,\n",
       " 'use_local_attn': False,\n",
       " 'use_normalization_before_and_after': False,\n",
       " 'use_split_qkv_input': False,\n",
       " 'window_size': None}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt2.cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 4,  0,  1,  0],\n",
       "        [ 0,  0, -1, -3],\n",
       "        [-3,  0,  0,  2]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def prune_wanda(W, X_norm, sparse_ratio=0.5):\n",
    "    W_metric = W.abs() * X_norm\n",
    "    _, sorted_idx = W_metric.sort(dim=1)\n",
    "    pruned_idx = sorted_idx[:, :int(W.shape[1] * sparse_ratio)]\n",
    "    \n",
    "    W_clone = W.detach().clone()    \n",
    "    W_clone.scatter_(dim=1, index=pruned_idx, src=t.zeros_like(pruned_idx, dtype=W.dtype))\n",
    "    return W_clone\n",
    "\n",
    "W = t.tensor([\n",
    "    [4, 0, 1, -1],\n",
    "    [3, -2, -1, -3],\n",
    "    [-3, 1, 0, 2]\n",
    "])\n",
    "X = t.tensor([\n",
    "    [1, 2, 8, 3]\n",
    "])\n",
    "\n",
    "prune_wanda(W, X, sparse_ratio=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_model(model, hooks):\n",
    "    text = \"A quick brown fox jumps over the lazy dog.\"\n",
    "    tokens = model.to_tokens(text)\n",
    "    logits, cache = model.run_with_cache(tokens, remove_batch_dim=True)\n",
    "\n",
    "    for hook in hooks:\n",
    "        logits, cache = model.run_with_cache(tokens, remove_batch_dim=True)\n",
    "        layer = int(hook.split('.')[1])\n",
    "\n",
    "        if 'mlp' in hook:\n",
    "            if 'out' in hook:\n",
    "                X = cache[hook].norm(p=2, dim=0)\n",
    "                model.W_out[layer] = t.nn.Parameter(prune_wanda(model.W_out[layer], X, sparse_ratio=0.5))\n",
    "            else:\n",
    "                X = cache[hook]\n",
    "                X = X.norm(p=2, dim=0)\n",
    "                model.W_in[layer] = t.nn.Parameter(prune_wanda(model.W_in[layer], X, sparse_ratio=0.5))\n",
    "        else:\n",
    "            for head in range(model.cfg.n_heads):\n",
    "                X = cache[hook][:, head, :]  \n",
    "                X = X.norm(p=2, dim=0)\n",
    "\n",
    "                if 'q' in hook:\n",
    "                    model.W_Q[layer, head] = t.nn.Parameter(prune_wanda(model.W_Q[layer, head], X, sparse_ratio=0.5)\n",
    ")\n",
    "                if 'k' in hook:\n",
    "                    model.W_K[layer, head] = t.nn.Parameter(prune_wanda(model.W_K[layer, head], X, sparse_ratio=0.5))\n",
    "                \n",
    "                if 'v' in hook:\n",
    "                    model.W_V[layer, head] = t.nn.Parameter(prune_wanda(model.W_V[layer, head], X, sparse_ratio=0.5))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "gpt2_text = \"A quick brown fox jumps over the lazy dog.\"\n",
    "gpt2_tokens = gpt2.to_tokens(gpt2_text)\n",
    "gpt2_logits, gpt2_cache = gpt2.run_with_cache(gpt2_tokens, remove_batch_dim=True)\n",
    "\n",
    "hooks = []\n",
    "for k, v in gpt2_cache.cache_dict.items():\n",
    "    if 'block' in k and (v.dim() == 3 or 'mlp' in k):\n",
    "        if 'score' not in k and 'pattern' not in k:\n",
    "            hooks.append(k)\n",
    "\n",
    "len(hooks)\n",
    "pruned_gpt2 = prune_model(gpt2, hooks)\n",
    "gpt2: sae_lens.HookedSAETransformer = sae_lens.HookedSAETransformer.from_pretrained(\"gpt2-small\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0., device='cuda:0')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.norm(pruned_gpt2.W_Q - gpt2.W_Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized prompt: ['<|endoftext|>', 'I']\n",
      "Tokenized answer: [' am']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"font-weight: bold\">        Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">13.88</span><span style=\"font-weight: bold\"> Prob:  </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4.74</span><span style=\"font-weight: bold\">% Token: | am|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m3\u001b[0m\u001b[1m        Logit: \u001b[0m\u001b[1;36m13.88\u001b[0m\u001b[1m Prob:  \u001b[0m\u001b[1;36m4.74\u001b[0m\u001b[1m% Token: | am|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 14.56 Prob:  9.34% Token: |'m|\n",
      "Top 1th token. Logit: 14.51 Prob:  8.81% Token: |'ve|\n",
      "Top 2th token. Logit: 14.17 Prob:  6.31% Token: | have|\n",
      "Top 3th token. Logit: 13.88 Prob:  4.74% Token: | am|\n",
      "Top 4th token. Logit: 13.88 Prob:  4.69% Token: | was|\n",
      "Top 5th token. Logit: 13.39 Prob:  2.88% Token: | don|\n",
      "Top 6th token. Logit: 13.29 Prob:  2.61% Token: | think|\n",
      "Top 7th token. Logit: 13.15 Prob:  2.27% Token: | had|\n",
      "Top 8th token. Logit: 13.09 Prob:  2.14% Token: | know|\n",
      "Top 9th token. Logit: 12.82 Prob:  1.64% Token: | can|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">' am'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"font-weight: bold\">)]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m' am'\u001b[0m, \u001b[1;36m3\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"I\"\n",
    "answer = \" am\"\n",
    "transformer_lens.utils.test_prompt(prompt, answer, gpt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized prompt: ['<|endoftext|>', 'I']\n",
      "Tokenized answer: [' am']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"font-weight: bold\">        Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">13.88</span><span style=\"font-weight: bold\"> Prob:  </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4.74</span><span style=\"font-weight: bold\">% Token: | am|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m3\u001b[0m\u001b[1m        Logit: \u001b[0m\u001b[1;36m13.88\u001b[0m\u001b[1m Prob:  \u001b[0m\u001b[1;36m4.74\u001b[0m\u001b[1m% Token: | am|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 14.56 Prob:  9.34% Token: |'m|\n",
      "Top 1th token. Logit: 14.51 Prob:  8.81% Token: |'ve|\n",
      "Top 2th token. Logit: 14.17 Prob:  6.31% Token: | have|\n",
      "Top 3th token. Logit: 13.88 Prob:  4.74% Token: | am|\n",
      "Top 4th token. Logit: 13.88 Prob:  4.69% Token: | was|\n",
      "Top 5th token. Logit: 13.39 Prob:  2.88% Token: | don|\n",
      "Top 6th token. Logit: 13.29 Prob:  2.61% Token: | think|\n",
      "Top 7th token. Logit: 13.15 Prob:  2.27% Token: | had|\n",
      "Top 8th token. Logit: 13.09 Prob:  2.14% Token: | know|\n",
      "Top 9th token. Logit: 12.82 Prob:  1.64% Token: | can|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">' am'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"font-weight: bold\">)]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m' am'\u001b[0m, \u001b[1;36m3\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"I\"\n",
    "answer = \" am\"\n",
    "transformer_lens.utils.test_prompt(prompt, answer, pruned_gpt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([11, 12, 64])\n"
     ]
    }
   ],
   "source": [
    "X = gpt2_cache['blocks.0.attn.hook_q']\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embed.W_E torch.Size([50257, 768])\n"
     ]
    }
   ],
   "source": [
    "X = gpt2_cache['blocks.0.attn.hook_q']\n",
    "for name, param in gpt2.named_parameters():\n",
    "    print(name, param.shape)\n",
    "    if name == 'blocks.0.attn.W_Q':\n",
    "        W = param\n",
    "        param = prune_wanda(W, X, sparse_ratio=0.5)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wanda new try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n",
      "torch.Size([1, 11])\n",
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "def prune_wanda(W, X_norm, sparse_ratio=0.5):\n",
    "    W_metric = W.abs() * X_norm\n",
    "    _, sorted_idx = W_metric.sort(dim=1)\n",
    "    pruned_idx = sorted_idx[:, :int(W.shape[1] * sparse_ratio)]\n",
    "    \n",
    "    W_clone = W.detach().clone()    \n",
    "    W_clone.scatter_(dim=1, index=pruned_idx, src=t.zeros_like(pruned_idx, dtype=W.dtype))\n",
    "    return W_clone\n",
    "\n",
    "def prune_model(model, tokens):\n",
    "    wts_act = {\n",
    "    'attn.W_Q': 'attn.hook_q',\n",
    "    'attn.W_K': 'attn.hook_k',\n",
    "    'attn.W_V': 'attn.hook_v',\n",
    "    'attn.W_O': 'hook_attn_out',\n",
    "    'mlp.W_in': 'mlp.hook_pre',\n",
    "    'mlp.W_out': 'hook_mlp_out'\n",
    "    }\n",
    "    for layer in range(model.cfg.n_layers):\n",
    "        logits, cache = model.run_with_cache(tokens, remove_batch_dim=True)\n",
    "        for wt, act in wts_act.items():\n",
    "            W = model.get_parameter(f'blocks.{layer}.{wt}')\n",
    "            X = cache[f'blocks.{layer}.{act}']\n",
    "\n",
    "            if W.dim() == 3:\n",
    "                if 'W_O' in wt:\n",
    "                    X_norm = X.norm(p=2, dim=0)\n",
    "                    for head in range(W.shape[0]):\n",
    "                        W[head] = prune_wanda(W[head], X_norm, sparse_ratio=0.5)\n",
    "                        \n",
    "                else:\n",
    "                    for head in range(W.shape[0]):\n",
    "                        X_norm = X[:, head, :].norm(p=2, dim=0)\n",
    "                        W[head] = prune_wanda(W[head], X_norm, sparse_ratio=0.5)\n",
    "            else:\n",
    "                X_norm = X.norm(p=2, dim=0)\n",
    "                W = prune_wanda(W, X_norm, sparse_ratio=0.5)\n",
    "            \n",
    "    return model\n",
    "\n",
    "gpt2: sae_lens.HookedSAETransformer = sae_lens.HookedSAETransformer.from_pretrained(\"gpt2-small\", device=device)\n",
    "gpt2_text = \"A quick brown fox jumps over the lazy dog.\"\n",
    "gpt2_tokens = gpt2.to_tokens(gpt2_text)\n",
    "print(gpt2_tokens.shape)\n",
    "pruned_gpt2 = prune_model(gpt2, gpt2_tokens)\n",
    "gpt2: sae_lens.HookedSAETransformer = sae_lens.HookedSAETransformer.from_pretrained(\"gpt2-small\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.save(pruned_gpt2.state_dict(), 'pruned_gpt2.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized prompt: ['<|endoftext|>', 'I']\n",
      "Tokenized answer: [' am']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"font-weight: bold\">        Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">14.87</span><span style=\"font-weight: bold\"> Prob:  </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4.20</span><span style=\"font-weight: bold\">% Token: | am|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m4\u001b[0m\u001b[1m        Logit: \u001b[0m\u001b[1;36m14.87\u001b[0m\u001b[1m Prob:  \u001b[0m\u001b[1;36m4.20\u001b[0m\u001b[1m% Token: | am|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 15.69 Prob:  9.48% Token: | have|\n",
      "Top 1th token. Logit: 15.55 Prob:  8.25% Token: | was|\n",
      "Top 2th token. Logit: 15.42 Prob:  7.22% Token: |'m|\n",
      "Top 3th token. Logit: 15.28 Prob:  6.32% Token: |'ve|\n",
      "Top 4th token. Logit: 14.87 Prob:  4.20% Token: | am|\n",
      "Top 5th token. Logit: 14.68 Prob:  3.46% Token: | think|\n",
      "Top 6th token. Logit: 14.64 Prob:  3.33% Token: | don|\n",
      "Top 7th token. Logit: 14.57 Prob:  3.11% Token: | had|\n",
      "Top 8th token. Logit: 14.46 Prob:  2.78% Token: | know|\n",
      "Top 9th token. Logit: 14.32 Prob:  2.42% Token: | can|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">' am'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"font-weight: bold\">)]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m' am'\u001b[0m, \u001b[1;36m4\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"I\"\n",
    "answer = \" am\"\n",
    "transformer_lens.utils.test_prompt(prompt, answer, pruned_gpt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized prompt: ['<|endoftext|>', 'I']\n",
      "Tokenized answer: [' am']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"font-weight: bold\">        Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">14.87</span><span style=\"font-weight: bold\"> Prob:  </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4.20</span><span style=\"font-weight: bold\">% Token: | am|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m4\u001b[0m\u001b[1m        Logit: \u001b[0m\u001b[1;36m14.87\u001b[0m\u001b[1m Prob:  \u001b[0m\u001b[1;36m4.20\u001b[0m\u001b[1m% Token: | am|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 15.69 Prob:  9.48% Token: | have|\n",
      "Top 1th token. Logit: 15.55 Prob:  8.25% Token: | was|\n",
      "Top 2th token. Logit: 15.42 Prob:  7.22% Token: |'m|\n",
      "Top 3th token. Logit: 15.28 Prob:  6.32% Token: |'ve|\n",
      "Top 4th token. Logit: 14.87 Prob:  4.20% Token: | am|\n",
      "Top 5th token. Logit: 14.68 Prob:  3.46% Token: | think|\n",
      "Top 6th token. Logit: 14.64 Prob:  3.33% Token: | don|\n",
      "Top 7th token. Logit: 14.57 Prob:  3.11% Token: | had|\n",
      "Top 8th token. Logit: 14.46 Prob:  2.78% Token: | know|\n",
      "Top 9th token. Logit: 14.32 Prob:  2.42% Token: | can|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">' am'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"font-weight: bold\">)]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m' am'\u001b[0m, \u001b[1;36m4\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"I\"\n",
    "answer = \" am\"\n",
    "transformer_lens.utils.test_prompt(prompt, answer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'>' not supported between instances of 'int' and 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[126], line 20\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m tokens\n\u001b[1;32m     18\u001b[0m openwebtext \u001b[38;5;241m=\u001b[39m OpenWebText(dataset)\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[43mopenwebtext\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m]\u001b[49m):\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;66;03m# print(batch.shape)\u001b[39;00m\n\u001b[1;32m     22\u001b[0m     pruned_gpt2 \u001b[38;5;241m=\u001b[39m prune_model(pruned_gpt2, batch)\n\u001b[1;32m     24\u001b[0m gpt2: sae_lens\u001b[38;5;241m.\u001b[39mHookedSAETransformer \u001b[38;5;241m=\u001b[39m sae_lens\u001b[38;5;241m.\u001b[39mHookedSAETransformer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt2-small\u001b[39m\u001b[38;5;124m\"\u001b[39m, device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "Cell \u001b[0;32mIn[126], line 13\u001b[0m, in \u001b[0;36mOpenWebText.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[0;32m---> 13\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     14\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m gpt2\u001b[38;5;241m.\u001b[39mto_tokens(text)\n\u001b[1;32m     15\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m tokens[:\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_length]\n",
      "File \u001b[0;32m~/.conda/envs/SAE/lib/python3.10/site-packages/datasets/arrow_dataset.py:2872\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2870\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):  \u001b[38;5;66;03m# noqa: F811\u001b[39;00m\n\u001b[1;32m   2871\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools).\"\"\"\u001b[39;00m\n\u001b[0;32m-> 2872\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/SAE/lib/python3.10/site-packages/datasets/arrow_dataset.py:2856\u001b[0m, in \u001b[0;36mDataset._getitem\u001b[0;34m(self, key, **kwargs)\u001b[0m\n\u001b[1;32m   2854\u001b[0m format_kwargs \u001b[38;5;241m=\u001b[39m format_kwargs \u001b[38;5;28;01mif\u001b[39;00m format_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[1;32m   2855\u001b[0m formatter \u001b[38;5;241m=\u001b[39m get_formatter(format_type, features\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info\u001b[38;5;241m.\u001b[39mfeatures, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mformat_kwargs)\n\u001b[0;32m-> 2856\u001b[0m pa_subtable \u001b[38;5;241m=\u001b[39m \u001b[43mquery_table\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_indices\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2857\u001b[0m formatted_output \u001b[38;5;241m=\u001b[39m format_table(\n\u001b[1;32m   2858\u001b[0m     pa_subtable, key, formatter\u001b[38;5;241m=\u001b[39mformatter, format_columns\u001b[38;5;241m=\u001b[39mformat_columns, output_all_columns\u001b[38;5;241m=\u001b[39moutput_all_columns\n\u001b[1;32m   2859\u001b[0m )\n\u001b[1;32m   2860\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m formatted_output\n",
      "File \u001b[0;32m~/.conda/envs/SAE/lib/python3.10/site-packages/datasets/formatting/formatting.py:593\u001b[0m, in \u001b[0;36mquery_table\u001b[0;34m(table, key, indices)\u001b[0m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    592\u001b[0m     size \u001b[38;5;241m=\u001b[39m indices\u001b[38;5;241m.\u001b[39mnum_rows \u001b[38;5;28;01mif\u001b[39;00m indices \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m table\u001b[38;5;241m.\u001b[39mnum_rows\n\u001b[0;32m--> 593\u001b[0m     \u001b[43m_check_valid_index_key\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    594\u001b[0m \u001b[38;5;66;03m# Query the main table\u001b[39;00m\n\u001b[1;32m    595\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m indices \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/SAE/lib/python3.10/site-packages/datasets/formatting/formatting.py:543\u001b[0m, in \u001b[0;36m_check_valid_index_key\u001b[0;34m(key, size)\u001b[0m\n\u001b[1;32m    541\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Iterable):\n\u001b[1;32m    542\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(key) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 543\u001b[0m         _check_valid_index_key(\u001b[38;5;28mint\u001b[39m(\u001b[38;5;28;43mmax\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m), size\u001b[38;5;241m=\u001b[39msize)\n\u001b[1;32m    544\u001b[0m         _check_valid_index_key(\u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mmin\u001b[39m(key)), size\u001b[38;5;241m=\u001b[39msize)\n\u001b[1;32m    545\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mTypeError\u001b[0m: '>' not supported between instances of 'int' and 'NoneType'"
     ]
    }
   ],
   "source": [
    "pruned_gpt2: sae_lens.HookedSAETransformer = sae_lens.HookedSAETransformer.from_pretrained(\"gpt2-small\", device=device)\n",
    "dataset = transformer_lens.utils.get_dataset('openwebtext')\n",
    "\n",
    "class OpenWebText(t.utils.data.Dataset):\n",
    "    def __init__(self, dataset, max_length=1024):\n",
    "        self.dataset = dataset\n",
    "        self.max_length = 1024\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.dataset[idx]['text']\n",
    "        tokens = gpt2.to_tokens(text)\n",
    "        tokens = tokens[:self.max_length]\n",
    "        return tokens\n",
    "    \n",
    "openwebtext = OpenWebText(dataset)\n",
    "\n",
    "for batch in tqdm(openwebtext):\n",
    "    # print(batch.shape)\n",
    "    pruned_gpt2 = prune_model(pruned_gpt2, batch)\n",
    "\n",
    "gpt2: sae_lens.HookedSAETransformer = sae_lens.HookedSAETransformer.from_pretrained(\"gpt2-small\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized prompt: ['<|endoftext|>', 'I']\n",
      "Tokenized answer: [' am']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"font-weight: bold\">        Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">13.88</span><span style=\"font-weight: bold\"> Prob:  </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4.74</span><span style=\"font-weight: bold\">% Token: | am|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m3\u001b[0m\u001b[1m        Logit: \u001b[0m\u001b[1;36m13.88\u001b[0m\u001b[1m Prob:  \u001b[0m\u001b[1;36m4.74\u001b[0m\u001b[1m% Token: | am|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 14.56 Prob:  9.34% Token: |'m|\n",
      "Top 1th token. Logit: 14.51 Prob:  8.81% Token: |'ve|\n",
      "Top 2th token. Logit: 14.17 Prob:  6.31% Token: | have|\n",
      "Top 3th token. Logit: 13.88 Prob:  4.74% Token: | am|\n",
      "Top 4th token. Logit: 13.88 Prob:  4.69% Token: | was|\n",
      "Top 5th token. Logit: 13.39 Prob:  2.88% Token: | don|\n",
      "Top 6th token. Logit: 13.29 Prob:  2.61% Token: | think|\n",
      "Top 7th token. Logit: 13.15 Prob:  2.27% Token: | had|\n",
      "Top 8th token. Logit: 13.09 Prob:  2.14% Token: | know|\n",
      "Top 9th token. Logit: 12.82 Prob:  1.64% Token: | can|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">' am'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"font-weight: bold\">)]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m' am'\u001b[0m, \u001b[1;36m3\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"I\"\n",
    "answer = \" am\"\n",
    "transformer_lens.utils.test_prompt(prompt, answer, gpt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized prompt: ['<|endoftext|>', 'I']\n",
      "Tokenized answer: [' am']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"font-weight: bold\">        Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">14.87</span><span style=\"font-weight: bold\"> Prob:  </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4.20</span><span style=\"font-weight: bold\">% Token: | am|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m4\u001b[0m\u001b[1m        Logit: \u001b[0m\u001b[1;36m14.87\u001b[0m\u001b[1m Prob:  \u001b[0m\u001b[1;36m4.20\u001b[0m\u001b[1m% Token: | am|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 15.69 Prob:  9.48% Token: | have|\n",
      "Top 1th token. Logit: 15.55 Prob:  8.25% Token: | was|\n",
      "Top 2th token. Logit: 15.42 Prob:  7.22% Token: |'m|\n",
      "Top 3th token. Logit: 15.28 Prob:  6.32% Token: |'ve|\n",
      "Top 4th token. Logit: 14.87 Prob:  4.20% Token: | am|\n",
      "Top 5th token. Logit: 14.68 Prob:  3.46% Token: | think|\n",
      "Top 6th token. Logit: 14.64 Prob:  3.33% Token: | don|\n",
      "Top 7th token. Logit: 14.57 Prob:  3.11% Token: | had|\n",
      "Top 8th token. Logit: 14.46 Prob:  2.78% Token: | know|\n",
      "Top 9th token. Logit: 14.32 Prob:  2.42% Token: | can|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">' am'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"font-weight: bold\">)]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m' am'\u001b[0m, \u001b[1;36m4\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"I\"\n",
    "answer = \" am\"\n",
    "transformer_lens.utils.test_prompt(prompt, answer, pruned_gpt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.3225, device='cuda:0')"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer_lens.evals.sanity_check(gpt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.6407, device='cuda:0')"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer_lens.evals.sanity_check(pruned_gpt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized prompt: ['<|endoftext|>', 'After', ' John', ' and', ' Mary', ' went', ' to', ' the', ' store', ',', ' John', ' gave', ' a', ' bottle', ' of', ' milk', ' to']\n",
      "Tokenized answer: [' Mary']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">        Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">18.09</span><span style=\"font-weight: bold\"> Prob: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">70.07</span><span style=\"font-weight: bold\">% Token: | Mary|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m0\u001b[0m\u001b[1m        Logit: \u001b[0m\u001b[1;36m18.09\u001b[0m\u001b[1m Prob: \u001b[0m\u001b[1;36m70.07\u001b[0m\u001b[1m% Token: | Mary|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 18.09 Prob: 70.07% Token: | Mary|\n",
      "Top 1th token. Logit: 15.38 Prob:  4.67% Token: | the|\n",
      "Top 2th token. Logit: 15.35 Prob:  4.54% Token: | John|\n",
      "Top 3th token. Logit: 15.25 Prob:  4.11% Token: | them|\n",
      "Top 4th token. Logit: 14.84 Prob:  2.73% Token: | his|\n",
      "Top 5th token. Logit: 14.06 Prob:  1.24% Token: | her|\n",
      "Top 6th token. Logit: 13.54 Prob:  0.74% Token: | a|\n",
      "Top 7th token. Logit: 13.52 Prob:  0.73% Token: | their|\n",
      "Top 8th token. Logit: 13.13 Prob:  0.49% Token: | Jesus|\n",
      "Top 9th token. Logit: 12.97 Prob:  0.42% Token: | him|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">' Mary'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">)]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m' Mary'\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "example_prompt = \"After John and Mary went to the store, John gave a bottle of milk to\"\n",
    "example_answer = \" Mary\"\n",
    "transformer_lens.utils.test_prompt(example_prompt, example_answer, gpt2, prepend_bos=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized prompt: ['<|endoftext|>', 'After', ' John', ' and', ' Mary', ' went', ' to', ' the', ' store', ',', ' John', ' gave', ' a', ' bottle', ' of', ' milk', ' to']\n",
      "Tokenized answer: [' Mary']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">        Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">17.87</span><span style=\"font-weight: bold\"> Prob: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">42.15</span><span style=\"font-weight: bold\">% Token: | Mary|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m0\u001b[0m\u001b[1m        Logit: \u001b[0m\u001b[1;36m17.87\u001b[0m\u001b[1m Prob: \u001b[0m\u001b[1;36m42.15\u001b[0m\u001b[1m% Token: | Mary|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 17.87 Prob: 42.15% Token: | Mary|\n",
      "Top 1th token. Logit: 16.95 Prob: 16.90% Token: | the|\n",
      "Top 2th token. Logit: 15.95 Prob:  6.18% Token: | John|\n",
      "Top 3th token. Logit: 15.79 Prob:  5.29% Token: | his|\n",
      "Top 4th token. Logit: 15.70 Prob:  4.82% Token: | them|\n",
      "Top 5th token. Logit: 15.56 Prob:  4.18% Token: | a|\n",
      "Top 6th token. Logit: 15.49 Prob:  3.92% Token: | her|\n",
      "Top 7th token. Logit: 14.36 Prob:  1.27% Token: | him|\n",
      "Top 8th token. Logit: 14.31 Prob:  1.20% Token: | one|\n",
      "Top 9th token. Logit: 14.26 Prob:  1.14% Token: | their|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">' Mary'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">)]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m' Mary'\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "example_prompt = \"After John and Mary went to the store, John gave a bottle of milk to\"\n",
    "example_answer = \" Mary\"\n",
    "transformer_lens.utils.test_prompt(example_prompt, example_answer, pruned_gpt2, prepend_bos=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gemma2\n",
    "\n",
    "Issue: Pretrained SAE is not available for the model - for attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You tried to specify center_unembed=True for a model using logit softcap, but this can't be done! Softcapping is not invariant upon adding a constant Setting center_unembed=False instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d43d88923b19449098cb3f88e93afc93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gemma-2-2b into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "gemma2b: sae_lens.HookedSAETransformer = sae_lens.HookedSAETransformer.from_pretrained(\"gemma-2-2b\", device=device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SAE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

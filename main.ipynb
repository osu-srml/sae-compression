{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import itertools\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "from collections import Counter, defaultdict\n",
    "from copy import deepcopy\n",
    "from dataclasses import dataclass\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "from typing import Any, Callable, Literal, TypeAlias\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch as t\n",
    "from datasets import load_dataset\n",
    "import transformer_lens\n",
    "import sae_lens\n",
    "\n",
    "import einops\n",
    "import circuitsvis as cv\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from IPython.display import HTML, IFrame, clear_output, display\n",
    "from jaxtyping import Float, Int\n",
    "from openai import OpenAI\n",
    "from rich import print as rprint\n",
    "from rich.table import Table\n",
    "from tabulate import tabulate\n",
    "\n",
    "\n",
    "# Hugging face: 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\n",
    "# Wandb: 6b549d940e7a29c79c184f27f25606e94a48a966"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = t.device(\"cuda\" if t.cuda.is_available() else \"cpu\")\n",
    "t.set_grad_enabled(False)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "gpt2: sae_lens.HookedSAETransformer = sae_lens.HookedSAETransformer.from_pretrained(\"gpt2-small\", device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attention SAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9c376168d144dd2a66ce89f52b0ebcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)-06_rsanthropic_rie25000_nr4_v9_cfg.json:   0%|          | 0.00/1.01k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42d68d40f9af4b46a318cd5c3d8076b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)c1.00e-06_rsanthropic_rie25000_nr4_v9.pt:   0%|          | 0.00/151M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e97449dd52141f6be96b905c15252a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)-06_rsanthropic_rie25000_nr4_v5_cfg.json:   0%|          | 0.00/1.02k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9cf1b87c313484e97c086386fd63a22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)c1.00e-06_rsanthropic_rie25000_nr4_v5.pt:   0%|          | 0.00/151M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "702494c611814cdc92a1bfeccfdbb503",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)-06_rsanthropic_rie25000_nr4_v4_cfg.json:   0%|          | 0.00/1.02k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "808ede7b9d6e4fdd9f015ba113ae8ce1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)c1.00e-06_rsanthropic_rie25000_nr4_v4.pt:   0%|          | 0.00/151M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9bf62c642c5436d9afca4507a47f15d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)-06_rsanthropic_rie25000_nr4_v9_cfg.json:   0%|          | 0.00/1.01k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d10d047494b44a4789f5383f8460f645",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)c1.00e-06_rsanthropic_rie25000_nr4_v9.pt:   0%|          | 0.00/151M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6821808b8ca4b5ba49de4e9e2f1f430",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)-06_rsanthropic_rie25000_nr4_v7_cfg.json:   0%|          | 0.00/1.01k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e0061cbdb14435496e9b8c9e4fb3bd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)c1.00e-06_rsanthropic_rie25000_nr4_v7.pt:   0%|          | 0.00/151M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2f366e1c4c44c13b52cf0249699be19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)-06_rsanthropic_rie25000_nr4_v9_cfg.json:   0%|          | 0.00/1.02k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35fe80cd4cdd4c80acae239d5dfca16e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)c1.00e-06_rsanthropic_rie25000_nr4_v9.pt:   0%|          | 0.00/302M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f614d87176e4d6da817699e5726bfb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)-06_rsanthropic_rie25000_nr4_v9_cfg.json:   0%|          | 0.00/1.01k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0256fa7f236340a08177b2493b549abf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)c1.00e-06_rsanthropic_rie25000_nr4_v9.pt:   0%|          | 0.00/151M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ef7abf2015249d8bf1e0241e65d4ae4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)-06_rsanthropic_rie25000_nr4_v9_cfg.json:   0%|          | 0.00/1.01k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e5639bc3e8641e9a746ca6fa09b349e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)c1.00e-06_rsanthropic_rie25000_nr4_v9.pt:   0%|          | 0.00/302M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64bf1f088e384feaa0c5078e1399a76c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)-05_rsanthropic_rie25000_nr4_v6_cfg.json:   0%|          | 0.00/1.02k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f8bc030aede474e945372ba03890c59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)c1.00e-05_rsanthropic_rie25000_nr4_v6.pt:   0%|          | 0.00/151M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0917c45b0f8a4acb8c0674116c633006",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)-06_rsanthropic_rie25000_nr4_v9_cfg.json:   0%|          | 0.00/1.01k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "357a64ee28ed45d4ac2934efb947429b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)c1.00e-06_rsanthropic_rie25000_nr4_v9.pt:   0%|          | 0.00/151M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5655517736e54bbaa4aef090695e8474",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)-05_rsanthropic_rie25000_nr4_v9_cfg.json:   0%|          | 0.00/1.02k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcf699835bc94188be87abd319ab5597",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)c1.00e-05_rsanthropic_rie25000_nr4_v9.pt:   0%|          | 0.00/151M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb58f319904a4482ad2e1e2d283ffae1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)-06_rsanthropic_rie25000_nr4_v9_cfg.json:   0%|          | 0.00/1.03k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80ac1abd619b44979a4c87935778cd40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)c3.16e-06_rsanthropic_rie25000_nr4_v9.pt:   0%|          | 0.00/151M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "attn_saes = {\n",
    "    layer: sae_lens.SAE.from_pretrained(\n",
    "        \"gpt2-small-hook-z-kk\",\n",
    "        f\"blocks.{layer}.hook_z\",\n",
    "        device=device,\n",
    "    )[0]\n",
    "    for layer in range(gpt2.cfg.n_layers)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: SAE(\n",
      "  (activation_fn): ReLU()\n",
      "  (hook_sae_input): HookPoint()\n",
      "  (hook_sae_acts_pre): HookPoint()\n",
      "  (hook_sae_acts_post): HookPoint()\n",
      "  (hook_sae_output): HookPoint()\n",
      "  (hook_sae_recons): HookPoint()\n",
      "  (hook_sae_error): HookPoint()\n",
      "), 1: SAE(\n",
      "  (activation_fn): ReLU()\n",
      "  (hook_sae_input): HookPoint()\n",
      "  (hook_sae_acts_pre): HookPoint()\n",
      "  (hook_sae_acts_post): HookPoint()\n",
      "  (hook_sae_output): HookPoint()\n",
      "  (hook_sae_recons): HookPoint()\n",
      "  (hook_sae_error): HookPoint()\n",
      "), 2: SAE(\n",
      "  (activation_fn): ReLU()\n",
      "  (hook_sae_input): HookPoint()\n",
      "  (hook_sae_acts_pre): HookPoint()\n",
      "  (hook_sae_acts_post): HookPoint()\n",
      "  (hook_sae_output): HookPoint()\n",
      "  (hook_sae_recons): HookPoint()\n",
      "  (hook_sae_error): HookPoint()\n",
      "), 3: SAE(\n",
      "  (activation_fn): ReLU()\n",
      "  (hook_sae_input): HookPoint()\n",
      "  (hook_sae_acts_pre): HookPoint()\n",
      "  (hook_sae_acts_post): HookPoint()\n",
      "  (hook_sae_output): HookPoint()\n",
      "  (hook_sae_recons): HookPoint()\n",
      "  (hook_sae_error): HookPoint()\n",
      "), 4: SAE(\n",
      "  (activation_fn): ReLU()\n",
      "  (hook_sae_input): HookPoint()\n",
      "  (hook_sae_acts_pre): HookPoint()\n",
      "  (hook_sae_acts_post): HookPoint()\n",
      "  (hook_sae_output): HookPoint()\n",
      "  (hook_sae_recons): HookPoint()\n",
      "  (hook_sae_error): HookPoint()\n",
      "), 5: SAE(\n",
      "  (activation_fn): ReLU()\n",
      "  (hook_sae_input): HookPoint()\n",
      "  (hook_sae_acts_pre): HookPoint()\n",
      "  (hook_sae_acts_post): HookPoint()\n",
      "  (hook_sae_output): HookPoint()\n",
      "  (hook_sae_recons): HookPoint()\n",
      "  (hook_sae_error): HookPoint()\n",
      "), 6: SAE(\n",
      "  (activation_fn): ReLU()\n",
      "  (hook_sae_input): HookPoint()\n",
      "  (hook_sae_acts_pre): HookPoint()\n",
      "  (hook_sae_acts_post): HookPoint()\n",
      "  (hook_sae_output): HookPoint()\n",
      "  (hook_sae_recons): HookPoint()\n",
      "  (hook_sae_error): HookPoint()\n",
      "), 7: SAE(\n",
      "  (activation_fn): ReLU()\n",
      "  (hook_sae_input): HookPoint()\n",
      "  (hook_sae_acts_pre): HookPoint()\n",
      "  (hook_sae_acts_post): HookPoint()\n",
      "  (hook_sae_output): HookPoint()\n",
      "  (hook_sae_recons): HookPoint()\n",
      "  (hook_sae_error): HookPoint()\n",
      "), 8: SAE(\n",
      "  (activation_fn): ReLU()\n",
      "  (hook_sae_input): HookPoint()\n",
      "  (hook_sae_acts_pre): HookPoint()\n",
      "  (hook_sae_acts_post): HookPoint()\n",
      "  (hook_sae_output): HookPoint()\n",
      "  (hook_sae_recons): HookPoint()\n",
      "  (hook_sae_error): HookPoint()\n",
      "), 9: SAE(\n",
      "  (activation_fn): ReLU()\n",
      "  (hook_sae_input): HookPoint()\n",
      "  (hook_sae_acts_pre): HookPoint()\n",
      "  (hook_sae_acts_post): HookPoint()\n",
      "  (hook_sae_output): HookPoint()\n",
      "  (hook_sae_recons): HookPoint()\n",
      "  (hook_sae_error): HookPoint()\n",
      "), 10: SAE(\n",
      "  (activation_fn): ReLU()\n",
      "  (hook_sae_input): HookPoint()\n",
      "  (hook_sae_acts_pre): HookPoint()\n",
      "  (hook_sae_acts_post): HookPoint()\n",
      "  (hook_sae_output): HookPoint()\n",
      "  (hook_sae_recons): HookPoint()\n",
      "  (hook_sae_error): HookPoint()\n",
      "), 11: SAE(\n",
      "  (activation_fn): ReLU()\n",
      "  (hook_sae_input): HookPoint()\n",
      "  (hook_sae_acts_pre): HookPoint()\n",
      "  (hook_sae_acts_post): HookPoint()\n",
      "  (hook_sae_output): HookPoint()\n",
      "  (hook_sae_recons): HookPoint()\n",
      "  (hook_sae_error): HookPoint()\n",
      ")}\n"
     ]
    }
   ],
   "source": [
    "print(attn_saes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://neuronpedia.org/gpt2-small/9-att-kk/2?embed=true&embedexplanation=true&embedplots=true&embedtest=true&height=300\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"800\"\n",
       "            height=\"600\"\n",
       "            src=\"https://neuronpedia.org/gpt2-small/9-att-kk/2?embed=true&embedexplanation=true&embedplots=true&embedtest=true&height=300\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7ef9b05566e0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def display_dashboard(\n",
    "    sae_release=\"gpt2-small-res-jb\",\n",
    "    sae_id=\"blocks.7.hook_resid_pre\",\n",
    "    latent_idx=0,\n",
    "    width=800,\n",
    "    height=600,\n",
    "):\n",
    "    release = sae_lens.toolkit.pretrained_saes_directory.get_pretrained_saes_directory()[sae_release]\n",
    "    neuronpedia_id = release.neuronpedia_id[sae_id]\n",
    "\n",
    "    url = f\"https://neuronpedia.org/{neuronpedia_id}/{latent_idx}?embed=true&embedexplanation=true&embedplots=true&embedtest=true&height=300\"\n",
    "\n",
    "    print(url)\n",
    "    display(IFrame(url, width=width, height=height))\n",
    "\n",
    "\n",
    "layer = 9\n",
    "\n",
    "display_dashboard(\n",
    "    sae_release=\"gpt2-small-hook-z-kk\",\n",
    "    sae_id=f\"blocks.{layer}.hook_z\",\n",
    "    latent_idx=2,  # or you can try `random.randint(0, attn_saes[layer].cfg.d_sae)`\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HookedTransformerConfig:\n",
       "{'NTK_by_parts_factor': 8.0,\n",
       " 'NTK_by_parts_high_freq_factor': 4.0,\n",
       " 'NTK_by_parts_low_freq_factor': 1.0,\n",
       " 'act_fn': 'gelu_new',\n",
       " 'attention_dir': 'causal',\n",
       " 'attn_only': False,\n",
       " 'attn_scale': 8.0,\n",
       " 'attn_scores_soft_cap': -1.0,\n",
       " 'attn_types': None,\n",
       " 'checkpoint_index': None,\n",
       " 'checkpoint_label_type': None,\n",
       " 'checkpoint_value': None,\n",
       " 'd_head': 64,\n",
       " 'd_mlp': 3072,\n",
       " 'd_model': 768,\n",
       " 'd_vocab': 50257,\n",
       " 'd_vocab_out': 50257,\n",
       " 'decoder_start_token_id': None,\n",
       " 'default_prepend_bos': True,\n",
       " 'device': device(type='cuda'),\n",
       " 'dtype': torch.float32,\n",
       " 'eps': 1e-05,\n",
       " 'experts_per_token': None,\n",
       " 'final_rms': False,\n",
       " 'from_checkpoint': False,\n",
       " 'gated_mlp': False,\n",
       " 'init_mode': 'gpt2',\n",
       " 'init_weights': False,\n",
       " 'initializer_range': 0.02886751345948129,\n",
       " 'load_in_4bit': False,\n",
       " 'model_name': 'gpt2',\n",
       " 'n_ctx': 1024,\n",
       " 'n_devices': 1,\n",
       " 'n_heads': 12,\n",
       " 'n_key_value_heads': None,\n",
       " 'n_layers': 12,\n",
       " 'n_params': 84934656,\n",
       " 'normalization_type': 'LNPre',\n",
       " 'num_experts': None,\n",
       " 'original_architecture': 'GPT2LMHeadModel',\n",
       " 'output_logits_soft_cap': -1.0,\n",
       " 'parallel_attn_mlp': False,\n",
       " 'positional_embedding_type': 'standard',\n",
       " 'post_embedding_ln': False,\n",
       " 'relative_attention_max_distance': None,\n",
       " 'relative_attention_num_buckets': None,\n",
       " 'rotary_adjacent_pairs': False,\n",
       " 'rotary_base': 10000,\n",
       " 'rotary_dim': None,\n",
       " 'scale_attn_by_inverse_layer_idx': False,\n",
       " 'seed': None,\n",
       " 'tie_word_embeddings': False,\n",
       " 'tokenizer_name': 'gpt2',\n",
       " 'tokenizer_prepends_bos': False,\n",
       " 'trust_remote_code': False,\n",
       " 'ungroup_grouped_query_attention': False,\n",
       " 'use_NTK_by_parts_rope': False,\n",
       " 'use_attn_in': False,\n",
       " 'use_attn_result': False,\n",
       " 'use_attn_scale': True,\n",
       " 'use_hook_mlp_in': False,\n",
       " 'use_hook_tokens': False,\n",
       " 'use_local_attn': False,\n",
       " 'use_normalization_before_and_after': False,\n",
       " 'use_split_qkv_input': False,\n",
       " 'window_size': None}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt2.cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Magnitude pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: embed.W_E, Shape: torch.Size([50257, 768])\n",
      "Layer: pos_embed.W_pos, Shape: torch.Size([1024, 768])\n",
      "Layer: blocks.0.attn.W_Q, Shape: torch.Size([12, 768, 64])\n",
      "Layer: blocks.0.attn.W_O, Shape: torch.Size([12, 64, 768])\n",
      "Layer: blocks.0.attn.b_Q, Shape: torch.Size([12, 64])\n",
      "Layer: blocks.0.attn.b_O, Shape: torch.Size([768])\n",
      "Layer: blocks.0.attn.W_K, Shape: torch.Size([12, 768, 64])\n",
      "Layer: blocks.0.attn.W_V, Shape: torch.Size([12, 768, 64])\n",
      "Layer: blocks.0.attn.b_K, Shape: torch.Size([12, 64])\n",
      "Layer: blocks.0.attn.b_V, Shape: torch.Size([12, 64])\n",
      "Layer: blocks.0.mlp.W_in, Shape: torch.Size([768, 3072])\n",
      "Layer: blocks.0.mlp.b_in, Shape: torch.Size([3072])\n",
      "Layer: blocks.0.mlp.W_out, Shape: torch.Size([3072, 768])\n",
      "Layer: blocks.0.mlp.b_out, Shape: torch.Size([768])\n",
      "Layer: blocks.1.attn.W_Q, Shape: torch.Size([12, 768, 64])\n",
      "Layer: blocks.1.attn.W_O, Shape: torch.Size([12, 64, 768])\n",
      "Layer: blocks.1.attn.b_Q, Shape: torch.Size([12, 64])\n",
      "Layer: blocks.1.attn.b_O, Shape: torch.Size([768])\n",
      "Layer: blocks.1.attn.W_K, Shape: torch.Size([12, 768, 64])\n",
      "Layer: blocks.1.attn.W_V, Shape: torch.Size([12, 768, 64])\n",
      "Layer: blocks.1.attn.b_K, Shape: torch.Size([12, 64])\n",
      "Layer: blocks.1.attn.b_V, Shape: torch.Size([12, 64])\n",
      "Layer: blocks.1.mlp.W_in, Shape: torch.Size([768, 3072])\n",
      "Layer: blocks.1.mlp.b_in, Shape: torch.Size([3072])\n",
      "Layer: blocks.1.mlp.W_out, Shape: torch.Size([3072, 768])\n",
      "Layer: blocks.1.mlp.b_out, Shape: torch.Size([768])\n",
      "Layer: blocks.2.attn.W_Q, Shape: torch.Size([12, 768, 64])\n",
      "Layer: blocks.2.attn.W_O, Shape: torch.Size([12, 64, 768])\n",
      "Layer: blocks.2.attn.b_Q, Shape: torch.Size([12, 64])\n",
      "Layer: blocks.2.attn.b_O, Shape: torch.Size([768])\n",
      "Layer: blocks.2.attn.W_K, Shape: torch.Size([12, 768, 64])\n",
      "Layer: blocks.2.attn.W_V, Shape: torch.Size([12, 768, 64])\n",
      "Layer: blocks.2.attn.b_K, Shape: torch.Size([12, 64])\n",
      "Layer: blocks.2.attn.b_V, Shape: torch.Size([12, 64])\n",
      "Layer: blocks.2.mlp.W_in, Shape: torch.Size([768, 3072])\n",
      "Layer: blocks.2.mlp.b_in, Shape: torch.Size([3072])\n",
      "Layer: blocks.2.mlp.W_out, Shape: torch.Size([3072, 768])\n",
      "Layer: blocks.2.mlp.b_out, Shape: torch.Size([768])\n",
      "Layer: blocks.3.attn.W_Q, Shape: torch.Size([12, 768, 64])\n",
      "Layer: blocks.3.attn.W_O, Shape: torch.Size([12, 64, 768])\n",
      "Layer: blocks.3.attn.b_Q, Shape: torch.Size([12, 64])\n",
      "Layer: blocks.3.attn.b_O, Shape: torch.Size([768])\n",
      "Layer: blocks.3.attn.W_K, Shape: torch.Size([12, 768, 64])\n",
      "Layer: blocks.3.attn.W_V, Shape: torch.Size([12, 768, 64])\n",
      "Layer: blocks.3.attn.b_K, Shape: torch.Size([12, 64])\n",
      "Layer: blocks.3.attn.b_V, Shape: torch.Size([12, 64])\n",
      "Layer: blocks.3.mlp.W_in, Shape: torch.Size([768, 3072])\n",
      "Layer: blocks.3.mlp.b_in, Shape: torch.Size([3072])\n",
      "Layer: blocks.3.mlp.W_out, Shape: torch.Size([3072, 768])\n",
      "Layer: blocks.3.mlp.b_out, Shape: torch.Size([768])\n",
      "Layer: blocks.4.attn.W_Q, Shape: torch.Size([12, 768, 64])\n",
      "Layer: blocks.4.attn.W_O, Shape: torch.Size([12, 64, 768])\n",
      "Layer: blocks.4.attn.b_Q, Shape: torch.Size([12, 64])\n",
      "Layer: blocks.4.attn.b_O, Shape: torch.Size([768])\n",
      "Layer: blocks.4.attn.W_K, Shape: torch.Size([12, 768, 64])\n",
      "Layer: blocks.4.attn.W_V, Shape: torch.Size([12, 768, 64])\n",
      "Layer: blocks.4.attn.b_K, Shape: torch.Size([12, 64])\n",
      "Layer: blocks.4.attn.b_V, Shape: torch.Size([12, 64])\n",
      "Layer: blocks.4.mlp.W_in, Shape: torch.Size([768, 3072])\n",
      "Layer: blocks.4.mlp.b_in, Shape: torch.Size([3072])\n",
      "Layer: blocks.4.mlp.W_out, Shape: torch.Size([3072, 768])\n",
      "Layer: blocks.4.mlp.b_out, Shape: torch.Size([768])\n",
      "Layer: blocks.5.attn.W_Q, Shape: torch.Size([12, 768, 64])\n",
      "Layer: blocks.5.attn.W_O, Shape: torch.Size([12, 64, 768])\n",
      "Layer: blocks.5.attn.b_Q, Shape: torch.Size([12, 64])\n",
      "Layer: blocks.5.attn.b_O, Shape: torch.Size([768])\n",
      "Layer: blocks.5.attn.W_K, Shape: torch.Size([12, 768, 64])\n",
      "Layer: blocks.5.attn.W_V, Shape: torch.Size([12, 768, 64])\n",
      "Layer: blocks.5.attn.b_K, Shape: torch.Size([12, 64])\n",
      "Layer: blocks.5.attn.b_V, Shape: torch.Size([12, 64])\n",
      "Layer: blocks.5.mlp.W_in, Shape: torch.Size([768, 3072])\n",
      "Layer: blocks.5.mlp.b_in, Shape: torch.Size([3072])\n",
      "Layer: blocks.5.mlp.W_out, Shape: torch.Size([3072, 768])\n",
      "Layer: blocks.5.mlp.b_out, Shape: torch.Size([768])\n",
      "Layer: blocks.6.attn.W_Q, Shape: torch.Size([12, 768, 64])\n",
      "Layer: blocks.6.attn.W_O, Shape: torch.Size([12, 64, 768])\n",
      "Layer: blocks.6.attn.b_Q, Shape: torch.Size([12, 64])\n",
      "Layer: blocks.6.attn.b_O, Shape: torch.Size([768])\n",
      "Layer: blocks.6.attn.W_K, Shape: torch.Size([12, 768, 64])\n",
      "Layer: blocks.6.attn.W_V, Shape: torch.Size([12, 768, 64])\n",
      "Layer: blocks.6.attn.b_K, Shape: torch.Size([12, 64])\n",
      "Layer: blocks.6.attn.b_V, Shape: torch.Size([12, 64])\n",
      "Layer: blocks.6.mlp.W_in, Shape: torch.Size([768, 3072])\n",
      "Layer: blocks.6.mlp.b_in, Shape: torch.Size([3072])\n",
      "Layer: blocks.6.mlp.W_out, Shape: torch.Size([3072, 768])\n",
      "Layer: blocks.6.mlp.b_out, Shape: torch.Size([768])\n",
      "Layer: blocks.7.attn.W_Q, Shape: torch.Size([12, 768, 64])\n",
      "Layer: blocks.7.attn.W_O, Shape: torch.Size([12, 64, 768])\n",
      "Layer: blocks.7.attn.b_Q, Shape: torch.Size([12, 64])\n",
      "Layer: blocks.7.attn.b_O, Shape: torch.Size([768])\n",
      "Layer: blocks.7.attn.W_K, Shape: torch.Size([12, 768, 64])\n",
      "Layer: blocks.7.attn.W_V, Shape: torch.Size([12, 768, 64])\n",
      "Layer: blocks.7.attn.b_K, Shape: torch.Size([12, 64])\n",
      "Layer: blocks.7.attn.b_V, Shape: torch.Size([12, 64])\n",
      "Layer: blocks.7.mlp.W_in, Shape: torch.Size([768, 3072])\n",
      "Layer: blocks.7.mlp.b_in, Shape: torch.Size([3072])\n",
      "Layer: blocks.7.mlp.W_out, Shape: torch.Size([3072, 768])\n",
      "Layer: blocks.7.mlp.b_out, Shape: torch.Size([768])\n",
      "Layer: blocks.8.attn.W_Q, Shape: torch.Size([12, 768, 64])\n",
      "Layer: blocks.8.attn.W_O, Shape: torch.Size([12, 64, 768])\n",
      "Layer: blocks.8.attn.b_Q, Shape: torch.Size([12, 64])\n",
      "Layer: blocks.8.attn.b_O, Shape: torch.Size([768])\n",
      "Layer: blocks.8.attn.W_K, Shape: torch.Size([12, 768, 64])\n",
      "Layer: blocks.8.attn.W_V, Shape: torch.Size([12, 768, 64])\n",
      "Layer: blocks.8.attn.b_K, Shape: torch.Size([12, 64])\n",
      "Layer: blocks.8.attn.b_V, Shape: torch.Size([12, 64])\n",
      "Layer: blocks.8.mlp.W_in, Shape: torch.Size([768, 3072])\n",
      "Layer: blocks.8.mlp.b_in, Shape: torch.Size([3072])\n",
      "Layer: blocks.8.mlp.W_out, Shape: torch.Size([3072, 768])\n",
      "Layer: blocks.8.mlp.b_out, Shape: torch.Size([768])\n",
      "Layer: blocks.9.attn.W_Q, Shape: torch.Size([12, 768, 64])\n",
      "Layer: blocks.9.attn.W_O, Shape: torch.Size([12, 64, 768])\n",
      "Layer: blocks.9.attn.b_Q, Shape: torch.Size([12, 64])\n",
      "Layer: blocks.9.attn.b_O, Shape: torch.Size([768])\n",
      "Layer: blocks.9.attn.W_K, Shape: torch.Size([12, 768, 64])\n",
      "Layer: blocks.9.attn.W_V, Shape: torch.Size([12, 768, 64])\n",
      "Layer: blocks.9.attn.b_K, Shape: torch.Size([12, 64])\n",
      "Layer: blocks.9.attn.b_V, Shape: torch.Size([12, 64])\n",
      "Layer: blocks.9.mlp.W_in, Shape: torch.Size([768, 3072])\n",
      "Layer: blocks.9.mlp.b_in, Shape: torch.Size([3072])\n",
      "Layer: blocks.9.mlp.W_out, Shape: torch.Size([3072, 768])\n",
      "Layer: blocks.9.mlp.b_out, Shape: torch.Size([768])\n",
      "Layer: blocks.10.attn.W_Q, Shape: torch.Size([12, 768, 64])\n",
      "Layer: blocks.10.attn.W_O, Shape: torch.Size([12, 64, 768])\n",
      "Layer: blocks.10.attn.b_Q, Shape: torch.Size([12, 64])\n",
      "Layer: blocks.10.attn.b_O, Shape: torch.Size([768])\n",
      "Layer: blocks.10.attn.W_K, Shape: torch.Size([12, 768, 64])\n",
      "Layer: blocks.10.attn.W_V, Shape: torch.Size([12, 768, 64])\n",
      "Layer: blocks.10.attn.b_K, Shape: torch.Size([12, 64])\n",
      "Layer: blocks.10.attn.b_V, Shape: torch.Size([12, 64])\n",
      "Layer: blocks.10.mlp.W_in, Shape: torch.Size([768, 3072])\n",
      "Layer: blocks.10.mlp.b_in, Shape: torch.Size([3072])\n",
      "Layer: blocks.10.mlp.W_out, Shape: torch.Size([3072, 768])\n",
      "Layer: blocks.10.mlp.b_out, Shape: torch.Size([768])\n",
      "Layer: blocks.11.attn.W_Q, Shape: torch.Size([12, 768, 64])\n",
      "Layer: blocks.11.attn.W_O, Shape: torch.Size([12, 64, 768])\n",
      "Layer: blocks.11.attn.b_Q, Shape: torch.Size([12, 64])\n",
      "Layer: blocks.11.attn.b_O, Shape: torch.Size([768])\n",
      "Layer: blocks.11.attn.W_K, Shape: torch.Size([12, 768, 64])\n",
      "Layer: blocks.11.attn.W_V, Shape: torch.Size([12, 768, 64])\n",
      "Layer: blocks.11.attn.b_K, Shape: torch.Size([12, 64])\n",
      "Layer: blocks.11.attn.b_V, Shape: torch.Size([12, 64])\n",
      "Layer: blocks.11.mlp.W_in, Shape: torch.Size([768, 3072])\n",
      "Layer: blocks.11.mlp.b_in, Shape: torch.Size([3072])\n",
      "Layer: blocks.11.mlp.W_out, Shape: torch.Size([3072, 768])\n",
      "Layer: blocks.11.mlp.b_out, Shape: torch.Size([768])\n",
      "Layer: unembed.W_U, Shape: torch.Size([768, 50257])\n",
      "Layer: unembed.b_U, Shape: torch.Size([50257])\n"
     ]
    }
   ],
   "source": [
    "for name, param in gpt2.named_parameters():\n",
    "    print(f\"Layer: {name}, Shape: {param.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "def prune_magnitude(W, sparse_ratio=0.5):\n",
    "    W_abs = W.abs()\n",
    "    k = int(W_abs.numel() * sparse_ratio)\n",
    "    _, indices = W_abs.view(-1).topk(k)\n",
    "    mask = t.zeros_like(W_abs)\n",
    "    mask.view(-1)[indices] = 1\n",
    "    return mask*W\n",
    "\n",
    "def prune_model(model):\n",
    "    for name, param in gpt2.named_parameters():\n",
    "        # print(f\"Layer: {name}, Shape: {param.shape}\")\n",
    "        if 'W' in name:\n",
    "            for i in range(param.shape[0]):\n",
    "                with t.no_grad():\n",
    "                    param[i] = prune_magnitude(param[i])\n",
    "    \n",
    "    return model\n",
    "\n",
    "pruned_gpt2 = prune_model(gpt2)\n",
    "gpt2: sae_lens.HookedSAETransformer = sae_lens.HookedSAETransformer.from_pretrained(\"gpt2-small\", device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized prompt: ['<|endoftext|>', 'I']\n",
      "Tokenized answer: [' am']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"font-weight: bold\">        Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">13.88</span><span style=\"font-weight: bold\"> Prob:  </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4.74</span><span style=\"font-weight: bold\">% Token: | am|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m3\u001b[0m\u001b[1m        Logit: \u001b[0m\u001b[1;36m13.88\u001b[0m\u001b[1m Prob:  \u001b[0m\u001b[1;36m4.74\u001b[0m\u001b[1m% Token: | am|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 14.56 Prob:  9.34% Token: |'m|\n",
      "Top 1th token. Logit: 14.51 Prob:  8.81% Token: |'ve|\n",
      "Top 2th token. Logit: 14.17 Prob:  6.31% Token: | have|\n",
      "Top 3th token. Logit: 13.88 Prob:  4.74% Token: | am|\n",
      "Top 4th token. Logit: 13.88 Prob:  4.69% Token: | was|\n",
      "Top 5th token. Logit: 13.39 Prob:  2.88% Token: | don|\n",
      "Top 6th token. Logit: 13.29 Prob:  2.61% Token: | think|\n",
      "Top 7th token. Logit: 13.15 Prob:  2.27% Token: | had|\n",
      "Top 8th token. Logit: 13.09 Prob:  2.14% Token: | know|\n",
      "Top 9th token. Logit: 12.82 Prob:  1.64% Token: | can|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">' am'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"font-weight: bold\">)]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m' am'\u001b[0m, \u001b[1;36m3\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"I\"\n",
    "answer = \" am\"\n",
    "transformer_lens.utils.test_prompt(prompt, answer, gpt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized prompt: ['<|endoftext|>', 'I']\n",
      "Tokenized answer: [' am']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">35</span><span style=\"font-weight: bold\">       Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11.18</span><span style=\"font-weight: bold\"> Prob:  </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.45</span><span style=\"font-weight: bold\">% Token: | am|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m35\u001b[0m\u001b[1m       Logit: \u001b[0m\u001b[1;36m11.18\u001b[0m\u001b[1m Prob:  \u001b[0m\u001b[1;36m0.45\u001b[0m\u001b[1m% Token: | am|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 14.23 Prob:  9.70% Token: |b|\n",
      "Top 1th token. Logit: 13.81 Prob:  6.36% Token: | was|\n",
      "Top 2th token. Logit: 13.80 Prob:  6.31% Token: |van|\n",
      "Top 3th token. Logit: 13.48 Prob:  4.54% Token: |on|\n",
      "Top 4th token. Logit: 12.97 Prob:  2.73% Token: | have|\n",
      "Top 5th token. Logit: 12.81 Prob:  2.32% Token: |nex|\n",
      "Top 6th token. Logit: 12.73 Prob:  2.16% Token: |e|\n",
      "Top 7th token. Logit: 12.66 Prob:  2.01% Token: |-|\n",
      "Top 8th token. Logit: 12.61 Prob:  1.90% Token: |'m|\n",
      "Top 9th token. Logit: 12.30 Prob:  1.40% Token: |'ve|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">' am'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">35</span><span style=\"font-weight: bold\">)]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m' am'\u001b[0m, \u001b[1;36m35\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"I\"\n",
    "answer = \" am\"\n",
    "transformer_lens.utils.test_prompt(prompt, answer, pruned_gpt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.3225, device='cuda:0')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer_lens.evals.sanity_check(gpt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.8446, device='cuda:0')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer_lens.evals.sanity_check(pruned_gpt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f0f641dcb4745beb482fe3a522fe502",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2119 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "2.9206282215994914"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dl = transformer_lens.evals.make_pile_data_loader(gpt2.tokenizer)\n",
    "transformer_lens.evals.evaluate_on_dataset(gpt2, dl, truncate=1000, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d46d536d32b740e0b6507b5642104dd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2119 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "3.793479422589282"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer_lens.evals.evaluate_on_dataset(pruned_gpt2, dl, truncate=1000,  device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wanda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HookedTransformerConfig:\n",
       "{'NTK_by_parts_factor': 8.0,\n",
       " 'NTK_by_parts_high_freq_factor': 4.0,\n",
       " 'NTK_by_parts_low_freq_factor': 1.0,\n",
       " 'act_fn': 'gelu_new',\n",
       " 'attention_dir': 'causal',\n",
       " 'attn_only': False,\n",
       " 'attn_scale': 8.0,\n",
       " 'attn_scores_soft_cap': -1.0,\n",
       " 'attn_types': None,\n",
       " 'checkpoint_index': None,\n",
       " 'checkpoint_label_type': None,\n",
       " 'checkpoint_value': None,\n",
       " 'd_head': 64,\n",
       " 'd_mlp': 3072,\n",
       " 'd_model': 768,\n",
       " 'd_vocab': 50257,\n",
       " 'd_vocab_out': 50257,\n",
       " 'decoder_start_token_id': None,\n",
       " 'default_prepend_bos': True,\n",
       " 'device': device(type='cuda'),\n",
       " 'dtype': torch.float32,\n",
       " 'eps': 1e-05,\n",
       " 'experts_per_token': None,\n",
       " 'final_rms': False,\n",
       " 'from_checkpoint': False,\n",
       " 'gated_mlp': False,\n",
       " 'init_mode': 'gpt2',\n",
       " 'init_weights': False,\n",
       " 'initializer_range': 0.02886751345948129,\n",
       " 'load_in_4bit': False,\n",
       " 'model_name': 'gpt2',\n",
       " 'n_ctx': 1024,\n",
       " 'n_devices': 1,\n",
       " 'n_heads': 12,\n",
       " 'n_key_value_heads': None,\n",
       " 'n_layers': 12,\n",
       " 'n_params': 84934656,\n",
       " 'normalization_type': 'LNPre',\n",
       " 'num_experts': None,\n",
       " 'original_architecture': 'GPT2LMHeadModel',\n",
       " 'output_logits_soft_cap': -1.0,\n",
       " 'parallel_attn_mlp': False,\n",
       " 'positional_embedding_type': 'standard',\n",
       " 'post_embedding_ln': False,\n",
       " 'relative_attention_max_distance': None,\n",
       " 'relative_attention_num_buckets': None,\n",
       " 'rotary_adjacent_pairs': False,\n",
       " 'rotary_base': 10000,\n",
       " 'rotary_dim': None,\n",
       " 'scale_attn_by_inverse_layer_idx': False,\n",
       " 'seed': None,\n",
       " 'tie_word_embeddings': False,\n",
       " 'tokenizer_name': 'gpt2',\n",
       " 'tokenizer_prepends_bos': False,\n",
       " 'trust_remote_code': False,\n",
       " 'ungroup_grouped_query_attention': False,\n",
       " 'use_NTK_by_parts_rope': False,\n",
       " 'use_attn_in': False,\n",
       " 'use_attn_result': False,\n",
       " 'use_attn_scale': True,\n",
       " 'use_hook_mlp_in': False,\n",
       " 'use_hook_tokens': False,\n",
       " 'use_local_attn': False,\n",
       " 'use_normalization_before_and_after': False,\n",
       " 'use_split_qkv_input': False,\n",
       " 'window_size': None}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt2.cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 4,  0,  1,  0],\n",
       "        [ 0,  0, -1, -3],\n",
       "        [-3,  0,  0,  2]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def prune_wanda(W, X_norm, sparse_ratio=0.5):\n",
    "    W_metric = W.abs() * X_norm\n",
    "    _, sorted_idx = W_metric.sort(dim=1)\n",
    "    pruned_idx = sorted_idx[:, :int(W.shape[1] * sparse_ratio)]\n",
    "    \n",
    "    W_clone = W.detach().clone()    \n",
    "    W_clone.scatter_(dim=1, index=pruned_idx, src=t.zeros_like(pruned_idx, dtype=W.dtype))\n",
    "    return W_clone\n",
    "\n",
    "W = t.tensor([\n",
    "    [4, 0, 1, -1],\n",
    "    [3, -2, -1, -3],\n",
    "    [-3, 1, 0, 2]\n",
    "])\n",
    "X = t.tensor([\n",
    "    [1, 2, 8, 3]\n",
    "])\n",
    "\n",
    "prune_wanda(W, X, sparse_ratio=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_model(model, hooks):\n",
    "    text = \"A quick brown fox jumps over the lazy dog.\"\n",
    "    tokens = model.to_tokens(text)\n",
    "    logits, cache = model.run_with_cache(tokens, remove_batch_dim=True)\n",
    "\n",
    "    for hook in hooks:\n",
    "        logits, cache = model.run_with_cache(tokens, remove_batch_dim=True)\n",
    "        layer = int(hook.split('.')[1])\n",
    "\n",
    "        if 'mlp' in hook:\n",
    "            if 'out' in hook:\n",
    "                X = cache[hook].norm(p=2, dim=0)\n",
    "                model.W_out[layer] = t.nn.Parameter(prune_wanda(model.W_out[layer], X, sparse_ratio=0.5))\n",
    "            else:\n",
    "                X = cache[hook]\n",
    "                X = X.norm(p=2, dim=0)\n",
    "                model.W_in[layer] = t.nn.Parameter(prune_wanda(model.W_in[layer], X, sparse_ratio=0.5))\n",
    "        else:\n",
    "            for head in range(model.cfg.n_heads):\n",
    "                X = cache[hook][:, head, :]  \n",
    "                X = X.norm(p=2, dim=0)\n",
    "\n",
    "                if 'q' in hook:\n",
    "                    model.W_Q[layer, head] = t.nn.Parameter(prune_wanda(model.W_Q[layer, head], X, sparse_ratio=0.5)\n",
    ")\n",
    "                if 'k' in hook:\n",
    "                    model.W_K[layer, head] = t.nn.Parameter(prune_wanda(model.W_K[layer, head], X, sparse_ratio=0.5))\n",
    "                \n",
    "                if 'v' in hook:\n",
    "                    model.W_V[layer, head] = t.nn.Parameter(prune_wanda(model.W_V[layer, head], X, sparse_ratio=0.5))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "gpt2_text = \"A quick brown fox jumps over the lazy dog.\"\n",
    "gpt2_tokens = gpt2.to_tokens(gpt2_text)\n",
    "gpt2_logits, gpt2_cache = gpt2.run_with_cache(gpt2_tokens, remove_batch_dim=True)\n",
    "\n",
    "hooks = []\n",
    "for k, v in gpt2_cache.cache_dict.items():\n",
    "    if 'block' in k and (v.dim() == 3 or 'mlp' in k):\n",
    "        if 'score' not in k and 'pattern' not in k:\n",
    "            hooks.append(k)\n",
    "\n",
    "len(hooks)\n",
    "pruned_gpt2 = prune_model(gpt2, hooks)\n",
    "gpt2: sae_lens.HookedSAETransformer = sae_lens.HookedSAETransformer.from_pretrained(\"gpt2-small\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0., device='cuda:0')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.norm(pruned_gpt2.W_Q - gpt2.W_Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized prompt: ['<|endoftext|>', 'I']\n",
      "Tokenized answer: [' am']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"font-weight: bold\">        Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">13.88</span><span style=\"font-weight: bold\"> Prob:  </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4.74</span><span style=\"font-weight: bold\">% Token: | am|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m3\u001b[0m\u001b[1m        Logit: \u001b[0m\u001b[1;36m13.88\u001b[0m\u001b[1m Prob:  \u001b[0m\u001b[1;36m4.74\u001b[0m\u001b[1m% Token: | am|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 14.56 Prob:  9.34% Token: |'m|\n",
      "Top 1th token. Logit: 14.51 Prob:  8.81% Token: |'ve|\n",
      "Top 2th token. Logit: 14.17 Prob:  6.31% Token: | have|\n",
      "Top 3th token. Logit: 13.88 Prob:  4.74% Token: | am|\n",
      "Top 4th token. Logit: 13.88 Prob:  4.69% Token: | was|\n",
      "Top 5th token. Logit: 13.39 Prob:  2.88% Token: | don|\n",
      "Top 6th token. Logit: 13.29 Prob:  2.61% Token: | think|\n",
      "Top 7th token. Logit: 13.15 Prob:  2.27% Token: | had|\n",
      "Top 8th token. Logit: 13.09 Prob:  2.14% Token: | know|\n",
      "Top 9th token. Logit: 12.82 Prob:  1.64% Token: | can|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">' am'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"font-weight: bold\">)]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m' am'\u001b[0m, \u001b[1;36m3\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"I\"\n",
    "answer = \" am\"\n",
    "transformer_lens.utils.test_prompt(prompt, answer, gpt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized prompt: ['<|endoftext|>', 'I']\n",
      "Tokenized answer: [' am']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"font-weight: bold\">        Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">13.88</span><span style=\"font-weight: bold\"> Prob:  </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4.74</span><span style=\"font-weight: bold\">% Token: | am|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m3\u001b[0m\u001b[1m        Logit: \u001b[0m\u001b[1;36m13.88\u001b[0m\u001b[1m Prob:  \u001b[0m\u001b[1;36m4.74\u001b[0m\u001b[1m% Token: | am|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 14.56 Prob:  9.34% Token: |'m|\n",
      "Top 1th token. Logit: 14.51 Prob:  8.81% Token: |'ve|\n",
      "Top 2th token. Logit: 14.17 Prob:  6.31% Token: | have|\n",
      "Top 3th token. Logit: 13.88 Prob:  4.74% Token: | am|\n",
      "Top 4th token. Logit: 13.88 Prob:  4.69% Token: | was|\n",
      "Top 5th token. Logit: 13.39 Prob:  2.88% Token: | don|\n",
      "Top 6th token. Logit: 13.29 Prob:  2.61% Token: | think|\n",
      "Top 7th token. Logit: 13.15 Prob:  2.27% Token: | had|\n",
      "Top 8th token. Logit: 13.09 Prob:  2.14% Token: | know|\n",
      "Top 9th token. Logit: 12.82 Prob:  1.64% Token: | can|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">' am'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"font-weight: bold\">)]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m' am'\u001b[0m, \u001b[1;36m3\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"I\"\n",
    "answer = \" am\"\n",
    "transformer_lens.utils.test_prompt(prompt, answer, pruned_gpt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([11, 12, 64])\n"
     ]
    }
   ],
   "source": [
    "X = gpt2_cache['blocks.0.attn.hook_q']\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embed.W_E torch.Size([50257, 768])\n"
     ]
    }
   ],
   "source": [
    "X = gpt2_cache['blocks.0.attn.hook_q']\n",
    "for name, param in gpt2.named_parameters():\n",
    "    print(name, param.shape)\n",
    "    if name == 'blocks.0.attn.W_Q':\n",
    "        W = param\n",
    "        param = prune_wanda(W, X, sparse_ratio=0.5)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wanda new try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embed.W_E torch.Size([50257, 768])\n",
      "pos_embed.W_pos torch.Size([1024, 768])\n",
      "blocks.0.attn.W_Q torch.Size([12, 768, 64])\n",
      "blocks.0.attn.W_O torch.Size([12, 64, 768])\n",
      "blocks.0.attn.b_Q torch.Size([12, 64])\n",
      "blocks.0.attn.b_O torch.Size([768])\n",
      "blocks.0.attn.W_K torch.Size([12, 768, 64])\n",
      "blocks.0.attn.W_V torch.Size([12, 768, 64])\n",
      "blocks.0.attn.b_K torch.Size([12, 64])\n",
      "blocks.0.attn.b_V torch.Size([12, 64])\n",
      "blocks.0.mlp.W_in torch.Size([768, 3072])\n",
      "blocks.0.mlp.b_in torch.Size([3072])\n",
      "blocks.0.mlp.W_out torch.Size([3072, 768])\n",
      "blocks.0.mlp.b_out torch.Size([768])\n",
      "blocks.1.attn.W_Q torch.Size([12, 768, 64])\n",
      "blocks.1.attn.W_O torch.Size([12, 64, 768])\n",
      "blocks.1.attn.b_Q torch.Size([12, 64])\n",
      "blocks.1.attn.b_O torch.Size([768])\n",
      "blocks.1.attn.W_K torch.Size([12, 768, 64])\n",
      "blocks.1.attn.W_V torch.Size([12, 768, 64])\n",
      "blocks.1.attn.b_K torch.Size([12, 64])\n",
      "blocks.1.attn.b_V torch.Size([12, 64])\n",
      "blocks.1.mlp.W_in torch.Size([768, 3072])\n",
      "blocks.1.mlp.b_in torch.Size([3072])\n",
      "blocks.1.mlp.W_out torch.Size([3072, 768])\n",
      "blocks.1.mlp.b_out torch.Size([768])\n",
      "blocks.2.attn.W_Q torch.Size([12, 768, 64])\n",
      "blocks.2.attn.W_O torch.Size([12, 64, 768])\n",
      "blocks.2.attn.b_Q torch.Size([12, 64])\n",
      "blocks.2.attn.b_O torch.Size([768])\n",
      "blocks.2.attn.W_K torch.Size([12, 768, 64])\n",
      "blocks.2.attn.W_V torch.Size([12, 768, 64])\n",
      "blocks.2.attn.b_K torch.Size([12, 64])\n",
      "blocks.2.attn.b_V torch.Size([12, 64])\n",
      "blocks.2.mlp.W_in torch.Size([768, 3072])\n",
      "blocks.2.mlp.b_in torch.Size([3072])\n",
      "blocks.2.mlp.W_out torch.Size([3072, 768])\n",
      "blocks.2.mlp.b_out torch.Size([768])\n",
      "blocks.3.attn.W_Q torch.Size([12, 768, 64])\n",
      "blocks.3.attn.W_O torch.Size([12, 64, 768])\n",
      "blocks.3.attn.b_Q torch.Size([12, 64])\n",
      "blocks.3.attn.b_O torch.Size([768])\n",
      "blocks.3.attn.W_K torch.Size([12, 768, 64])\n",
      "blocks.3.attn.W_V torch.Size([12, 768, 64])\n",
      "blocks.3.attn.b_K torch.Size([12, 64])\n",
      "blocks.3.attn.b_V torch.Size([12, 64])\n",
      "blocks.3.mlp.W_in torch.Size([768, 3072])\n",
      "blocks.3.mlp.b_in torch.Size([3072])\n",
      "blocks.3.mlp.W_out torch.Size([3072, 768])\n",
      "blocks.3.mlp.b_out torch.Size([768])\n",
      "blocks.4.attn.W_Q torch.Size([12, 768, 64])\n",
      "blocks.4.attn.W_O torch.Size([12, 64, 768])\n",
      "blocks.4.attn.b_Q torch.Size([12, 64])\n",
      "blocks.4.attn.b_O torch.Size([768])\n",
      "blocks.4.attn.W_K torch.Size([12, 768, 64])\n",
      "blocks.4.attn.W_V torch.Size([12, 768, 64])\n",
      "blocks.4.attn.b_K torch.Size([12, 64])\n",
      "blocks.4.attn.b_V torch.Size([12, 64])\n",
      "blocks.4.mlp.W_in torch.Size([768, 3072])\n",
      "blocks.4.mlp.b_in torch.Size([3072])\n",
      "blocks.4.mlp.W_out torch.Size([3072, 768])\n",
      "blocks.4.mlp.b_out torch.Size([768])\n",
      "blocks.5.attn.W_Q torch.Size([12, 768, 64])\n",
      "blocks.5.attn.W_O torch.Size([12, 64, 768])\n",
      "blocks.5.attn.b_Q torch.Size([12, 64])\n",
      "blocks.5.attn.b_O torch.Size([768])\n",
      "blocks.5.attn.W_K torch.Size([12, 768, 64])\n",
      "blocks.5.attn.W_V torch.Size([12, 768, 64])\n",
      "blocks.5.attn.b_K torch.Size([12, 64])\n",
      "blocks.5.attn.b_V torch.Size([12, 64])\n",
      "blocks.5.mlp.W_in torch.Size([768, 3072])\n",
      "blocks.5.mlp.b_in torch.Size([3072])\n",
      "blocks.5.mlp.W_out torch.Size([3072, 768])\n",
      "blocks.5.mlp.b_out torch.Size([768])\n",
      "blocks.6.attn.W_Q torch.Size([12, 768, 64])\n",
      "blocks.6.attn.W_O torch.Size([12, 64, 768])\n",
      "blocks.6.attn.b_Q torch.Size([12, 64])\n",
      "blocks.6.attn.b_O torch.Size([768])\n",
      "blocks.6.attn.W_K torch.Size([12, 768, 64])\n",
      "blocks.6.attn.W_V torch.Size([12, 768, 64])\n",
      "blocks.6.attn.b_K torch.Size([12, 64])\n",
      "blocks.6.attn.b_V torch.Size([12, 64])\n",
      "blocks.6.mlp.W_in torch.Size([768, 3072])\n",
      "blocks.6.mlp.b_in torch.Size([3072])\n",
      "blocks.6.mlp.W_out torch.Size([3072, 768])\n",
      "blocks.6.mlp.b_out torch.Size([768])\n",
      "blocks.7.attn.W_Q torch.Size([12, 768, 64])\n",
      "blocks.7.attn.W_O torch.Size([12, 64, 768])\n",
      "blocks.7.attn.b_Q torch.Size([12, 64])\n",
      "blocks.7.attn.b_O torch.Size([768])\n",
      "blocks.7.attn.W_K torch.Size([12, 768, 64])\n",
      "blocks.7.attn.W_V torch.Size([12, 768, 64])\n",
      "blocks.7.attn.b_K torch.Size([12, 64])\n",
      "blocks.7.attn.b_V torch.Size([12, 64])\n",
      "blocks.7.mlp.W_in torch.Size([768, 3072])\n",
      "blocks.7.mlp.b_in torch.Size([3072])\n",
      "blocks.7.mlp.W_out torch.Size([3072, 768])\n",
      "blocks.7.mlp.b_out torch.Size([768])\n",
      "blocks.8.attn.W_Q torch.Size([12, 768, 64])\n",
      "blocks.8.attn.W_O torch.Size([12, 64, 768])\n",
      "blocks.8.attn.b_Q torch.Size([12, 64])\n",
      "blocks.8.attn.b_O torch.Size([768])\n",
      "blocks.8.attn.W_K torch.Size([12, 768, 64])\n",
      "blocks.8.attn.W_V torch.Size([12, 768, 64])\n",
      "blocks.8.attn.b_K torch.Size([12, 64])\n",
      "blocks.8.attn.b_V torch.Size([12, 64])\n",
      "blocks.8.mlp.W_in torch.Size([768, 3072])\n",
      "blocks.8.mlp.b_in torch.Size([3072])\n",
      "blocks.8.mlp.W_out torch.Size([3072, 768])\n",
      "blocks.8.mlp.b_out torch.Size([768])\n",
      "blocks.9.attn.W_Q torch.Size([12, 768, 64])\n",
      "blocks.9.attn.W_O torch.Size([12, 64, 768])\n",
      "blocks.9.attn.b_Q torch.Size([12, 64])\n",
      "blocks.9.attn.b_O torch.Size([768])\n",
      "blocks.9.attn.W_K torch.Size([12, 768, 64])\n",
      "blocks.9.attn.W_V torch.Size([12, 768, 64])\n",
      "blocks.9.attn.b_K torch.Size([12, 64])\n",
      "blocks.9.attn.b_V torch.Size([12, 64])\n",
      "blocks.9.mlp.W_in torch.Size([768, 3072])\n",
      "blocks.9.mlp.b_in torch.Size([3072])\n",
      "blocks.9.mlp.W_out torch.Size([3072, 768])\n",
      "blocks.9.mlp.b_out torch.Size([768])\n",
      "blocks.10.attn.W_Q torch.Size([12, 768, 64])\n",
      "blocks.10.attn.W_O torch.Size([12, 64, 768])\n",
      "blocks.10.attn.b_Q torch.Size([12, 64])\n",
      "blocks.10.attn.b_O torch.Size([768])\n",
      "blocks.10.attn.W_K torch.Size([12, 768, 64])\n",
      "blocks.10.attn.W_V torch.Size([12, 768, 64])\n",
      "blocks.10.attn.b_K torch.Size([12, 64])\n",
      "blocks.10.attn.b_V torch.Size([12, 64])\n",
      "blocks.10.mlp.W_in torch.Size([768, 3072])\n",
      "blocks.10.mlp.b_in torch.Size([3072])\n",
      "blocks.10.mlp.W_out torch.Size([3072, 768])\n",
      "blocks.10.mlp.b_out torch.Size([768])\n",
      "blocks.11.attn.W_Q torch.Size([12, 768, 64])\n",
      "blocks.11.attn.W_O torch.Size([12, 64, 768])\n",
      "blocks.11.attn.b_Q torch.Size([12, 64])\n",
      "blocks.11.attn.b_O torch.Size([768])\n",
      "blocks.11.attn.W_K torch.Size([12, 768, 64])\n",
      "blocks.11.attn.W_V torch.Size([12, 768, 64])\n",
      "blocks.11.attn.b_K torch.Size([12, 64])\n",
      "blocks.11.attn.b_V torch.Size([12, 64])\n",
      "blocks.11.mlp.W_in torch.Size([768, 3072])\n",
      "blocks.11.mlp.b_in torch.Size([3072])\n",
      "blocks.11.mlp.W_out torch.Size([3072, 768])\n",
      "blocks.11.mlp.b_out torch.Size([768])\n",
      "unembed.W_U torch.Size([768, 50257])\n",
      "unembed.b_U torch.Size([50257])\n"
     ]
    }
   ],
   "source": [
    "for name, param in pruned_gpt2.named_parameters():\n",
    "    print(name, param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hook_embed torch.Size([11, 768])\n",
      "hook_pos_embed torch.Size([11, 768])\n",
      "blocks.0.hook_resid_pre torch.Size([11, 768])\n",
      "blocks.0.ln1.hook_scale torch.Size([11, 1])\n",
      "blocks.0.ln1.hook_normalized torch.Size([11, 768])\n",
      "blocks.0.attn.hook_q torch.Size([11, 12, 64])\n",
      "blocks.0.attn.hook_k torch.Size([11, 12, 64])\n",
      "blocks.0.attn.hook_v torch.Size([11, 12, 64])\n",
      "blocks.0.attn.hook_attn_scores torch.Size([12, 11, 11])\n",
      "blocks.0.attn.hook_pattern torch.Size([12, 11, 11])\n",
      "blocks.0.attn.hook_z torch.Size([11, 12, 64])\n",
      "blocks.0.hook_attn_out torch.Size([11, 768])\n",
      "blocks.0.hook_resid_mid torch.Size([11, 768])\n",
      "blocks.0.ln2.hook_scale torch.Size([11, 1])\n",
      "blocks.0.ln2.hook_normalized torch.Size([11, 768])\n",
      "blocks.0.mlp.hook_pre torch.Size([11, 3072])\n",
      "blocks.0.mlp.hook_post torch.Size([11, 3072])\n",
      "blocks.0.hook_mlp_out torch.Size([11, 768])\n",
      "blocks.0.hook_resid_post torch.Size([11, 768])\n",
      "blocks.1.hook_resid_pre torch.Size([11, 768])\n",
      "blocks.1.ln1.hook_scale torch.Size([11, 1])\n",
      "blocks.1.ln1.hook_normalized torch.Size([11, 768])\n",
      "blocks.1.attn.hook_q torch.Size([11, 12, 64])\n",
      "blocks.1.attn.hook_k torch.Size([11, 12, 64])\n",
      "blocks.1.attn.hook_v torch.Size([11, 12, 64])\n",
      "blocks.1.attn.hook_attn_scores torch.Size([12, 11, 11])\n",
      "blocks.1.attn.hook_pattern torch.Size([12, 11, 11])\n",
      "blocks.1.attn.hook_z torch.Size([11, 12, 64])\n",
      "blocks.1.hook_attn_out torch.Size([11, 768])\n",
      "blocks.1.hook_resid_mid torch.Size([11, 768])\n",
      "blocks.1.ln2.hook_scale torch.Size([11, 1])\n",
      "blocks.1.ln2.hook_normalized torch.Size([11, 768])\n",
      "blocks.1.mlp.hook_pre torch.Size([11, 3072])\n",
      "blocks.1.mlp.hook_post torch.Size([11, 3072])\n",
      "blocks.1.hook_mlp_out torch.Size([11, 768])\n",
      "blocks.1.hook_resid_post torch.Size([11, 768])\n",
      "blocks.2.hook_resid_pre torch.Size([11, 768])\n",
      "blocks.2.ln1.hook_scale torch.Size([11, 1])\n",
      "blocks.2.ln1.hook_normalized torch.Size([11, 768])\n",
      "blocks.2.attn.hook_q torch.Size([11, 12, 64])\n",
      "blocks.2.attn.hook_k torch.Size([11, 12, 64])\n",
      "blocks.2.attn.hook_v torch.Size([11, 12, 64])\n",
      "blocks.2.attn.hook_attn_scores torch.Size([12, 11, 11])\n",
      "blocks.2.attn.hook_pattern torch.Size([12, 11, 11])\n",
      "blocks.2.attn.hook_z torch.Size([11, 12, 64])\n",
      "blocks.2.hook_attn_out torch.Size([11, 768])\n",
      "blocks.2.hook_resid_mid torch.Size([11, 768])\n",
      "blocks.2.ln2.hook_scale torch.Size([11, 1])\n",
      "blocks.2.ln2.hook_normalized torch.Size([11, 768])\n",
      "blocks.2.mlp.hook_pre torch.Size([11, 3072])\n",
      "blocks.2.mlp.hook_post torch.Size([11, 3072])\n",
      "blocks.2.hook_mlp_out torch.Size([11, 768])\n",
      "blocks.2.hook_resid_post torch.Size([11, 768])\n",
      "blocks.3.hook_resid_pre torch.Size([11, 768])\n",
      "blocks.3.ln1.hook_scale torch.Size([11, 1])\n",
      "blocks.3.ln1.hook_normalized torch.Size([11, 768])\n",
      "blocks.3.attn.hook_q torch.Size([11, 12, 64])\n",
      "blocks.3.attn.hook_k torch.Size([11, 12, 64])\n",
      "blocks.3.attn.hook_v torch.Size([11, 12, 64])\n",
      "blocks.3.attn.hook_attn_scores torch.Size([12, 11, 11])\n",
      "blocks.3.attn.hook_pattern torch.Size([12, 11, 11])\n",
      "blocks.3.attn.hook_z torch.Size([11, 12, 64])\n",
      "blocks.3.hook_attn_out torch.Size([11, 768])\n",
      "blocks.3.hook_resid_mid torch.Size([11, 768])\n",
      "blocks.3.ln2.hook_scale torch.Size([11, 1])\n",
      "blocks.3.ln2.hook_normalized torch.Size([11, 768])\n",
      "blocks.3.mlp.hook_pre torch.Size([11, 3072])\n",
      "blocks.3.mlp.hook_post torch.Size([11, 3072])\n",
      "blocks.3.hook_mlp_out torch.Size([11, 768])\n",
      "blocks.3.hook_resid_post torch.Size([11, 768])\n",
      "blocks.4.hook_resid_pre torch.Size([11, 768])\n",
      "blocks.4.ln1.hook_scale torch.Size([11, 1])\n",
      "blocks.4.ln1.hook_normalized torch.Size([11, 768])\n",
      "blocks.4.attn.hook_q torch.Size([11, 12, 64])\n",
      "blocks.4.attn.hook_k torch.Size([11, 12, 64])\n",
      "blocks.4.attn.hook_v torch.Size([11, 12, 64])\n",
      "blocks.4.attn.hook_attn_scores torch.Size([12, 11, 11])\n",
      "blocks.4.attn.hook_pattern torch.Size([12, 11, 11])\n",
      "blocks.4.attn.hook_z torch.Size([11, 12, 64])\n",
      "blocks.4.hook_attn_out torch.Size([11, 768])\n",
      "blocks.4.hook_resid_mid torch.Size([11, 768])\n",
      "blocks.4.ln2.hook_scale torch.Size([11, 1])\n",
      "blocks.4.ln2.hook_normalized torch.Size([11, 768])\n",
      "blocks.4.mlp.hook_pre torch.Size([11, 3072])\n",
      "blocks.4.mlp.hook_post torch.Size([11, 3072])\n",
      "blocks.4.hook_mlp_out torch.Size([11, 768])\n",
      "blocks.4.hook_resid_post torch.Size([11, 768])\n",
      "blocks.5.hook_resid_pre torch.Size([11, 768])\n",
      "blocks.5.ln1.hook_scale torch.Size([11, 1])\n",
      "blocks.5.ln1.hook_normalized torch.Size([11, 768])\n",
      "blocks.5.attn.hook_q torch.Size([11, 12, 64])\n",
      "blocks.5.attn.hook_k torch.Size([11, 12, 64])\n",
      "blocks.5.attn.hook_v torch.Size([11, 12, 64])\n",
      "blocks.5.attn.hook_attn_scores torch.Size([12, 11, 11])\n",
      "blocks.5.attn.hook_pattern torch.Size([12, 11, 11])\n",
      "blocks.5.attn.hook_z torch.Size([11, 12, 64])\n",
      "blocks.5.hook_attn_out torch.Size([11, 768])\n",
      "blocks.5.hook_resid_mid torch.Size([11, 768])\n",
      "blocks.5.ln2.hook_scale torch.Size([11, 1])\n",
      "blocks.5.ln2.hook_normalized torch.Size([11, 768])\n",
      "blocks.5.mlp.hook_pre torch.Size([11, 3072])\n",
      "blocks.5.mlp.hook_post torch.Size([11, 3072])\n",
      "blocks.5.hook_mlp_out torch.Size([11, 768])\n",
      "blocks.5.hook_resid_post torch.Size([11, 768])\n",
      "blocks.6.hook_resid_pre torch.Size([11, 768])\n",
      "blocks.6.ln1.hook_scale torch.Size([11, 1])\n",
      "blocks.6.ln1.hook_normalized torch.Size([11, 768])\n",
      "blocks.6.attn.hook_q torch.Size([11, 12, 64])\n",
      "blocks.6.attn.hook_k torch.Size([11, 12, 64])\n",
      "blocks.6.attn.hook_v torch.Size([11, 12, 64])\n",
      "blocks.6.attn.hook_attn_scores torch.Size([12, 11, 11])\n",
      "blocks.6.attn.hook_pattern torch.Size([12, 11, 11])\n",
      "blocks.6.attn.hook_z torch.Size([11, 12, 64])\n",
      "blocks.6.hook_attn_out torch.Size([11, 768])\n",
      "blocks.6.hook_resid_mid torch.Size([11, 768])\n",
      "blocks.6.ln2.hook_scale torch.Size([11, 1])\n",
      "blocks.6.ln2.hook_normalized torch.Size([11, 768])\n",
      "blocks.6.mlp.hook_pre torch.Size([11, 3072])\n",
      "blocks.6.mlp.hook_post torch.Size([11, 3072])\n",
      "blocks.6.hook_mlp_out torch.Size([11, 768])\n",
      "blocks.6.hook_resid_post torch.Size([11, 768])\n",
      "blocks.7.hook_resid_pre torch.Size([11, 768])\n",
      "blocks.7.ln1.hook_scale torch.Size([11, 1])\n",
      "blocks.7.ln1.hook_normalized torch.Size([11, 768])\n",
      "blocks.7.attn.hook_q torch.Size([11, 12, 64])\n",
      "blocks.7.attn.hook_k torch.Size([11, 12, 64])\n",
      "blocks.7.attn.hook_v torch.Size([11, 12, 64])\n",
      "blocks.7.attn.hook_attn_scores torch.Size([12, 11, 11])\n",
      "blocks.7.attn.hook_pattern torch.Size([12, 11, 11])\n",
      "blocks.7.attn.hook_z torch.Size([11, 12, 64])\n",
      "blocks.7.hook_attn_out torch.Size([11, 768])\n",
      "blocks.7.hook_resid_mid torch.Size([11, 768])\n",
      "blocks.7.ln2.hook_scale torch.Size([11, 1])\n",
      "blocks.7.ln2.hook_normalized torch.Size([11, 768])\n",
      "blocks.7.mlp.hook_pre torch.Size([11, 3072])\n",
      "blocks.7.mlp.hook_post torch.Size([11, 3072])\n",
      "blocks.7.hook_mlp_out torch.Size([11, 768])\n",
      "blocks.7.hook_resid_post torch.Size([11, 768])\n",
      "blocks.8.hook_resid_pre torch.Size([11, 768])\n",
      "blocks.8.ln1.hook_scale torch.Size([11, 1])\n",
      "blocks.8.ln1.hook_normalized torch.Size([11, 768])\n",
      "blocks.8.attn.hook_q torch.Size([11, 12, 64])\n",
      "blocks.8.attn.hook_k torch.Size([11, 12, 64])\n",
      "blocks.8.attn.hook_v torch.Size([11, 12, 64])\n",
      "blocks.8.attn.hook_attn_scores torch.Size([12, 11, 11])\n",
      "blocks.8.attn.hook_pattern torch.Size([12, 11, 11])\n",
      "blocks.8.attn.hook_z torch.Size([11, 12, 64])\n",
      "blocks.8.hook_attn_out torch.Size([11, 768])\n",
      "blocks.8.hook_resid_mid torch.Size([11, 768])\n",
      "blocks.8.ln2.hook_scale torch.Size([11, 1])\n",
      "blocks.8.ln2.hook_normalized torch.Size([11, 768])\n",
      "blocks.8.mlp.hook_pre torch.Size([11, 3072])\n",
      "blocks.8.mlp.hook_post torch.Size([11, 3072])\n",
      "blocks.8.hook_mlp_out torch.Size([11, 768])\n",
      "blocks.8.hook_resid_post torch.Size([11, 768])\n",
      "blocks.9.hook_resid_pre torch.Size([11, 768])\n",
      "blocks.9.ln1.hook_scale torch.Size([11, 1])\n",
      "blocks.9.ln1.hook_normalized torch.Size([11, 768])\n",
      "blocks.9.attn.hook_q torch.Size([11, 12, 64])\n",
      "blocks.9.attn.hook_k torch.Size([11, 12, 64])\n",
      "blocks.9.attn.hook_v torch.Size([11, 12, 64])\n",
      "blocks.9.attn.hook_attn_scores torch.Size([12, 11, 11])\n",
      "blocks.9.attn.hook_pattern torch.Size([12, 11, 11])\n",
      "blocks.9.attn.hook_z torch.Size([11, 12, 64])\n",
      "blocks.9.hook_attn_out torch.Size([11, 768])\n",
      "blocks.9.hook_resid_mid torch.Size([11, 768])\n",
      "blocks.9.ln2.hook_scale torch.Size([11, 1])\n",
      "blocks.9.ln2.hook_normalized torch.Size([11, 768])\n",
      "blocks.9.mlp.hook_pre torch.Size([11, 3072])\n",
      "blocks.9.mlp.hook_post torch.Size([11, 3072])\n",
      "blocks.9.hook_mlp_out torch.Size([11, 768])\n",
      "blocks.9.hook_resid_post torch.Size([11, 768])\n",
      "blocks.10.hook_resid_pre torch.Size([11, 768])\n",
      "blocks.10.ln1.hook_scale torch.Size([11, 1])\n",
      "blocks.10.ln1.hook_normalized torch.Size([11, 768])\n",
      "blocks.10.attn.hook_q torch.Size([11, 12, 64])\n",
      "blocks.10.attn.hook_k torch.Size([11, 12, 64])\n",
      "blocks.10.attn.hook_v torch.Size([11, 12, 64])\n",
      "blocks.10.attn.hook_attn_scores torch.Size([12, 11, 11])\n",
      "blocks.10.attn.hook_pattern torch.Size([12, 11, 11])\n",
      "blocks.10.attn.hook_z torch.Size([11, 12, 64])\n",
      "blocks.10.hook_attn_out torch.Size([11, 768])\n",
      "blocks.10.hook_resid_mid torch.Size([11, 768])\n",
      "blocks.10.ln2.hook_scale torch.Size([11, 1])\n",
      "blocks.10.ln2.hook_normalized torch.Size([11, 768])\n",
      "blocks.10.mlp.hook_pre torch.Size([11, 3072])\n",
      "blocks.10.mlp.hook_post torch.Size([11, 3072])\n",
      "blocks.10.hook_mlp_out torch.Size([11, 768])\n",
      "blocks.10.hook_resid_post torch.Size([11, 768])\n",
      "blocks.11.hook_resid_pre torch.Size([11, 768])\n",
      "blocks.11.ln1.hook_scale torch.Size([11, 1])\n",
      "blocks.11.ln1.hook_normalized torch.Size([11, 768])\n",
      "blocks.11.attn.hook_q torch.Size([11, 12, 64])\n",
      "blocks.11.attn.hook_k torch.Size([11, 12, 64])\n",
      "blocks.11.attn.hook_v torch.Size([11, 12, 64])\n",
      "blocks.11.attn.hook_attn_scores torch.Size([12, 11, 11])\n",
      "blocks.11.attn.hook_pattern torch.Size([12, 11, 11])\n",
      "blocks.11.attn.hook_z torch.Size([11, 12, 64])\n",
      "blocks.11.hook_attn_out torch.Size([11, 768])\n",
      "blocks.11.hook_resid_mid torch.Size([11, 768])\n",
      "blocks.11.ln2.hook_scale torch.Size([11, 1])\n",
      "blocks.11.ln2.hook_normalized torch.Size([11, 768])\n",
      "blocks.11.mlp.hook_pre torch.Size([11, 3072])\n",
      "blocks.11.mlp.hook_post torch.Size([11, 3072])\n",
      "blocks.11.hook_mlp_out torch.Size([11, 768])\n",
      "blocks.11.hook_resid_post torch.Size([11, 768])\n",
      "ln_final.hook_scale torch.Size([11, 1])\n",
      "ln_final.hook_normalized torch.Size([11, 768])\n"
     ]
    }
   ],
   "source": [
    "gpt2_text = \"A quick brown fox jumps over the lazy dog.\"\n",
    "gpt2_tokens = gpt2.to_tokens(gpt2_text)\n",
    "gpt2_logits, gpt2_cache = gpt2.run_with_cache(gpt2_tokens, remove_batch_dim=True)\n",
    "\n",
    "for k, v in gpt2_cache.cache_dict.items():\n",
    "    print(k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attn.W_Q attn.hook_q\n",
      "attn.W_K attn.hook_k\n",
      "attn.W_V attn.hook_v\n",
      "attn.W_O hook_attn_out\n",
      "mlp.W_in mlp.hook_pre\n",
      "mlp.W_out hook_mlp_out\n"
     ]
    }
   ],
   "source": [
    "acts = ['attn.hook_q','attn.hook_k','attn.hook_v','hook_attn_out','mlp.hook_pre','hook_mlp_out']\n",
    "wts = ['attn.W_Q', 'attn.W_K', 'attn.W_V', 'mlp.W_in', 'mlp.W_out']\n",
    "\n",
    "wts_act = {\n",
    "    'attn.W_Q': 'attn.hook_q',\n",
    "    'attn.W_K': 'attn.hook_k',\n",
    "    'attn.W_V': 'attn.hook_v',\n",
    "    'attn.W_O': 'hook_attn_out',\n",
    "    'mlp.W_in': 'mlp.hook_pre',\n",
    "    'mlp.W_out': 'hook_mlp_out'\n",
    "}\n",
    "\n",
    "for wt, act in wts_act.items():\n",
    "    print(wt, act)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12, 768, 64])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt2.get_parameter('blocks.0.attn.W_Q').shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n",
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def prune_wanda(W, X_norm, sparse_ratio=0.5):\n",
    "    W_metric = W.abs() * X_norm\n",
    "    _, sorted_idx = W_metric.sort(dim=1)\n",
    "    pruned_idx = sorted_idx[:, :int(W.shape[1] * sparse_ratio)]\n",
    "    \n",
    "    W_clone = W.detach().clone()    \n",
    "    W_clone.scatter_(dim=1, index=pruned_idx, src=t.zeros_like(pruned_idx, dtype=W.dtype))\n",
    "    return W_clone\n",
    "\n",
    "def prune_model(model, tokens):\n",
    "    wts_act = {\n",
    "    'attn.W_Q': 'attn.hook_q',\n",
    "    'attn.W_K': 'attn.hook_k',\n",
    "    'attn.W_V': 'attn.hook_v',\n",
    "    'attn.W_O': 'hook_attn_out',\n",
    "    'mlp.W_in': 'mlp.hook_pre',\n",
    "    'mlp.W_out': 'hook_mlp_out'\n",
    "    }\n",
    "    for layer in range(model.cfg.n_layers):\n",
    "        logits, cache = model.run_with_cache(tokens, remove_batch_dim=True)\n",
    "        for wt, act in wts_act.items():\n",
    "            W = model.get_parameter(f'blocks.{layer}.{wt}')\n",
    "            X = cache[f'blocks.{layer}.{act}']\n",
    "\n",
    "            if W.dim() == 3:\n",
    "                if 'W_O' in wt:\n",
    "                    X_norm = X.norm(p=2, dim=0)\n",
    "                    for head in range(W.shape[0]):\n",
    "                        W[head] = prune_wanda(W[head], X_norm, sparse_ratio=0.5)\n",
    "                        \n",
    "                else:\n",
    "                    for head in range(W.shape[0]):\n",
    "                        X_norm = X[:, head, :].norm(p=2, dim=0)\n",
    "                        W[head] = prune_wanda(W[head], X_norm, sparse_ratio=0.5)\n",
    "            else:\n",
    "                X_norm = X.norm(p=2, dim=0)\n",
    "                W = prune_wanda(W, X_norm, sparse_ratio=0.5)\n",
    "\n",
    "    return model\n",
    "\n",
    "gpt2: sae_lens.HookedSAETransformer = sae_lens.HookedSAETransformer.from_pretrained(\"gpt2-small\", device=device)\n",
    "gpt2_text = \"A quick brown fox jumps over the lazy dog.\"\n",
    "gpt2_tokens = gpt2.to_tokens(gpt2_text)\n",
    "pruned_gpt2 = prune_model(gpt2, gpt2_tokens)\n",
    "gpt2: sae_lens.HookedSAETransformer = sae_lens.HookedSAETransformer.from_pretrained(\"gpt2-small\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized prompt: ['<|endoftext|>', 'I']\n",
      "Tokenized answer: [' am']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"font-weight: bold\">        Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">13.88</span><span style=\"font-weight: bold\"> Prob:  </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4.74</span><span style=\"font-weight: bold\">% Token: | am|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m3\u001b[0m\u001b[1m        Logit: \u001b[0m\u001b[1;36m13.88\u001b[0m\u001b[1m Prob:  \u001b[0m\u001b[1;36m4.74\u001b[0m\u001b[1m% Token: | am|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 14.56 Prob:  9.34% Token: |'m|\n",
      "Top 1th token. Logit: 14.51 Prob:  8.81% Token: |'ve|\n",
      "Top 2th token. Logit: 14.17 Prob:  6.31% Token: | have|\n",
      "Top 3th token. Logit: 13.88 Prob:  4.74% Token: | am|\n",
      "Top 4th token. Logit: 13.88 Prob:  4.69% Token: | was|\n",
      "Top 5th token. Logit: 13.39 Prob:  2.88% Token: | don|\n",
      "Top 6th token. Logit: 13.29 Prob:  2.61% Token: | think|\n",
      "Top 7th token. Logit: 13.15 Prob:  2.27% Token: | had|\n",
      "Top 8th token. Logit: 13.09 Prob:  2.14% Token: | know|\n",
      "Top 9th token. Logit: 12.82 Prob:  1.64% Token: | can|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">' am'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"font-weight: bold\">)]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m' am'\u001b[0m, \u001b[1;36m3\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"I\"\n",
    "answer = \" am\"\n",
    "transformer_lens.utils.test_prompt(prompt, answer, gpt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized prompt: ['<|endoftext|>', 'I']\n",
      "Tokenized answer: [' am']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"font-weight: bold\">        Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">14.87</span><span style=\"font-weight: bold\"> Prob:  </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4.20</span><span style=\"font-weight: bold\">% Token: | am|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m4\u001b[0m\u001b[1m        Logit: \u001b[0m\u001b[1;36m14.87\u001b[0m\u001b[1m Prob:  \u001b[0m\u001b[1;36m4.20\u001b[0m\u001b[1m% Token: | am|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 15.69 Prob:  9.48% Token: | have|\n",
      "Top 1th token. Logit: 15.55 Prob:  8.25% Token: | was|\n",
      "Top 2th token. Logit: 15.42 Prob:  7.22% Token: |'m|\n",
      "Top 3th token. Logit: 15.28 Prob:  6.32% Token: |'ve|\n",
      "Top 4th token. Logit: 14.87 Prob:  4.20% Token: | am|\n",
      "Top 5th token. Logit: 14.68 Prob:  3.46% Token: | think|\n",
      "Top 6th token. Logit: 14.64 Prob:  3.33% Token: | don|\n",
      "Top 7th token. Logit: 14.57 Prob:  3.11% Token: | had|\n",
      "Top 8th token. Logit: 14.46 Prob:  2.78% Token: | know|\n",
      "Top 9th token. Logit: 14.32 Prob:  2.42% Token: | can|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">' am'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"font-weight: bold\">)]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m' am'\u001b[0m, \u001b[1;36m4\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"I\"\n",
    "answer = \" am\"\n",
    "transformer_lens.utils.test_prompt(prompt, answer, pruned_gpt2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gemma2\n",
    "\n",
    "Issue: Pretrained SAE is not available for the model - for attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You tried to specify center_unembed=True for a model using logit softcap, but this can't be done! Softcapping is not invariant upon adding a constant Setting center_unembed=False instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d43d88923b19449098cb3f88e93afc93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gemma-2-2b into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "gemma2b: sae_lens.HookedSAETransformer = sae_lens.HookedSAETransformer.from_pretrained(\"gemma-2-2b\", device=device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SAE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

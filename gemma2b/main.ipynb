{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import itertools\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "from collections import Counter, defaultdict\n",
    "from copy import deepcopy\n",
    "from dataclasses import dataclass\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "from typing import Any, Callable, Literal, TypeAlias\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch as t\n",
    "from datasets import load_dataset\n",
    "import transformer_lens\n",
    "import sae_lens\n",
    "\n",
    "import einops\n",
    "import circuitsvis as cv\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from IPython.display import HTML, IFrame, clear_output, display\n",
    "from jaxtyping import Float, Int\n",
    "from openai import OpenAI\n",
    "from rich import print as rprint\n",
    "from rich.table import Table\n",
    "from tabulate import tabulate\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Issues: \n",
    "\n",
    "* Pretrained SAEs not available for attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You tried to specify center_unembed=True for a model using logit softcap, but this can't be done! Softcapping is not invariant upon adding a constant Setting center_unembed=False instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91f226e46c684ddfb966cd34c7e6d375",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gemma-2-2b into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "gemma2b: sae_lens.HookedSAETransformer = sae_lens.HookedSAETransformer.from_pretrained(\"gemma-2-2b\", device=\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HookedSAETransformer(\n",
       "  (embed): Embed()\n",
       "  (hook_embed): HookPoint()\n",
       "  (blocks): ModuleList(\n",
       "    (0-25): 26 x TransformerBlock(\n",
       "      (ln1): RMSNormPre(\n",
       "        (hook_scale): HookPoint()\n",
       "        (hook_normalized): HookPoint()\n",
       "      )\n",
       "      (ln1_post): RMSNorm(\n",
       "        (hook_scale): HookPoint()\n",
       "        (hook_normalized): HookPoint()\n",
       "      )\n",
       "      (ln2): RMSNormPre(\n",
       "        (hook_scale): HookPoint()\n",
       "        (hook_normalized): HookPoint()\n",
       "      )\n",
       "      (ln2_post): RMSNorm(\n",
       "        (hook_scale): HookPoint()\n",
       "        (hook_normalized): HookPoint()\n",
       "      )\n",
       "      (attn): GroupedQueryAttention(\n",
       "        (hook_k): HookPoint()\n",
       "        (hook_q): HookPoint()\n",
       "        (hook_v): HookPoint()\n",
       "        (hook_z): HookPoint()\n",
       "        (hook_attn_scores): HookPoint()\n",
       "        (hook_pattern): HookPoint()\n",
       "        (hook_result): HookPoint()\n",
       "        (hook_rot_k): HookPoint()\n",
       "        (hook_rot_q): HookPoint()\n",
       "      )\n",
       "      (mlp): GatedMLP(\n",
       "        (hook_pre): HookPoint()\n",
       "        (hook_pre_linear): HookPoint()\n",
       "        (hook_post): HookPoint()\n",
       "      )\n",
       "      (hook_attn_in): HookPoint()\n",
       "      (hook_q_input): HookPoint()\n",
       "      (hook_k_input): HookPoint()\n",
       "      (hook_v_input): HookPoint()\n",
       "      (hook_mlp_in): HookPoint()\n",
       "      (hook_attn_out): HookPoint()\n",
       "      (hook_mlp_out): HookPoint()\n",
       "      (hook_resid_pre): HookPoint()\n",
       "      (hook_resid_mid): HookPoint()\n",
       "      (hook_resid_post): HookPoint()\n",
       "    )\n",
       "  )\n",
       "  (ln_final): RMSNormPre(\n",
       "    (hook_scale): HookPoint()\n",
       "    (hook_normalized): HookPoint()\n",
       "  )\n",
       "  (unembed): Unembed()\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gemma2b.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized prompt: ['<bos>', 'M', 'itig', 'ating', ' the', ' risk', ' of', ' extinction', ' from', ' AI', ' should', ' be', ' a', ' global']\n",
      "Tokenized answer: [' priority']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">        Logit:  </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8.54</span><span style=\"font-weight: bold\"> Prob: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">96.44</span><span style=\"font-weight: bold\">% Token: | priority|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m0\u001b[0m\u001b[1m        Logit:  \u001b[0m\u001b[1;36m8.54\u001b[0m\u001b[1m Prob: \u001b[0m\u001b[1;36m96.44\u001b[0m\u001b[1m% Token: | priority|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit:  8.54 Prob: 96.44% Token: | priority|\n",
      "Top 1th token. Logit:  4.57 Prob:  1.81% Token: | imperative|\n",
      "Top 2th token. Logit:  3.13 Prob:  0.43% Token: | challenge|\n",
      "Top 3th token. Logit:  2.82 Prob:  0.31% Token: | goal|\n",
      "Top 4th token. Logit:  2.59 Prob:  0.25% Token: | effort|\n",
      "Top 5th token. Logit:  1.91 Prob:  0.13% Token: | concern|\n",
      "Top 6th token. Logit:  1.21 Prob:  0.06% Token: | security|\n",
      "Top 7th token. Logit:  1.21 Prob:  0.06% Token: | public|\n",
      "Top 8th token. Logit:  0.92 Prob:  0.05% Token: |,|\n",
      "Top 9th token. Logit:  0.84 Prob:  0.04% Token: | activity|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">' priority'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">)]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m' priority'\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized prompt: ['<bos>', 'Mary', ' and', ' John', ' went', ' to', ' the', ' park', ' to', ' play', '.', ' Mary', ' gave', ' the', ' ball', ' to']\n",
      "Tokenized answer: [' John']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">        Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">27.41</span><span style=\"font-weight: bold\"> Prob: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">85.00</span><span style=\"font-weight: bold\">% Token: | John|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m0\u001b[0m\u001b[1m        Logit: \u001b[0m\u001b[1;36m27.41\u001b[0m\u001b[1m Prob: \u001b[0m\u001b[1;36m85.00\u001b[0m\u001b[1m% Token: | John|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 27.41 Prob: 85.00% Token: | John|\n",
      "Top 1th token. Logit: 24.90 Prob:  6.86% Token: | her|\n",
      "Top 2th token. Logit: 23.82 Prob:  2.34% Token: | the|\n",
      "Top 3th token. Logit: 23.37 Prob:  1.48% Token: | john|\n",
      "Top 4th token. Logit: 22.98 Prob:  1.01% Token: | a|\n",
      "Top 5th token. Logit: 22.09 Prob:  0.42% Token: | Mary|\n",
      "Top 6th token. Logit: 21.95 Prob:  0.36% Token: | Johnny|\n",
      "Top 7th token. Logit: 21.60 Prob:  0.25% Token: | him|\n",
      "Top 8th token. Logit: 21.35 Prob:  0.20% Token: | Jack|\n",
      "Top 9th token. Logit: 20.59 Prob:  0.09% Token: | Joe|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">' John'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">)]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m' John'\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"Mitigating the risk of extinction from AI should be a global\"\n",
    "answer = \" priority\"\n",
    "transformer_lens.utils.test_prompt(prompt, answer, gemma2b)\n",
    "\n",
    "prompt = \"Mary and John went to the park to play. Mary gave the ball to\"\n",
    "answer = \" John\"\n",
    "\n",
    "transformer_lens.utils.test_prompt(prompt, answer, gemma2b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You tried to specify center_unembed=True for a model using logit softcap, but this can't be done! Softcapping is not invariant upon adding a constant Setting center_unembed=False instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09961ee05b8845b1be3e1cf35622277b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gemma-2-2b into HookedTransformer\n",
      "Tokenized prompt: ['<bos>', 'M', 'itig', 'ating', ' the', ' risk', ' of', ' extinction', ' from', ' AI', ' should', ' be', ' a', ' global']\n",
      "Tokenized answer: [' priority']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">        Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">24.92</span><span style=\"font-weight: bold\"> Prob: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">43.13</span><span style=\"font-weight: bold\">% Token: | priority|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m0\u001b[0m\u001b[1m        Logit: \u001b[0m\u001b[1;36m24.92\u001b[0m\u001b[1m Prob: \u001b[0m\u001b[1;36m43.13\u001b[0m\u001b[1m% Token: | priority|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 24.92 Prob: 43.13% Token: | priority|\n",
      "Top 1th token. Logit: 23.44 Prob:  9.77% Token: | effort|\n",
      "Top 2th token. Logit: 23.02 Prob:  6.44% Token: | goal|\n",
      "Top 3th token. Logit: 22.70 Prob:  4.68% Token: | concern|\n",
      "Top 4th token. Logit: 22.47 Prob:  3.71% Token: | responsibility|\n",
      "Top 5th token. Logit: 22.26 Prob:  3.02% Token: | endeavor|\n",
      "Top 6th token. Logit: 22.21 Prob:  2.86% Token: |,|\n",
      "Top 7th token. Logit: 22.08 Prob:  2.51% Token: | public|\n",
      "Top 8th token. Logit: 21.74 Prob:  1.79% Token: | imperative|\n",
      "Top 9th token. Logit: 21.48 Prob:  1.38% Token: | focus|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">' priority'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">)]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m' priority'\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized prompt: ['<bos>', 'Mary', ' and', ' John', ' went', ' to', ' the', ' park', ' to', ' play', '.', ' Mary', ' gave', ' the', ' ball', ' to']\n",
      "Tokenized answer: [' John']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">        Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26.82</span><span style=\"font-weight: bold\"> Prob: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">77.71</span><span style=\"font-weight: bold\">% Token: | John|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m0\u001b[0m\u001b[1m        Logit: \u001b[0m\u001b[1;36m26.82\u001b[0m\u001b[1m Prob: \u001b[0m\u001b[1;36m77.71\u001b[0m\u001b[1m% Token: | John|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 26.82 Prob: 77.71% Token: | John|\n",
      "Top 1th token. Logit: 24.56 Prob:  8.12% Token: | her|\n",
      "Top 2th token. Logit: 24.33 Prob:  6.44% Token: | the|\n",
      "Top 3th token. Logit: 23.09 Prob:  1.87% Token: | a|\n",
      "Top 4th token. Logit: 22.24 Prob:  0.80% Token: | john|\n",
      "Top 5th token. Logit: 21.89 Prob:  0.56% Token: | Johnny|\n",
      "Top 6th token. Logit: 21.62 Prob:  0.43% Token: | him|\n",
      "Top 7th token. Logit: 21.62 Prob:  0.43% Token: | Mary|\n",
      "Top 8th token. Logit: 21.49 Prob:  0.38% Token: | |\n",
      "Top 9th token. Logit: 21.37 Prob:  0.33% Token: | one|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">' John'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">)]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m' John'\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pruned_gemma2b: sae_lens.HookedSAETransformer = sae_lens.HookedSAETransformer.from_pretrained(\"gemma-2-2b\", device=\"cuda:1\")\n",
    "pruned_gemma2b.load_state_dict(t.load(\"pruned/gemma2b_wanda.pth\"))\n",
    "\n",
    "\n",
    "prompt = \"Mitigating the risk of extinction from AI should be a global\"\n",
    "answer = \" priority\"\n",
    "transformer_lens.utils.test_prompt(prompt, answer, pruned_gemma2b)\n",
    "\n",
    "prompt = \"Mary and John went to the park to play. Mary gave the ball to\"\n",
    "answer = \" John\"\n",
    "\n",
    "transformer_lens.utils.test_prompt(prompt, answer, pruned_gemma2b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You tried to specify center_unembed=True for a model using logit softcap, but this can't be done! Softcapping is not invariant upon adding a constant Setting center_unembed=False instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e370d8f4f4c41e79c793c73ab6605ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gemma-2-2b into HookedTransformer\n",
      "Tokenized prompt: ['<bos>', 'M', 'itig', 'ating', ' the', ' risk', ' of', ' extinction', ' from', ' AI', ' should', ' be', ' a', ' global']\n",
      "Tokenized answer: [' priority']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">        Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">17.32</span><span style=\"font-weight: bold\"> Prob: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">67.70</span><span style=\"font-weight: bold\">% Token: | priority|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m0\u001b[0m\u001b[1m        Logit: \u001b[0m\u001b[1;36m17.32\u001b[0m\u001b[1m Prob: \u001b[0m\u001b[1;36m67.70\u001b[0m\u001b[1m% Token: | priority|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 17.32 Prob: 67.70% Token: | priority|\n",
      "Top 1th token. Logit: 14.93 Prob:  6.16% Token: | effort|\n",
      "Top 2th token. Logit: 14.22 Prob:  3.05% Token: | goal|\n",
      "Top 3th token. Logit: 14.07 Prob:  2.61% Token: | public|\n",
      "Top 4th token. Logit: 13.77 Prob:  1.95% Token: | concern|\n",
      "Top 5th token. Logit: 13.75 Prob:  1.90% Token: | imperative|\n",
      "Top 6th token. Logit: 13.44 Prob:  1.39% Token: | responsibility|\n",
      "Top 7th token. Logit: 13.44 Prob:  1.39% Token: | challenge|\n",
      "Top 8th token. Logit: 13.21 Prob:  1.11% Token: |,|\n",
      "Top 9th token. Logit: 12.99 Prob:  0.88% Token: | undertaking|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">' priority'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">)]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m' priority'\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized prompt: ['<bos>', 'Mary', ' and', ' John', ' went', ' to', ' the', ' park', ' to', ' play', '.', ' Mary', ' gave', ' the', ' ball', ' to']\n",
      "Tokenized answer: [' John']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">        Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26.79</span><span style=\"font-weight: bold\"> Prob: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">78.71</span><span style=\"font-weight: bold\">% Token: | John|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m0\u001b[0m\u001b[1m        Logit: \u001b[0m\u001b[1;36m26.79\u001b[0m\u001b[1m Prob: \u001b[0m\u001b[1;36m78.71\u001b[0m\u001b[1m% Token: | John|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 26.79 Prob: 78.71% Token: | John|\n",
      "Top 1th token. Logit: 24.55 Prob:  8.42% Token: | her|\n",
      "Top 2th token. Logit: 23.89 Prob:  4.33% Token: | the|\n",
      "Top 3th token. Logit: 23.19 Prob:  2.16% Token: | john|\n",
      "Top 4th token. Logit: 22.51 Prob:  1.09% Token: | a|\n",
      "Top 5th token. Logit: 21.97 Prob:  0.64% Token: | Mary|\n",
      "Top 6th token. Logit: 21.70 Prob:  0.49% Token: | him|\n",
      "Top 7th token. Logit: 21.56 Prob:  0.42% Token: | Johnny|\n",
      "Top 8th token. Logit: 21.10 Prob:  0.27% Token: | Jack|\n",
      "Top 9th token. Logit: 20.88 Prob:  0.21% Token: | me|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">' John'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">)]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m' John'\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pruned_gemma2b: sae_lens.HookedSAETransformer = sae_lens.HookedSAETransformer.from_pretrained(\"gemma-2-2b\", device=\"cuda:1\")\n",
    "pruned_gemma2b.load_state_dict(t.load(\"pruned/gemma2b_magnitude.pth\"))\n",
    "\n",
    "\n",
    "prompt = \"Mitigating the risk of extinction from AI should be a global\"\n",
    "answer = \" priority\"\n",
    "transformer_lens.utils.test_prompt(prompt, answer, pruned_gemma2b)\n",
    "\n",
    "prompt = \"Mary and John went to the park to play. Mary gave the ball to\"\n",
    "answer = \" John\"\n",
    "\n",
    "transformer_lens.utils.test_prompt(prompt, answer, pruned_gemma2b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sae_lens\n",
    "from sae_lens import SAE\n",
    "\n",
    "release = \"gemma-scope-2b-pt-res\"\n",
    "sae_id = \"layer_10/width_16k/average_l0_21\"\n",
    "sae, cfg_dict, sparsity = SAE.from_pretrained(release, sae_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'architecture': 'jumprelu',\n",
       " 'd_in': 2304,\n",
       " 'd_sae': 16384,\n",
       " 'dtype': 'float32',\n",
       " 'model_name': 'gemma-2-2b',\n",
       " 'hook_name': 'blocks.10.hook_resid_post',\n",
       " 'hook_layer': 10,\n",
       " 'hook_head_index': None,\n",
       " 'activation_fn_str': 'relu',\n",
       " 'finetuning_scaling_factor': False,\n",
       " 'sae_lens_training_version': None,\n",
       " 'prepend_bos': True,\n",
       " 'dataset_path': 'monology/pile-uncopyrighted',\n",
       " 'context_size': 1024,\n",
       " 'dataset_trust_remote_code': True,\n",
       " 'apply_b_dec_to_input': False,\n",
       " 'normalize_activations': None,\n",
       " 'device': 'cpu',\n",
       " 'neuronpedia_id': None}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SAE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

Using device: cuda:2
You tried to specify center_unembed=True for a model using logit softcap, but this can't be done! Softcapping is not invariant upon adding a constant Setting center_unembed=False instead.
You are not using LayerNorm, so the writing weights can't be centered! Skipping
Model loaded
Loading pretrained SAE: gemma-scope-2b-pt-res-canonical/layer_0/width_16k/canonical
Before pruning layer 0 | Val loss: 5.802 |
Layer 0 | Sparse Ratio: 0.75 | Val Loss: 11.769
README.md already exists in the repo, skipping upload
Layer 0 | Sparse Ratio: 0.5 | Val Loss: 8.478
README.md already exists in the repo, skipping upload
Layer 0 | Sparse Ratio: 0.25 | Val Loss: 6.405
README.md already exists in the repo, skipping upload
Loading pretrained SAE: gemma-scope-2b-pt-res-canonical/layer_1/width_16k/canonical
Before pruning layer 1 | Val loss: 5.87 |
Layer 1 | Sparse Ratio: 0.75 | Val Loss: 11.045
README.md already exists in the repo, skipping upload
Layer 1 | Sparse Ratio: 0.5 | Val Loss: 6.943
README.md already exists in the repo, skipping upload
Layer 1 | Sparse Ratio: 0.25 | Val Loss: 5.983
README.md already exists in the repo, skipping upload
Loading pretrained SAE: gemma-scope-2b-pt-res-canonical/layer_2/width_16k/canonical
Before pruning layer 2 | Val loss: 6.306 |
Layer 2 | Sparse Ratio: 0.75 | Val Loss: 10.441
README.md already exists in the repo, skipping upload
Using device: cuda:2
You tried to specify center_unembed=True for a model using logit softcap, but this can't be done! Softcapping is not invariant upon adding a constant Setting center_unembed=False instead.
You are not using LayerNorm, so the writing weights can't be centered! Skipping
Model loaded
Loading pretrained SAE: gemma-scope-2b-pt-res-canonical/layer_0/width_16k/canonical
Before pruning layer 0 | Val loss: 5.532 |
Layer 0 | Sparse Ratio: 0.75 | Val Loss: 11.853
README.md already exists in the repo, skipping upload
Layer 0 | Sparse Ratio: 0.5 | Val Loss: 8.228
README.md already exists in the repo, skipping upload
Layer 0 | Sparse Ratio: 0.25 | Val Loss: 6.027
README.md already exists in the repo, skipping upload
Loading pretrained SAE: gemma-scope-2b-pt-res-canonical/layer_1/width_16k/canonical
Before pruning layer 1 | Val loss: 5.642 |
Layer 1 | Sparse Ratio: 0.75 | Val Loss: 10.983
README.md already exists in the repo, skipping upload
Layer 1 | Sparse Ratio: 0.5 | Val Loss: 6.396
README.md already exists in the repo, skipping upload
Layer 1 | Sparse Ratio: 0.25 | Val Loss: 5.689
README.md already exists in the repo, skipping upload
Loading pretrained SAE: gemma-scope-2b-pt-res-canonical/layer_2/width_16k/canonical
Before pruning layer 2 | Val loss: 5.981 |
Layer 2 | Sparse Ratio: 0.75 | Val Loss: 10.195
README.md already exists in the repo, skipping upload
Layer 2 | Sparse Ratio: 0.5 | Val Loss: 7.028
README.md already exists in the repo, skipping upload
Layer 2 | Sparse Ratio: 0.25 | Val Loss: 5.943
README.md already exists in the repo, skipping upload
Loading pretrained SAE: gemma-scope-2b-pt-res-canonical/layer_3/width_16k/canonical
Before pruning layer 3 | Val loss: 6.245 |
Layer 3 | Sparse Ratio: 0.75 | Val Loss: 9.413
README.md already exists in the repo, skipping upload
Layer 3 | Sparse Ratio: 0.5 | Val Loss: 7.007
README.md already exists in the repo, skipping upload
Layer 3 | Sparse Ratio: 0.25 | Val Loss: 6.252
README.md already exists in the repo, skipping upload
Loading pretrained SAE: gemma-scope-2b-pt-res-canonical/layer_4/width_16k/canonical
Before pruning layer 4 | Val loss: 7.208 |
Layer 4 | Sparse Ratio: 0.75 | Val Loss: 10.313
README.md already exists in the repo, skipping upload
Layer 4 | Sparse Ratio: 0.5 | Val Loss: 7.969
README.md already exists in the repo, skipping upload
Layer 4 | Sparse Ratio: 0.25 | Val Loss: 7.378
README.md already exists in the repo, skipping upload
Loading pretrained SAE: gemma-scope-2b-pt-res-canonical/layer_5/width_16k/canonical
Before pruning layer 5 | Val loss: 6.514 |
Layer 5 | Sparse Ratio: 0.75 | Val Loss: 10.852
README.md already exists in the repo, skipping upload
Layer 5 | Sparse Ratio: 0.5 | Val Loss: 7.455
README.md already exists in the repo, skipping upload
Layer 5 | Sparse Ratio: 0.25 | Val Loss: 6.583
README.md already exists in the repo, skipping upload
Loading pretrained SAE: gemma-scope-2b-pt-res-canonical/layer_6/width_16k/canonical
Before pruning layer 6 | Val loss: 6.422 |
Layer 6 | Sparse Ratio: 0.75 | Val Loss: 13.594
README.md already exists in the repo, skipping upload
Layer 6 | Sparse Ratio: 0.5 | Val Loss: 7.59
README.md already exists in the repo, skipping upload
Layer 6 | Sparse Ratio: 0.25 | Val Loss: 6.554
README.md already exists in the repo, skipping upload
Loading pretrained SAE: gemma-scope-2b-pt-res-canonical/layer_7/width_16k/canonical
Before pruning layer 7 | Val loss: 6.053 |
Layer 7 | Sparse Ratio: 0.75 | Val Loss: 8.705
README.md already exists in the repo, skipping upload
Layer 7 | Sparse Ratio: 0.5 | Val Loss: 6.394
README.md already exists in the repo, skipping upload
Layer 7 | Sparse Ratio: 0.25 | Val Loss: 6.057
README.md already exists in the repo, skipping upload
Loading pretrained SAE: gemma-scope-2b-pt-res-canonical/layer_8/width_16k/canonical
Before pruning layer 8 | Val loss: 6.148 |
Layer 8 | Sparse Ratio: 0.75 | Val Loss: 8.866
README.md already exists in the repo, skipping upload
Layer 8 | Sparse Ratio: 0.5 | Val Loss: 6.313
README.md already exists in the repo, skipping upload
Layer 8 | Sparse Ratio: 0.25 | Val Loss: 6.155
README.md already exists in the repo, skipping upload
Loading pretrained SAE: gemma-scope-2b-pt-res-canonical/layer_9/width_16k/canonical
Before pruning layer 9 | Val loss: 6.046 |
Layer 9 | Sparse Ratio: 0.75 | Val Loss: 7.412
README.md already exists in the repo, skipping upload
Layer 9 | Sparse Ratio: 0.5 | Val Loss: 6.142
README.md already exists in the repo, skipping upload
Layer 9 | Sparse Ratio: 0.25 | Val Loss: 6.063
README.md already exists in the repo, skipping upload
Loading pretrained SAE: gemma-scope-2b-pt-res-canonical/layer_10/width_16k/canonical
Before pruning layer 10 | Val loss: 6.161 |
Layer 10 | Sparse Ratio: 0.75 | Val Loss: 7.343
README.md already exists in the repo, skipping upload
Layer 10 | Sparse Ratio: 0.5 | Val Loss: 6.327
README.md already exists in the repo, skipping upload
Layer 10 | Sparse Ratio: 0.25 | Val Loss: 6.205
README.md already exists in the repo, skipping upload
Loading pretrained SAE: gemma-scope-2b-pt-res-canonical/layer_11/width_16k/canonical
Before pruning layer 11 | Val loss: 6.192 |
Layer 11 | Sparse Ratio: 0.75 | Val Loss: 7.301
README.md already exists in the repo, skipping upload
Layer 11 | Sparse Ratio: 0.5 | Val Loss: 6.445
README.md already exists in the repo, skipping upload
Layer 11 | Sparse Ratio: 0.25 | Val Loss: 6.256
README.md already exists in the repo, skipping upload
Loading pretrained SAE: gemma-scope-2b-pt-res-canonical/layer_12/width_16k/canonical
Before pruning layer 12 | Val loss: 6.043 |
Layer 12 | Sparse Ratio: 0.75 | Val Loss: 7.544
README.md already exists in the repo, skipping upload
Layer 12 | Sparse Ratio: 0.5 | Val Loss: 6.327
README.md already exists in the repo, skipping upload
Layer 12 | Sparse Ratio: 0.25 | Val Loss: 6.029
README.md already exists in the repo, skipping upload
Loading pretrained SAE: gemma-scope-2b-pt-res-canonical/layer_13/width_16k/canonical
Before pruning layer 13 | Val loss: 5.803 |
Layer 13 | Sparse Ratio: 0.75 | Val Loss: 7.339
README.md already exists in the repo, skipping upload
Layer 13 | Sparse Ratio: 0.5 | Val Loss: 6.14
README.md already exists in the repo, skipping upload
Layer 13 | Sparse Ratio: 0.25 | Val Loss: 5.834
README.md already exists in the repo, skipping upload
Loading pretrained SAE: gemma-scope-2b-pt-res-canonical/layer_14/width_16k/canonical
Before pruning layer 14 | Val loss: 5.705 |
Layer 14 | Sparse Ratio: 0.75 | Val Loss: 7.155
README.md already exists in the repo, skipping upload
Layer 14 | Sparse Ratio: 0.5 | Val Loss: 5.973
README.md already exists in the repo, skipping upload
Layer 14 | Sparse Ratio: 0.25 | Val Loss: 5.713
README.md already exists in the repo, skipping upload
Loading pretrained SAE: gemma-scope-2b-pt-res-canonical/layer_15/width_16k/canonical
Before pruning layer 15 | Val loss: 5.157 |
Layer 15 | Sparse Ratio: 0.75 | Val Loss: 6.534
README.md already exists in the repo, skipping upload
Layer 15 | Sparse Ratio: 0.5 | Val Loss: 5.444
README.md already exists in the repo, skipping upload
Layer 15 | Sparse Ratio: 0.25 | Val Loss: 5.175
README.md already exists in the repo, skipping upload
Loading pretrained SAE: gemma-scope-2b-pt-res-canonical/layer_16/width_16k/canonical
Before pruning layer 16 | Val loss: 5.249 |
Layer 16 | Sparse Ratio: 0.75 | Val Loss: 6.586
README.md already exists in the repo, skipping upload
Layer 16 | Sparse Ratio: 0.5 | Val Loss: 5.475
README.md already exists in the repo, skipping upload
Layer 16 | Sparse Ratio: 0.25 | Val Loss: 5.258
README.md already exists in the repo, skipping upload
Loading pretrained SAE: gemma-scope-2b-pt-res-canonical/layer_17/width_16k/canonical
Before pruning layer 17 | Val loss: 5.156 |
Layer 17 | Sparse Ratio: 0.75 | Val Loss: 6.66
README.md already exists in the repo, skipping upload
Layer 17 | Sparse Ratio: 0.5 | Val Loss: 5.437
README.md already exists in the repo, skipping upload
Layer 17 | Sparse Ratio: 0.25 | Val Loss: 5.161
README.md already exists in the repo, skipping upload
Loading pretrained SAE: gemma-scope-2b-pt-res-canonical/layer_18/width_16k/canonical
Before pruning layer 18 | Val loss: 5.167 |
Layer 18 | Sparse Ratio: 0.75 | Val Loss: 6.744
README.md already exists in the repo, skipping upload
Layer 18 | Sparse Ratio: 0.5 | Val Loss: 5.426
README.md already exists in the repo, skipping upload
Layer 18 | Sparse Ratio: 0.25 | Val Loss: 5.188
README.md already exists in the repo, skipping upload
Loading pretrained SAE: gemma-scope-2b-pt-res-canonical/layer_19/width_16k/canonical
Before pruning layer 19 | Val loss: 5.221 |
Layer 19 | Sparse Ratio: 0.75 | Val Loss: 6.863
README.md already exists in the repo, skipping upload
Layer 19 | Sparse Ratio: 0.5 | Val Loss: 5.53
README.md already exists in the repo, skipping upload
Layer 19 | Sparse Ratio: 0.25 | Val Loss: 5.244
README.md already exists in the repo, skipping upload
Loading pretrained SAE: gemma-scope-2b-pt-res-canonical/layer_20/width_16k/canonical
Before pruning layer 20 | Val loss: 5.254 |
Layer 20 | Sparse Ratio: 0.75 | Val Loss: 7.093
README.md already exists in the repo, skipping upload
Layer 20 | Sparse Ratio: 0.5 | Val Loss: 5.589
README.md already exists in the repo, skipping upload
Layer 20 | Sparse Ratio: 0.25 | Val Loss: 5.28
README.md already exists in the repo, skipping upload
Loading pretrained SAE: gemma-scope-2b-pt-res-canonical/layer_21/width_16k/canonical
Before pruning layer 21 | Val loss: 5.232 |
Layer 21 | Sparse Ratio: 0.75 | Val Loss: 6.921
README.md already exists in the repo, skipping upload
Layer 21 | Sparse Ratio: 0.5 | Val Loss: 5.501
README.md already exists in the repo, skipping upload
Layer 21 | Sparse Ratio: 0.25 | Val Loss: 5.251
README.md already exists in the repo, skipping upload
Loading pretrained SAE: gemma-scope-2b-pt-res-canonical/layer_22/width_16k/canonical
Before pruning layer 22 | Val loss: 5.518 |
Layer 22 | Sparse Ratio: 0.75 | Val Loss: 6.986
README.md already exists in the repo, skipping upload
Layer 22 | Sparse Ratio: 0.5 | Val Loss: 5.802
README.md already exists in the repo, skipping upload
Layer 22 | Sparse Ratio: 0.25 | Val Loss: 5.566
README.md already exists in the repo, skipping upload
Loading pretrained SAE: gemma-scope-2b-pt-res-canonical/layer_23/width_16k/canonical
Before pruning layer 23 | Val loss: 5.509 |
Layer 23 | Sparse Ratio: 0.75 | Val Loss: 6.717
README.md already exists in the repo, skipping upload
Layer 23 | Sparse Ratio: 0.5 | Val Loss: 5.738
README.md already exists in the repo, skipping upload
Layer 23 | Sparse Ratio: 0.25 | Val Loss: 5.542
README.md already exists in the repo, skipping upload
Loading pretrained SAE: gemma-scope-2b-pt-res-canonical/layer_24/width_16k/canonical
Before pruning layer 24 | Val loss: 5.72 |
Layer 24 | Sparse Ratio: 0.75 | Val Loss: 6.416
README.md already exists in the repo, skipping upload
Layer 24 | Sparse Ratio: 0.5 | Val Loss: 5.815
README.md already exists in the repo, skipping upload
Layer 24 | Sparse Ratio: 0.25 | Val Loss: 5.731
README.md already exists in the repo, skipping upload
Loading pretrained SAE: gemma-scope-2b-pt-res-canonical/layer_25/width_16k/canonical
Before pruning layer 25 | Val loss: 5.797 |
Layer 25 | Sparse Ratio: 0.75 | Val Loss: 7.073
README.md already exists in the repo, skipping upload
Layer 25 | Sparse Ratio: 0.5 | Val Loss: 6.109
README.md already exists in the repo, skipping upload
Layer 25 | Sparse Ratio: 0.25 | Val Loss: 5.839
README.md already exists in the repo, skipping upload
Using device: cuda:2
You tried to specify center_unembed=True for a model using logit softcap, but this can't be done! Softcapping is not invariant upon adding a constant Setting center_unembed=False instead.
You are not using LayerNorm, so the writing weights can't be centered! Skipping
Model loaded
Loading pretrained SAE: gemma-scope-2b-pt-mlp-canonical/layer_0/width_16k/canonical
Before pruning layer 0 | Val loss: 5.228 |
Layer 0 | Sparse Ratio: 0.75 | Val Loss: 5.38
README.md already exists in the repo, skipping upload
Layer 0 | Sparse Ratio: 0.5 | Val Loss: 5.26
README.md already exists in the repo, skipping upload
Layer 0 | Sparse Ratio: 0.25 | Val Loss: 5.23
README.md already exists in the repo, skipping upload
Loading pretrained SAE: gemma-scope-2b-pt-mlp-canonical/layer_1/width_16k/canonical
Before pruning layer 1 | Val loss: 5.236 |
Layer 1 | Sparse Ratio: 0.75 | Val Loss: 5.245
README.md already exists in the repo, skipping upload
Layer 1 | Sparse Ratio: 0.5 | Val Loss: 5.243
README.md already exists in the repo, skipping upload
Layer 1 | Sparse Ratio: 0.25 | Val Loss: 5.245
README.md already exists in the repo, skipping upload
Loading pretrained SAE: gemma-scope-2b-pt-mlp-canonical/layer_2/width_16k/canonical
Before pruning layer 2 | Val loss: 5.234 |
Layer 2 | Sparse Ratio: 0.75 | Val Loss: 5.35
README.md already exists in the repo, skipping upload
Layer 2 | Sparse Ratio: 0.5 | Val Loss: 5.269
README.md already exists in the repo, skipping upload
Layer 2 | Sparse Ratio: 0.25 | Val Loss: 5.253
README.md already exists in the repo, skipping upload
Loading pretrained SAE: gemma-scope-2b-pt-mlp-canonical/layer_3/width_16k/canonical
Before pruning layer 3 | Val loss: 5.244 |
Layer 3 | Sparse Ratio: 0.75 | Val Loss: 5.401
README.md already exists in the repo, skipping upload
Layer 3 | Sparse Ratio: 0.5 | Val Loss: 5.343
README.md already exists in the repo, skipping upload
Layer 3 | Sparse Ratio: 0.25 | Val Loss: 5.26
README.md already exists in the repo, skipping upload
Loading pretrained SAE: gemma-scope-2b-pt-mlp-canonical/layer_4/width_16k/canonical
Before pruning layer 4 | Val loss: 5.242 |
Layer 4 | Sparse Ratio: 0.75 | Val Loss: 5.414
README.md already exists in the repo, skipping upload
Layer 4 | Sparse Ratio: 0.5 | Val Loss: 5.312
README.md already exists in the repo, skipping upload
Layer 4 | Sparse Ratio: 0.25 | Val Loss: 5.261
README.md already exists in the repo, skipping upload
Loading pretrained SAE: gemma-scope-2b-pt-mlp-canonical/layer_5/width_16k/canonical
Before pruning layer 5 | Val loss: 5.238 |
Layer 5 | Sparse Ratio: 0.75 | Val Loss: 5.275
README.md already exists in the repo, skipping upload
Layer 5 | Sparse Ratio: 0.5 | Val Loss: 5.252
README.md already exists in the repo, skipping upload
Layer 5 | Sparse Ratio: 0.25 | Val Loss: 5.238
README.md already exists in the repo, skipping upload
Loading pretrained SAE: gemma-scope-2b-pt-mlp-canonical/layer_6/width_16k/canonical
Before pruning layer 6 | Val loss: 5.22 |
Layer 6 | Sparse Ratio: 0.75 | Val Loss: 5.302
README.md already exists in the repo, skipping upload
Layer 6 | Sparse Ratio: 0.5 | Val Loss: 5.271
README.md already exists in the repo, skipping upload
Layer 6 | Sparse Ratio: 0.25 | Val Loss: 5.235
README.md already exists in the repo, skipping upload
Loading pretrained SAE: gemma-scope-2b-pt-mlp-canonical/layer_7/width_16k/canonical
Before pruning layer 7 | Val loss: 5.241 |
Layer 7 | Sparse Ratio: 0.75 | Val Loss: 5.276
README.md already exists in the repo, skipping upload
Layer 7 | Sparse Ratio: 0.5 | Val Loss: 5.264
README.md already exists in the repo, skipping upload
Layer 7 | Sparse Ratio: 0.25 | Val Loss: 5.247
README.md already exists in the repo, skipping upload
Loading pretrained SAE: gemma-scope-2b-pt-mlp-canonical/layer_8/width_16k/canonical
Before pruning layer 8 | Val loss: 5.227 |
Layer 8 | Sparse Ratio: 0.75 | Val Loss: 5.312
README.md already exists in the repo, skipping upload
Layer 8 | Sparse Ratio: 0.5 | Val Loss: 5.265
README.md already exists in the repo, skipping upload
Layer 8 | Sparse Ratio: 0.25 | Val Loss: 5.237
README.md already exists in the repo, skipping upload
Loading pretrained SAE: gemma-scope-2b-pt-mlp-canonical/layer_9/width_16k/canonical
Before pruning layer 9 | Val loss: 5.224 |
Layer 9 | Sparse Ratio: 0.75 | Val Loss: 5.286
README.md already exists in the repo, skipping upload
Layer 9 | Sparse Ratio: 0.5 | Val Loss: 5.257
README.md already exists in the repo, skipping upload
Layer 9 | Sparse Ratio: 0.25 | Val Loss: 5.241
README.md already exists in the repo, skipping upload
Loading pretrained SAE: gemma-scope-2b-pt-mlp-canonical/layer_10/width_16k/canonical
Before pruning layer 10 | Val loss: 5.22 |
Layer 10 | Sparse Ratio: 0.75 | Val Loss: 5.255
README.md already exists in the repo, skipping upload
Layer 10 | Sparse Ratio: 0.5 | Val Loss: 5.235
README.md already exists in the repo, skipping upload
Layer 10 | Sparse Ratio: 0.25 | Val Loss: 5.234
README.md already exists in the repo, skipping upload
Loading pretrained SAE: gemma-scope-2b-pt-mlp-canonical/layer_11/width_16k/canonical
Before pruning layer 11 | Val loss: 5.254 |
Layer 11 | Sparse Ratio: 0.75 | Val Loss: 5.31
README.md already exists in the repo, skipping upload
Layer 11 | Sparse Ratio: 0.5 | Val Loss: 5.265
README.md already exists in the repo, skipping upload
Layer 11 | Sparse Ratio: 0.25 | Val Loss: 5.266
README.md already exists in the repo, skipping upload
Loading pretrained SAE: gemma-scope-2b-pt-mlp-canonical/layer_12/width_16k/canonical
Before pruning layer 12 | Val loss: 5.272 |
Layer 12 | Sparse Ratio: 0.75 | Val Loss: 5.297
README.md already exists in the repo, skipping upload
Layer 12 | Sparse Ratio: 0.5 | Val Loss: 5.288
README.md already exists in the repo, skipping upload
Layer 12 | Sparse Ratio: 0.25 | Val Loss: 5.282
README.md already exists in the repo, skipping upload
Loading pretrained SAE: gemma-scope-2b-pt-mlp-canonical/layer_13/width_16k/canonical
Before pruning layer 13 | Val loss: 5.337 |
Layer 13 | Sparse Ratio: 0.75 | Val Loss: 5.437
README.md already exists in the repo, skipping upload
Layer 13 | Sparse Ratio: 0.5 | Val Loss: 5.391
README.md already exists in the repo, skipping upload
Layer 13 | Sparse Ratio: 0.25 | Val Loss: 5.349
README.md already exists in the repo, skipping upload
Loading pretrained SAE: gemma-scope-2b-pt-mlp-canonical/layer_14/width_16k/canonical
Before pruning layer 14 | Val loss: 5.344 |
Layer 14 | Sparse Ratio: 0.75 | Val Loss: 5.478
README.md already exists in the repo, skipping upload
Layer 14 | Sparse Ratio: 0.5 | Val Loss: 5.404
README.md already exists in the repo, skipping upload
Layer 14 | Sparse Ratio: 0.25 | Val Loss: 5.37
README.md already exists in the repo, skipping upload
Loading pretrained SAE: gemma-scope-2b-pt-mlp-canonical/layer_15/width_16k/canonical
Before pruning layer 15 | Val loss: 5.405 |
Layer 15 | Sparse Ratio: 0.75 | Val Loss: 5.564
README.md already exists in the repo, skipping upload
Layer 15 | Sparse Ratio: 0.5 | Val Loss: 5.479
README.md already exists in the repo, skipping upload
Layer 15 | Sparse Ratio: 0.25 | Val Loss: 5.438
README.md already exists in the repo, skipping upload
Loading pretrained SAE: gemma-scope-2b-pt-mlp-canonical/layer_16/width_16k/canonical
Before pruning layer 16 | Val loss: 5.371 |
Layer 16 | Sparse Ratio: 0.75 | Val Loss: 5.503
README.md already exists in the repo, skipping upload
Layer 16 | Sparse Ratio: 0.5 | Val Loss: 5.452
README.md already exists in the repo, skipping upload
Layer 16 | Sparse Ratio: 0.25 | Val Loss: 5.4
README.md already exists in the repo, skipping upload
Loading pretrained SAE: gemma-scope-2b-pt-mlp-canonical/layer_17/width_16k/canonical
Before pruning layer 17 | Val loss: 5.607 |
Layer 17 | Sparse Ratio: 0.75 | Val Loss: 6.048
README.md already exists in the repo, skipping upload
Layer 17 | Sparse Ratio: 0.5 | Val Loss: 5.89
README.md already exists in the repo, skipping upload
Layer 17 | Sparse Ratio: 0.25 | Val Loss: 5.748
README.md already exists in the repo, skipping upload
Loading pretrained SAE: gemma-scope-2b-pt-mlp-canonical/layer_18/width_16k/canonical
Before pruning layer 18 | Val loss: 5.437 |
Layer 18 | Sparse Ratio: 0.75 | Val Loss: 5.77
README.md already exists in the repo, skipping upload
Layer 18 | Sparse Ratio: 0.5 | Val Loss: 5.623
README.md already exists in the repo, skipping upload
Layer 18 | Sparse Ratio: 0.25 | Val Loss: 5.514
README.md already exists in the repo, skipping upload
Loading pretrained SAE: gemma-scope-2b-pt-mlp-canonical/layer_19/width_16k/canonical
Before pruning layer 19 | Val loss: 5.33 |
Layer 19 | Sparse Ratio: 0.75 | Val Loss: 5.658
README.md already exists in the repo, skipping upload
Layer 19 | Sparse Ratio: 0.5 | Val Loss: 5.529
README.md already exists in the repo, skipping upload
Layer 19 | Sparse Ratio: 0.25 | Val Loss: 5.411
README.md already exists in the repo, skipping upload
Loading pretrained SAE: gemma-scope-2b-pt-mlp-canonical/layer_20/width_16k/canonical
Before pruning layer 20 | Val loss: 5.302 |
Layer 20 | Sparse Ratio: 0.75 | Val Loss: 5.559
README.md already exists in the repo, skipping upload
Layer 20 | Sparse Ratio: 0.5 | Val Loss: 5.436
README.md already exists in the repo, skipping upload
Layer 20 | Sparse Ratio: 0.25 | Val Loss: 5.334
README.md already exists in the repo, skipping upload
Loading pretrained SAE: gemma-scope-2b-pt-mlp-canonical/layer_21/width_16k/canonical
Before pruning layer 21 | Val loss: 5.235 |
Layer 21 | Sparse Ratio: 0.75 | Val Loss: 5.367
README.md already exists in the repo, skipping upload
Layer 21 | Sparse Ratio: 0.5 | Val Loss: 5.312
README.md already exists in the repo, skipping upload
Layer 21 | Sparse Ratio: 0.25 | Val Loss: 5.263
README.md already exists in the repo, skipping upload
Loading pretrained SAE: gemma-scope-2b-pt-mlp-canonical/layer_22/width_16k/canonical
Before pruning layer 22 | Val loss: 5.233 |
Layer 22 | Sparse Ratio: 0.75 | Val Loss: 5.309
README.md already exists in the repo, skipping upload
Layer 22 | Sparse Ratio: 0.5 | Val Loss: 5.252
README.md already exists in the repo, skipping upload
Layer 22 | Sparse Ratio: 0.25 | Val Loss: 5.247
README.md already exists in the repo, skipping upload
Loading pretrained SAE: gemma-scope-2b-pt-mlp-canonical/layer_23/width_16k/canonical
Before pruning layer 23 | Val loss: 5.213 |
Layer 23 | Sparse Ratio: 0.75 | Val Loss: 5.285
README.md already exists in the repo, skipping upload
Layer 23 | Sparse Ratio: 0.5 | Val Loss: 5.245
README.md already exists in the repo, skipping upload
Layer 23 | Sparse Ratio: 0.25 | Val Loss: 5.225
README.md already exists in the repo, skipping upload
Loading pretrained SAE: gemma-scope-2b-pt-mlp-canonical/layer_24/width_16k/canonical
Before pruning layer 24 | Val loss: 5.215 |
Layer 24 | Sparse Ratio: 0.75 | Val Loss: 5.297
README.md already exists in the repo, skipping upload
Layer 24 | Sparse Ratio: 0.5 | Val Loss: 5.239
README.md already exists in the repo, skipping upload
Layer 24 | Sparse Ratio: 0.25 | Val Loss: 5.222
README.md already exists in the repo, skipping upload
Loading pretrained SAE: gemma-scope-2b-pt-mlp-canonical/layer_25/width_16k/canonical
Before pruning layer 25 | Val loss: 5.279 |
Layer 25 | Sparse Ratio: 0.75 | Val Loss: 5.491
README.md already exists in the repo, skipping upload
Layer 25 | Sparse Ratio: 0.5 | Val Loss: 5.347
README.md already exists in the repo, skipping upload
Layer 25 | Sparse Ratio: 0.25 | Val Loss: 5.284
README.md already exists in the repo, skipping upload
Using device: cuda:2
You tried to specify center_unembed=True for a model using logit softcap, but this can't be done! Softcapping is not invariant upon adding a constant Setting center_unembed=False instead.
You are not using LayerNorm, so the writing weights can't be centered! Skipping
Model loaded
Using device: cuda:2
You tried to specify center_unembed=True for a model using logit softcap, but this can't be done! Softcapping is not invariant upon adding a constant Setting center_unembed=False instead.
You are not using LayerNorm, so the writing weights can't be centered! Skipping
Model loaded
Loading pretrained SAE: gemma-scope-2b-pt-res-canonical/layer_0/width_16k/canonical
Using device: cuda:2
You tried to specify center_unembed=True for a model using logit softcap, but this can't be done! Softcapping is not invariant upon adding a constant Setting center_unembed=False instead.
You are not using LayerNorm, so the writing weights can't be centered! Skipping
Model loaded
Loading pretrained SAE: gemma-scope-2b-pt-res-canonical/layer_0/width_16k/canonical
Loading pretrained SAE: gemma-scope-2b-pt-res-canonical/layer_1/width_16k/canonical

Using device: cuda:0
You tried to specify center_unembed=True for a model using logit softcap, but this can't be done! Softcapping is not invariant upon adding a constant Setting center_unembed=False instead.
You are not using LayerNorm, so the writing weights can't be centered! Skipping
Model loaded
Loading pretrained SAE: gemma-scope-2b-pt-res-canonical/layer_0/width_16k/canonical
Before pruning layer 0 | Val loss: 2.709 |
Layer 0 | Sparse Ratio: 0.75 | Val Loss: 8.152
Layer 0 | Sparse Ratio: 0.5 | Val Loss: 4.355
README.md already exists in the repo, skipping upload
Layer 0 | Sparse Ratio: 0.25 | Val Loss: 3.029
README.md already exists in the repo, skipping upload
Loading pretrained SAE: gemma-scope-2b-pt-res-canonical/layer_1/width_16k/canonical
Before pruning layer 1 | Val loss: 2.771 |
Layer 1 | Sparse Ratio: 0.75 | Val Loss: 6.918
README.md already exists in the repo, skipping upload
Layer 1 | Sparse Ratio: 0.5 | Val Loss: 3.252
README.md already exists in the repo, skipping upload
Layer 1 | Sparse Ratio: 0.25 | Val Loss: 2.815
README.md already exists in the repo, skipping upload
Loading pretrained SAE: gemma-scope-2b-pt-res-canonical/layer_2/width_16k/canonical
Before pruning layer 2 | Val loss: 3.288 |
Layer 2 | Sparse Ratio: 0.75 | Val Loss: 6.265
README.md already exists in the repo, skipping upload
Layer 2 | Sparse Ratio: 0.5 | Val Loss: 4.097
Using device: cuda:0
You tried to specify center_unembed=True for a model using logit softcap, but this can't be done! Softcapping is not invariant upon adding a constant Setting center_unembed=False instead.
You are not using LayerNorm, so the writing weights can't be centered! Skipping
Model loaded
Loading pretrained SAE: gemma-scope-2b-pt-res-canonical/layer_0/width_16k/canonical
Before pruning layer 0 | Val loss: 2.676 |
Layer 0 | Sparse Ratio: 0.75 | Val Loss: 8.079
README.md already exists in the repo, skipping upload
Layer 0 | Sparse Ratio: 0.5 | Val Loss: 4.281
README.md already exists in the repo, skipping upload
Layer 0 | Sparse Ratio: 0.25 | Val Loss: 2.929
README.md already exists in the repo, skipping upload
Loading pretrained SAE: gemma-scope-2b-pt-res-canonical/layer_1/width_16k/canonical
Before pruning layer 1 | Val loss: 2.733 |
Layer 1 | Sparse Ratio: 0.75 | Val Loss: 6.698
README.md already exists in the repo, skipping upload
Layer 1 | Sparse Ratio: 0.5 | Val Loss: 3.177
README.md already exists in the repo, skipping upload
Layer 1 | Sparse Ratio: 0.25 | Val Loss: 2.774
README.md already exists in the repo, skipping upload
Loading pretrained SAE: gemma-scope-2b-pt-res-canonical/layer_2/width_16k/canonical
Before pruning layer 2 | Val loss: 3.195 |
Layer 2 | Sparse Ratio: 0.75 | Val Loss: 6.068
README.md already exists in the repo, skipping upload
Layer 2 | Sparse Ratio: 0.5 | Val Loss: 3.956
README.md already exists in the repo, skipping upload
Layer 2 | Sparse Ratio: 0.25 | Val Loss: 3.228
README.md already exists in the repo, skipping upload
Loading pretrained SAE: gemma-scope-2b-pt-res-canonical/layer_3/width_16k/canonical
Before pruning layer 3 | Val loss: 3.341 |
Layer 3 | Sparse Ratio: 0.75 | Val Loss: 5.37
README.md already exists in the repo, skipping upload
Layer 3 | Sparse Ratio: 0.5 | Val Loss: 3.912
README.md already exists in the repo, skipping upload
Layer 3 | Sparse Ratio: 0.25 | Val Loss: 3.392
README.md already exists in the repo, skipping upload
Loading pretrained SAE: gemma-scope-2b-pt-res-canonical/layer_4/width_16k/canonical
Before pruning layer 4 | Val loss: 4.125 |
Layer 4 | Sparse Ratio: 0.75 | Val Loss: 6.959
README.md already exists in the repo, skipping upload
Layer 4 | Sparse Ratio: 0.5 | Val Loss: 5.129
README.md already exists in the repo, skipping upload
Layer 4 | Sparse Ratio: 0.25 | Val Loss: 4.201
README.md already exists in the repo, skipping upload
Loading pretrained SAE: gemma-scope-2b-pt-res-canonical/layer_5/width_16k/canonical
Before pruning layer 5 | Val loss: 3.636 |
Layer 5 | Sparse Ratio: 0.75 | Val Loss: 6.084
README.md already exists in the repo, skipping upload
Layer 5 | Sparse Ratio: 0.5 | Val Loss: 4.539
README.md already exists in the repo, skipping upload
Layer 5 | Sparse Ratio: 0.25 | Val Loss: 3.701
README.md already exists in the repo, skipping upload
Loading pretrained SAE: gemma-scope-2b-pt-res-canonical/layer_6/width_16k/canonical
Before pruning layer 6 | Val loss: 3.448 |
Layer 6 | Sparse Ratio: 0.75 | Val Loss: 8.725
README.md already exists in the repo, skipping upload
Layer 6 | Sparse Ratio: 0.5 | Val Loss: 4.869
README.md already exists in the repo, skipping upload
Layer 6 | Sparse Ratio: 0.25 | Val Loss: 3.594
README.md already exists in the repo, skipping upload
Loading pretrained SAE: gemma-scope-2b-pt-res-canonical/layer_7/width_16k/canonical
Before pruning layer 7 | Val loss: 3.097 |
Layer 7 | Sparse Ratio: 0.75 | Val Loss: 5.22
README.md already exists in the repo, skipping upload
Layer 7 | Sparse Ratio: 0.5 | Val Loss: 3.792
README.md already exists in the repo, skipping upload
Layer 7 | Sparse Ratio: 0.25 | Val Loss: 3.117
README.md already exists in the repo, skipping upload
Loading pretrained SAE: gemma-scope-2b-pt-res-canonical/layer_8/width_16k/canonical
Before pruning layer 8 | Val loss: 3.049 |
Layer 8 | Sparse Ratio: 0.75 | Val Loss: 5.099
README.md already exists in the repo, skipping upload
Layer 8 | Sparse Ratio: 0.5 | Val Loss: 3.457
README.md already exists in the repo, skipping upload
Layer 8 | Sparse Ratio: 0.25 | Val Loss: 3.061
README.md already exists in the repo, skipping upload
Loading pretrained SAE: gemma-scope-2b-pt-res-canonical/layer_9/width_16k/canonical
Before pruning layer 9 | Val loss: 3.008 |
Layer 9 | Sparse Ratio: 0.75 | Val Loss: 4.963
README.md already exists in the repo, skipping upload
Layer 9 | Sparse Ratio: 0.5 | Val Loss: 3.369
README.md already exists in the repo, skipping upload
Layer 9 | Sparse Ratio: 0.25 | Val Loss: 3.011

Using device: cuda:2
You tried to specify center_unembed=True for a model using logit softcap, but this can't be done! Softcapping is not invariant upon adding a constant Setting center_unembed=False instead.
You are not using LayerNorm, so the writing weights can't be centered! Skipping
Model loaded
Loading pretrained SAE: gemma-scope-2b-pt-att-canonical/layer_0/width_16k/canonical
README.md already exists in the repo, skipping upload
README.md already exists in the repo, skipping upload
README.md already exists in the repo, skipping upload
Loading pretrained SAE: gemma-scope-2b-pt-att-canonical/layer_1/width_16k/canonical
README.md already exists in the repo, skipping upload
README.md already exists in the repo, skipping upload
README.md already exists in the repo, skipping upload
Loading pretrained SAE: gemma-scope-2b-pt-att-canonical/layer_2/width_16k/canonical
README.md already exists in the repo, skipping upload
README.md already exists in the repo, skipping upload
README.md already exists in the repo, skipping upload
Loading pretrained SAE: gemma-scope-2b-pt-att-canonical/layer_3/width_16k/canonical
README.md already exists in the repo, skipping upload
README.md already exists in the repo, skipping upload
README.md already exists in the repo, skipping upload
Loading pretrained SAE: gemma-scope-2b-pt-att-canonical/layer_4/width_16k/canonical
README.md already exists in the repo, skipping upload
README.md already exists in the repo, skipping upload
README.md already exists in the repo, skipping upload
Loading pretrained SAE: gemma-scope-2b-pt-att-canonical/layer_5/width_16k/canonical
README.md already exists in the repo, skipping upload
README.md already exists in the repo, skipping upload
README.md already exists in the repo, skipping upload
Loading pretrained SAE: gemma-scope-2b-pt-att-canonical/layer_6/width_16k/canonical
README.md already exists in the repo, skipping upload
README.md already exists in the repo, skipping upload
README.md already exists in the repo, skipping upload
Loading pretrained SAE: gemma-scope-2b-pt-att-canonical/layer_7/width_16k/canonical
README.md already exists in the repo, skipping upload
README.md already exists in the repo, skipping upload
README.md already exists in the repo, skipping upload
Loading pretrained SAE: gemma-scope-2b-pt-att-canonical/layer_8/width_16k/canonical
README.md already exists in the repo, skipping upload
README.md already exists in the repo, skipping upload
README.md already exists in the repo, skipping upload
Loading pretrained SAE: gemma-scope-2b-pt-att-canonical/layer_9/width_16k/canonical
README.md already exists in the repo, skipping upload
README.md already exists in the repo, skipping upload
README.md already exists in the repo, skipping upload
Loading pretrained SAE: gemma-scope-2b-pt-att-canonical/layer_10/width_16k/canonical
README.md already exists in the repo, skipping upload
README.md already exists in the repo, skipping upload
README.md already exists in the repo, skipping upload
Loading pretrained SAE: gemma-scope-2b-pt-att-canonical/layer_11/width_16k/canonical
README.md already exists in the repo, skipping upload
README.md already exists in the repo, skipping upload
README.md already exists in the repo, skipping upload
Loading pretrained SAE: gemma-scope-2b-pt-att-canonical/layer_12/width_16k/canonical
README.md already exists in the repo, skipping upload
README.md already exists in the repo, skipping upload
README.md already exists in the repo, skipping upload
Loading pretrained SAE: gemma-scope-2b-pt-att-canonical/layer_13/width_16k/canonical
README.md already exists in the repo, skipping upload
README.md already exists in the repo, skipping upload
README.md already exists in the repo, skipping upload
Loading pretrained SAE: gemma-scope-2b-pt-att-canonical/layer_14/width_16k/canonical
README.md already exists in the repo, skipping upload
README.md already exists in the repo, skipping upload
README.md already exists in the repo, skipping upload
Loading pretrained SAE: gemma-scope-2b-pt-att-canonical/layer_15/width_16k/canonical
README.md already exists in the repo, skipping upload
README.md already exists in the repo, skipping upload
README.md already exists in the repo, skipping upload
Loading pretrained SAE: gemma-scope-2b-pt-att-canonical/layer_16/width_16k/canonical
README.md already exists in the repo, skipping upload
README.md already exists in the repo, skipping upload
Using device: cuda:0
You tried to specify center_unembed=True for a model using logit softcap, but this can't be done! Softcapping is not invariant upon adding a constant Setting center_unembed=False instead.
You are not using LayerNorm, so the writing weights can't be centered! Skipping
Model loaded
Loading pretrained SAE: gemma-scope-2b-pt-att-canonical/layer_25/width_16k/canonical
Before pruning layer 25 | Val loss: 2.001 |
Layer 25 | Sparse Ratio: 0.75 | Val Loss: 2.022
README.md already exists in the repo, skipping upload
Layer 25 | Sparse Ratio: 0.5 | Val Loss: 2.003
README.md already exists in the repo, skipping upload
Layer 25 | Sparse Ratio: 0.25 | Val Loss: 2.003
README.md already exists in the repo, skipping upload
Loading pretrained SAE: gemma-scope-2b-pt-att-canonical/layer_24/width_16k/canonical
Before pruning layer 24 | Val loss: 1.994 |
Layer 24 | Sparse Ratio: 0.75 | Val Loss: 1.995
README.md already exists in the repo, skipping upload
Layer 24 | Sparse Ratio: 0.5 | Val Loss: 1.995
README.md already exists in the repo, skipping upload
Layer 24 | Sparse Ratio: 0.25 | Val Loss: 1.993
README.md already exists in the repo, skipping upload
Loading pretrained SAE: gemma-scope-2b-pt-att-canonical/layer_23/width_16k/canonical
Before pruning layer 23 | Val loss: 2.007 |
Layer 23 | Sparse Ratio: 0.75 | Val Loss: 2.018
README.md already exists in the repo, skipping upload
Layer 23 | Sparse Ratio: 0.5 | Val Loss: 2.005
README.md already exists in the repo, skipping upload
Layer 23 | Sparse Ratio: 0.25 | Val Loss: 2.006
README.md already exists in the repo, skipping upload
Loading pretrained SAE: gemma-scope-2b-pt-att-canonical/layer_22/width_16k/canonical
Before pruning layer 22 | Val loss: 2.007 |
Layer 22 | Sparse Ratio: 0.75 | Val Loss: 2.063
README.md already exists in the repo, skipping upload
Layer 22 | Sparse Ratio: 0.5 | Val Loss: 2.01
README.md already exists in the repo, skipping upload
Layer 22 | Sparse Ratio: 0.25 | Val Loss: 2.005
README.md already exists in the repo, skipping upload
Loading pretrained SAE: gemma-scope-2b-pt-att-canonical/layer_21/width_16k/canonical
Before pruning layer 21 | Val loss: 2.015 |
Layer 21 | Sparse Ratio: 0.75 | Val Loss: 2.02
README.md already exists in the repo, skipping upload
Layer 21 | Sparse Ratio: 0.5 | Val Loss: 2.015
README.md already exists in the repo, skipping upload
Layer 21 | Sparse Ratio: 0.25 | Val Loss: 2.015
README.md already exists in the repo, skipping upload
Loading pretrained SAE: gemma-scope-2b-pt-att-canonical/layer_20/width_16k/canonical
Before pruning layer 20 | Val loss: 2.007 |
Layer 20 | Sparse Ratio: 0.75 | Val Loss: 2.006
README.md already exists in the repo, skipping upload
Layer 20 | Sparse Ratio: 0.5 | Val Loss: 2.006
README.md already exists in the repo, skipping upload
Layer 20 | Sparse Ratio: 0.25 | Val Loss: 2.006
README.md already exists in the repo, skipping upload
Loading pretrained SAE: gemma-scope-2b-pt-att-canonical/layer_19/width_16k/canonical
Before pruning layer 19 | Val loss: 2.005 |
Layer 19 | Sparse Ratio: 0.75 | Val Loss: 2.009
README.md already exists in the repo, skipping upload
Layer 19 | Sparse Ratio: 0.5 | Val Loss: 2.006
README.md already exists in the repo, skipping upload
Layer 19 | Sparse Ratio: 0.25 | Val Loss: 2.005
README.md already exists in the repo, skipping upload
Loading pretrained SAE: gemma-scope-2b-pt-att-canonical/layer_18/width_16k/canonical
Before pruning layer 18 | Val loss: 2.009 |
Layer 18 | Sparse Ratio: 0.75 | Val Loss: 2.01
README.md already exists in the repo, skipping upload
Layer 18 | Sparse Ratio: 0.5 | Val Loss: 2.008
README.md already exists in the repo, skipping upload
Layer 18 | Sparse Ratio: 0.25 | Val Loss: 2.01
README.md already exists in the repo, skipping upload
Loading pretrained SAE: gemma-scope-2b-pt-att-canonical/layer_17/width_16k/canonical
Before pruning layer 17 | Val loss: 2.007 |
Layer 17 | Sparse Ratio: 0.75 | Val Loss: 2.013
README.md already exists in the repo, skipping upload
Layer 17 | Sparse Ratio: 0.5 | Val Loss: 2.008
README.md already exists in the repo, skipping upload
Layer 17 | Sparse Ratio: 0.25 | Val Loss: 2.008
README.md already exists in the repo, skipping upload
Loading pretrained SAE: gemma-scope-2b-pt-att-canonical/layer_16/width_16k/canonical
Before pruning layer 16 | Val loss: 2.016 |
Layer 16 | Sparse Ratio: 0.25 | Val Loss: 2.016
README.md already exists in the repo, skipping upload
Loading pretrained SAE: gemma-scope-2b-pt-att-canonical/layer_15/width_16k/canonical
Before pruning layer 15 | Val loss: 2.017 |
Loading pretrained SAE: gemma-scope-2b-pt-att-canonical/layer_14/width_16k/canonical
Before pruning layer 14 | Val loss: 2.022 |
Loading pretrained SAE: gemma-scope-2b-pt-att-canonical/layer_13/width_16k/canonical
Before pruning layer 13 | Val loss: 2.008 |
Loading pretrained SAE: gemma-scope-2b-pt-att-canonical/layer_12/width_16k/canonical
Before pruning layer 12 | Val loss: 2.014 |
Loading pretrained SAE: gemma-scope-2b-pt-att-canonical/layer_11/width_16k/canonical
Before pruning layer 11 | Val loss: 2.0 |
Loading pretrained SAE: gemma-scope-2b-pt-att-canonical/layer_10/width_16k/canonical
Before pruning layer 10 | Val loss: 2.012 |
Loading pretrained SAE: gemma-scope-2b-pt-att-canonical/layer_9/width_16k/canonical
Before pruning layer 9 | Val loss: 1.999 |
Loading pretrained SAE: gemma-scope-2b-pt-att-canonical/layer_8/width_16k/canonical
Before pruning layer 8 | Val loss: 2.006 |
Loading pretrained SAE: gemma-scope-2b-pt-att-canonical/layer_7/width_16k/canonical
Before pruning layer 7 | Val loss: 1.999 |
Loading pretrained SAE: gemma-scope-2b-pt-att-canonical/layer_6/width_16k/canonical
Before pruning layer 6 | Val loss: 2.014 |
Loading pretrained SAE: gemma-scope-2b-pt-att-canonical/layer_5/width_16k/canonical
Before pruning layer 5 | Val loss: 1.996 |
Loading pretrained SAE: gemma-scope-2b-pt-att-canonical/layer_4/width_16k/canonical
Before pruning layer 4 | Val loss: 2.015 |
Loading pretrained SAE: gemma-scope-2b-pt-att-canonical/layer_3/width_16k/canonical
Before pruning layer 3 | Val loss: 2.009 |
Loading pretrained SAE: gemma-scope-2b-pt-att-canonical/layer_2/width_16k/canonical
Before pruning layer 2 | Val loss: 2.005 |
Loading pretrained SAE: gemma-scope-2b-pt-att-canonical/layer_1/width_16k/canonical
Before pruning layer 1 | Val loss: 2.002 |
Loading pretrained SAE: gemma-scope-2b-pt-att-canonical/layer_0/width_16k/canonical
Before pruning layer 0 | Val loss: 2.008 |

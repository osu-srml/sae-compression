{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformer_lens, sae_lens\n",
    "from transformer_lens import HookedTransformer\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparsegpt(W, X, λ=0.01, p=0.5, B=128, Bs=32, device='cuda'):\n",
    "    '''\n",
    "    W: weight matrix\n",
    "    X: input matrix\n",
    "    λ: regularization parameter\n",
    "    p: sparsity parameter\n",
    "    B: batch size\n",
    "    Bs: block size\n",
    "    '''\n",
    "\n",
    "    # print(W.shape, X.shape)\n",
    "    # Initialize mask and block quantization errors\n",
    "    M = torch.ones_like(W) # Binary pruning mask\n",
    "    E = torch.zeros_like(W) # Block quantization errors\n",
    "\n",
    "    # Compute Hessian inverse\n",
    "    H = X.T @ X + λ * torch.eye(X.shape[1], device=device)\n",
    "    H_inv = torch.cholesky(torch.inverse(H)).T\n",
    "\n",
    "    W = W.detach().clone()  # Prevent gradient tracking for pruning\n",
    "\n",
    "    # Blockwise pruning\n",
    "    for i in range(0, W.shape[1], B):\n",
    "        for j in range(i, min(i + B, W.shape[1])):\n",
    "            if j % Bs == 0:\n",
    "                # Select (1-p) fraction of weights based on Hessian information\n",
    "                W_squared = W[:, j:j + Bs] ** 2\n",
    "                H_diag = torch.diag(H_inv)[j : j+Bs].unsqueeze(0)\n",
    "                score = W_squared / H_diag ** 2\n",
    "                thr = torch.quantile(score, p)  # Pruning threshold\n",
    "                M[:, j:j + Bs] = (score > thr).float()  # Binary mask\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # Prune and update quantization error\n",
    "                E[:, j] += (1 - M[:, j]) * W[:, j]\n",
    "                W[:, j] = M[:, j] * W[:, j]  # Apply mask\n",
    "\n",
    "            # Update weights using Hessian inverse\n",
    "            W[:, i:(i + B)] = W[:, i:(i + B)] - (E[:, j].unsqueeze(1) * H_inv[j, i:(i + B)]).clone()\n",
    "\n",
    "    return W * M\n",
    "\n",
    "\n",
    "\n",
    "def prune_sgpt(model, tokens):\n",
    "    wts_act = {\n",
    "        'attn.W_Q': 'attn.hook_q',\n",
    "        'attn.W_K': 'attn.hook_k',\n",
    "        'attn.W_V': 'attn.hook_v',\n",
    "        'attn.W_O': 'hook_attn_out',\n",
    "        'mlp.W_in': 'mlp.hook_pre',\n",
    "        'mlp.W_out': 'hook_mlp_out'\n",
    "    }\n",
    "    \n",
    "    for layer in range(model.cfg.n_layers):\n",
    "        logits, cache = model.run_with_cache(tokens, remove_batch_dim=True)\n",
    "        \n",
    "        for wt, act in wts_act.items():\n",
    "            W = model.get_parameter(f'blocks.{layer}.{wt}')\n",
    "            X = cache[f'blocks.{layer}.{act}']\n",
    "\n",
    "            # Detach and clone W to prevent gradient tracking issues\n",
    "            W = W.detach().clone()\n",
    "\n",
    "            if W.dim() == 3:\n",
    "                if 'W_O' in wt:\n",
    "                    X_norm = X\n",
    "                    pruned_W = torch.stack([sparsegpt(W[head], X_norm) for head in range(W.shape[0])])\n",
    "                else:\n",
    "                    pruned_W = torch.stack([sparsegpt(W[head], X[:, head, :]) for head in range(W.shape[0])])\n",
    "            else:\n",
    "                X_norm = X\n",
    "                pruned_W = sparsegpt(W, X_norm)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                param = model.get_parameter(f'blocks.{layer}.{wt}')\n",
    "                param.copy_(pruned_W)  # Update weights without breaking graph\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "gpt2: sae_lens.HookedSAETransformer = sae_lens.HookedSAETransformer.from_pretrained(\"gpt2-small\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized prompt: ['<|endoftext|>', 'Mit', 'igating', ' the', ' risk', ' of', ' extinction', ' from', ' AI', ' should', ' be', ' a', ' global']\n",
      "Tokenized answer: [' priority']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">        Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">19.46</span><span style=\"font-weight: bold\"> Prob: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">52.99</span><span style=\"font-weight: bold\">% Token: | priority|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m0\u001b[0m\u001b[1m        Logit: \u001b[0m\u001b[1;36m19.46\u001b[0m\u001b[1m Prob: \u001b[0m\u001b[1;36m52.99\u001b[0m\u001b[1m% Token: | priority|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 19.46 Prob: 52.99% Token: | priority|\n",
      "Top 1th token. Logit: 17.44 Prob:  7.02% Token: | effort|\n",
      "Top 2th token. Logit: 16.94 Prob:  4.26% Token: | issue|\n",
      "Top 3th token. Logit: 16.63 Prob:  3.14% Token: | challenge|\n",
      "Top 4th token. Logit: 16.37 Prob:  2.42% Token: | goal|\n",
      "Top 5th token. Logit: 16.06 Prob:  1.78% Token: | concern|\n",
      "Top 6th token. Logit: 15.88 Prob:  1.47% Token: | focus|\n",
      "Top 7th token. Logit: 15.61 Prob:  1.13% Token: | approach|\n",
      "Top 8th token. Logit: 15.53 Prob:  1.04% Token: | policy|\n",
      "Top 9th token. Logit: 15.42 Prob:  0.93% Token: | initiative|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">' priority'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">)]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m' priority'\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"Mitigating the risk of extinction from AI should be a global\"\n",
    "answer = \" priority\"\n",
    "\n",
    "# First see how the model does without SAEs\n",
    "transformer_lens.utils.test_prompt(prompt, answer, gpt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = transformer_lens.utils.get_dataset('openwebtext')\n",
    "\n",
    "class OpenWebText(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset, max_length=1024):\n",
    "        self.dataset = dataset\n",
    "        self.max_length = 1024\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.dataset[idx]['text']\n",
    "        tokens = gpt2.to_tokens(text)\n",
    "        tokens = tokens[:self.max_length]\n",
    "        return tokens\n",
    "    \n",
    "openwebtext = OpenWebText(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/10000 [00:49<69:05:47, 24.88s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "pruned_gpt2 = sae_lens.HookedSAETransformer.from_pretrained(\"gpt2-small\", device=device)\n",
    "i = 0\n",
    "for batch in tqdm(openwebtext):\n",
    "    if i == 2:\n",
    "        break\n",
    "    pruned_gpt2 = prune_sgpt(pruned_gpt2, batch)\n",
    "    i += 1\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized prompt: ['<|endoftext|>', 'Mit', 'igating', ' the', ' risk', ' of', ' extinction', ' from', ' AI', ' should', ' be', ' a', ' global']\n",
      "Tokenized answer: [' priority']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8475</span><span style=\"font-weight: bold\">     Logit:   nan Prob:   nan% Token: | priority|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m8475\u001b[0m\u001b[1m     Logit:   nan Prob:   nan% Token: | priority|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit:   nan Prob:   nan% Token: |!|\n",
      "Top 1th token. Logit:   nan Prob:   nan% Token: |\"|\n",
      "Top 2th token. Logit:   nan Prob:   nan% Token: |#|\n",
      "Top 3th token. Logit:   nan Prob:   nan% Token: |$|\n",
      "Top 4th token. Logit:   nan Prob:   nan% Token: |%|\n",
      "Top 5th token. Logit:   nan Prob:   nan% Token: |&|\n",
      "Top 6th token. Logit:   nan Prob:   nan% Token: |'|\n",
      "Top 7th token. Logit:   nan Prob:   nan% Token: |(|\n",
      "Top 8th token. Logit:   nan Prob:   nan% Token: |)|\n",
      "Top 9th token. Logit:   nan Prob:   nan% Token: |*|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">' priority'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8475</span><span style=\"font-weight: bold\">)]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m' priority'\u001b[0m, \u001b[1;36m8475\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"Mitigating the risk of extinction from AI should be a global\"\n",
    "answer = \" priority\"\n",
    "\n",
    "# First see how the model does without SAEs\n",
    "transformer_lens.utils.test_prompt(prompt, answer, pruned_gpt2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SAE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

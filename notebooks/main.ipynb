{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:1\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import itertools\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "from collections import Counter, defaultdict\n",
    "from copy import deepcopy\n",
    "from dataclasses import dataclass\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "from typing import Any, Callable, Literal, TypeAlias\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch as t\n",
    "from datasets import load_dataset\n",
    "import sae_lens\n",
    "import transformer_lens\n",
    "from sae_lens import (\n",
    "    SAE,\n",
    "    ActivationsStore,\n",
    "    HookedSAETransformer,\n",
    "    LanguageModelSAERunnerConfig,\n",
    "    SAEConfig,\n",
    "    SAETrainingRunner,\n",
    "    upload_saes_to_huggingface,\n",
    ")\n",
    "from sae_lens.toolkit.pretrained_saes_directory import get_pretrained_saes_directory\n",
    "from sae_vis import SaeVisConfig, SaeVisData, SaeVisLayoutConfig\n",
    "from tabulate import tabulate\n",
    "from torch import Tensor, nn\n",
    "from torch.distributions.categorical import Categorical\n",
    "from torch.nn import functional as F\n",
    "from tqdm.auto import tqdm\n",
    "from transformer_lens import ActivationCache, HookedTransformer\n",
    "from transformer_lens.hook_points import HookPoint\n",
    "from transformer_lens.utils import get_act_name, test_prompt, to_numpy\n",
    "\n",
    "import einops\n",
    "import circuitsvis as cv\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from IPython.display import HTML, IFrame, clear_output, display\n",
    "from jaxtyping import Float, Int\n",
    "from openai import OpenAI\n",
    "from rich import print as rprint\n",
    "from rich.table import Table\n",
    "from tabulate import tabulate\n",
    "\n",
    "from tqdm import tqdm\n",
    "device = t.device(\"cuda:1\" if t.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "# Hugging face: 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\n",
    "# Wandb: 6b549d940e7a29c79c184f27f25606e94a48a966"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attention SAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_saes = {\n",
    "    layer: sae_lens.SAE.from_pretrained(\n",
    "        \"gpt2-small-hook-z-kk\",\n",
    "        f\"blocks.{layer}.hook_z\",\n",
    "        device=device,\n",
    "    )[0]\n",
    "    for layer in range(12)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for attn_sae in attn_saes.values():\n",
    "    print(attn_sae.cfg)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(attn_saes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_dashboard(\n",
    "    sae_release=\"gpt2-small-res-jb\",\n",
    "    sae_id=\"blocks.7.hook_resid_pre\",\n",
    "    latent_idx=0,\n",
    "    width=800,\n",
    "    height=600,\n",
    "):\n",
    "    release = sae_lens.toolkit.pretrained_saes_directory.get_pretrained_saes_directory()[sae_release]\n",
    "    neuronpedia_id = release.neuronpedia_id[sae_id]\n",
    "\n",
    "    url = f\"https://neuronpedia.org/{neuronpedia_id}/{latent_idx}?embed=true&embedexplanation=true&embedplots=true&embedtest=true&height=300\"\n",
    "\n",
    "    print(url)\n",
    "    display(IFrame(url, width=width, height=height))\n",
    "\n",
    "\n",
    "layer = 9\n",
    "\n",
    "display_dashboard(\n",
    "    sae_release=\"gpt2-small-hook-z-kk\",\n",
    "    sae_id=f\"blocks.{layer}.hook_z\",\n",
    "    latent_idx=2,  # or you can try `random.randint(0, attn_saes[layer].cfg.d_sae)`\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2.cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Magnitude pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_magnitude(W, sparse_ratio=0.5):\n",
    "    W_abs = W.abs()\n",
    "    k = int(W_abs.numel() * sparse_ratio)\n",
    "    _, indices = W_abs.view(-1).topk(k)\n",
    "    mask = t.zeros_like(W_abs)\n",
    "    mask.view(-1)[indices] = 1\n",
    "    return mask*W\n",
    "\n",
    "def prune_model(model):\n",
    "    wts = ['W_Q', 'W_K', 'W_V', 'W_O', 'W_in', 'W_out']\n",
    "    for name, param in gpt2.named_parameters():\n",
    "        # print(f\"Layer: {name}, Shape: {param.shape}\")\n",
    "        if name.split('.')[-1] in wts:\n",
    "            if param.dim() == 3:    \n",
    "                for i in range(param.shape[0]):\n",
    "                    param[i] = prune_magnitude(param[i])\n",
    "            else:\n",
    "                param = prune_magnitude(param)\n",
    "    \n",
    "    return model\n",
    "\n",
    "gpt2: sae_lens.HookedSAETransformer = sae_lens.HookedSAETransformer.from_pretrained(\"gpt2-small\", device=device)\n",
    "pruned_gpt2 = prune_model(gpt2)\n",
    "gpt2: sae_lens.HookedSAETransformer = sae_lens.HookedSAETransformer.from_pretrained(\"gpt2-small\", device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"I\"\n",
    "answer = \" am\"\n",
    "transformer_lens.utils.test_prompt(prompt, answer, gpt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"I\"\n",
    "answer = \" am\"\n",
    "transformer_lens.utils.test_prompt(prompt, answer, pruned_gpt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_lens.evals.sanity_check(gpt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_lens.evals.sanity_check(pruned_gpt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_prompt = \"After John and Mary went to the store, John gave a bottle of milk to\"\n",
    "example_answer = \" Mary\"\n",
    "transformer_lens.utils.test_prompt(example_prompt, example_answer, gpt2, prepend_bos=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_prompt = \"After John and Mary went to the store, John gave a bottle of milk to\"\n",
    "example_answer = \" Mary\"\n",
    "transformer_lens.utils.test_prompt(example_prompt, example_answer, pruned_gpt2, prepend_bos=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wanda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_wanda(W, X_norm, sparse_ratio=0.5):\n",
    "    W_metric = W.abs() * X_norm\n",
    "    _, sorted_idx = W_metric.sort(dim=1)\n",
    "    pruned_idx = sorted_idx[:, :int(W.shape[1] * sparse_ratio)]\n",
    "    \n",
    "    W_clone = W.detach().clone()    \n",
    "    W_clone.scatter_(dim=1, index=pruned_idx, src=t.zeros_like(pruned_idx, dtype=W.dtype))\n",
    "    return W_clone\n",
    "\n",
    "W = t.tensor([\n",
    "    [4, 0, 1, -1],\n",
    "    [3, -2, -1, -3],\n",
    "    [-3, 1, 0, 2]\n",
    "])\n",
    "X = t.tensor([\n",
    "    [1, 2, 8, 3]\n",
    "])\n",
    "\n",
    "prune_wanda(W, X, sparse_ratio=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_wanda(W, X_norm, sparse_ratio=0.5):\n",
    "    W_metric = W.abs() * X_norm\n",
    "    _, sorted_idx = W_metric.sort(dim=1)\n",
    "    pruned_idx = sorted_idx[:, :int(W.shape[1] * sparse_ratio)]\n",
    "    \n",
    "    W_clone = W.detach().clone()    \n",
    "    W_clone.scatter_(dim=1, index=pruned_idx, src=t.zeros_like(pruned_idx, dtype=W.dtype))\n",
    "    return W_clone\n",
    "\n",
    "def prune_model(model, tokens):\n",
    "    wts_act = {\n",
    "    'attn.W_Q': 'attn.hook_q',\n",
    "    'attn.W_K': 'attn.hook_k',\n",
    "    'attn.W_V': 'attn.hook_v',\n",
    "    'attn.W_O': 'hook_attn_out',\n",
    "    'mlp.W_in': 'mlp.hook_pre',\n",
    "    'mlp.W_out': 'hook_mlp_out'\n",
    "    }\n",
    "    for layer in range(model.cfg.n_layers):\n",
    "        logits, cache = model.run_with_cache(tokens, remove_batch_dim=True)\n",
    "        for wt, act in wts_act.items():\n",
    "            W = model.get_parameter(f'blocks.{layer}.{wt}')\n",
    "            X = cache[f'blocks.{layer}.{act}']\n",
    "\n",
    "            if W.dim() == 3:\n",
    "                if 'W_O' in wt:\n",
    "                    X_norm = X.norm(p=2, dim=0)\n",
    "                    for head in range(W.shape[0]):\n",
    "                        W[head] = prune_wanda(W[head], X_norm, sparse_ratio=0.5)\n",
    "                        \n",
    "                else:\n",
    "                    for head in range(W.shape[0]):\n",
    "                        X_norm = X[:, head, :].norm(p=2, dim=0)\n",
    "                        W[head] = prune_wanda(W[head], X_norm, sparse_ratio=0.5)\n",
    "            else:\n",
    "                X_norm = X.norm(p=2, dim=0)\n",
    "                W = prune_wanda(W, X_norm, sparse_ratio=0.5)\n",
    "            \n",
    "    return model\n",
    "\n",
    "gpt2: sae_lens.HookedSAETransformer = sae_lens.HookedSAETransformer.from_pretrained(\"gpt2-small\", device=device)\n",
    "gpt2_text = \"A quick brown fox jumps over the lazy dog.\"\n",
    "gpt2_tokens = gpt2.to_tokens(gpt2_text)\n",
    "print(gpt2_tokens.shape)\n",
    "pruned_gpt2 = prune_model(gpt2, gpt2_tokens)\n",
    "gpt2: sae_lens.HookedSAETransformer = sae_lens.HookedSAETransformer.from_pretrained(\"gpt2-small\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"I\"\n",
    "answer = \" am\"\n",
    "transformer_lens.utils.test_prompt(prompt, answer, pruned_gpt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_prompt = \"After John and Mary went to the store, John gave a bottle of milk to\"\n",
    "example_answer = \" Mary\"\n",
    "transformer_lens.utils.test_prompt(example_prompt, example_answer, gpt2, prepend_bos=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_prompt = \"After John and Mary went to the store, John gave a bottle of milk to\"\n",
    "example_answer = \" Mary\"\n",
    "transformer_lens.utils.test_prompt(example_prompt, example_answer, pruned_gpt2, prepend_bos=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SAEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n",
      "Tokenized prompt: ['<|endoftext|>', 'After', ' John', ' and', ' Mary', ' went', ' to', ' the', ' store', ',', ' John', ' gave', ' a', ' bottle', ' of', ' milk', ' to']\n",
      "Tokenized answer: [' Mary']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">        Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">18.11</span><span style=\"font-weight: bold\"> Prob: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">61.18</span><span style=\"font-weight: bold\">% Token: | Mary|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m0\u001b[0m\u001b[1m        Logit: \u001b[0m\u001b[1;36m18.11\u001b[0m\u001b[1m Prob: \u001b[0m\u001b[1;36m61.18\u001b[0m\u001b[1m% Token: | Mary|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 18.11 Prob: 61.18% Token: | Mary|\n",
      "Top 1th token. Logit: 16.06 Prob:  7.82% Token: | the|\n",
      "Top 2th token. Logit: 15.85 Prob:  6.39% Token: | John|\n",
      "Top 3th token. Logit: 15.57 Prob:  4.80% Token: | them|\n",
      "Top 4th token. Logit: 15.08 Prob:  2.95% Token: | his|\n",
      "Top 5th token. Logit: 14.63 Prob:  1.88% Token: | a|\n",
      "Top 6th token. Logit: 14.18 Prob:  1.20% Token: | her|\n",
      "Top 7th token. Logit: 14.07 Prob:  1.07% Token: | their|\n",
      "Top 8th token. Logit: 13.62 Prob:  0.68% Token: | him|\n",
      "Top 9th token. Logit: 13.51 Prob:  0.61% Token: | Mrs|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">' Mary'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">)]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m' Mary'\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "t.set_grad_enabled(False)\n",
    "gpt2 = sae_lens.HookedSAETransformer.from_pretrained(\"gpt2-small\", device=device)\n",
    "gpt2.load_state_dict(t.load('/home/gupte.31/COLM/sae-compression/gpt2-small/pruned/pruned_gpt2_wanda.pth'))\n",
    "example_prompt = \"After John and Mary went to the store, John gave a bottle of milk to\"\n",
    "example_answer = \" Mary\"\n",
    "transformer_lens.utils.test_prompt(example_prompt, example_answer, gpt2, prepend_bos=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tabulate(gpt2_sae.cfg.__dict__.items(), headers=[\"name\", \"value\"], tablefmt=\"simple_outline\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Mitigating the risk of extinction from AI should be a global\"\n",
    "answer = \" priority\"\n",
    "\n",
    "# First see how the model does without SAEs\n",
    "test_prompt(prompt, answer, gpt2)\n",
    "\n",
    "# Test our prompt, to see what the model says\n",
    "with gpt2.saes(saes=[gpt2_sae]):\n",
    "    test_prompt(prompt, answer, gpt2)\n",
    "\n",
    "# # Same thing, done in a different way\n",
    "# model.add_sae(attn_sae)\n",
    "# test_prompt(prompt, answer, model)\n",
    "# model.reset_saes()  # Remember to always do this!\n",
    "\n",
    "# Using `run_with_saes` method in place of standard forward pass\n",
    "logits = gpt2(prompt, return_type=\"logits\")\n",
    "logits_sae = gpt2.run_with_saes(prompt, saes=[gpt2_sae], return_type=\"logits\")\n",
    "answer_token_id = gpt2.to_single_token(answer)\n",
    "\n",
    "# Getting model's prediction\n",
    "top_prob, token_id_prediction = logits[0, -1].softmax(-1).max(-1)\n",
    "top_prob_sae, token_id_prediction_sae = logits_sae[0, -1].softmax(-1).max(-1)\n",
    "\n",
    "print(f\"\"\"Standard model: top prediction = {gpt2.to_string(token_id_prediction)!r}, prob = {top_prob.item():.2%}\n",
    "SAE reconstruction: top prediction = {gpt2.to_string(token_id_prediction_sae)!r}, prob = {top_prob_sae.item():.2%}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Difference: In the demo, priority is the token reconstructed by the SAE. Does this mean we need more epochs? Or does this mean pruning has affected the SAE? Or possible reason\n",
    "\n",
    "**prepend_bos=False?** Check what is the default value of the parameter - Thats not the reason\n",
    "Also architecture is not the reason - standard or gated give the same output\n",
    "Standard has lower probability for priority as compared to gated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"After John and Mary went to the store, John gave a bottle of milk to\"\n",
    "answer = \" Mary\"\n",
    "\n",
    "# First see how the model does without SAEs\n",
    "test_prompt(prompt, answer, gpt2)\n",
    "\n",
    "# Test our prompt, to see what the model says\n",
    "with gpt2.saes(saes=[gpt2_sae]):\n",
    "    test_prompt(prompt, answer, gpt2)\n",
    "\n",
    "# Using `run_with_saes` method in place of standard forward pass\n",
    "logits = gpt2(prompt, return_type=\"logits\")\n",
    "logits_sae = gpt2.run_with_saes(prompt, saes=[gpt2_sae], return_type=\"logits\")\n",
    "answer_token_id = gpt2.to_single_token(answer)\n",
    "\n",
    "# Getting model's prediction\n",
    "top_prob, token_id_prediction = logits[0, -1].softmax(-1).max(-1)\n",
    "top_prob_sae, token_id_prediction_sae = logits_sae[0, -1].softmax(-1).max(-1)\n",
    "\n",
    "print(f\"\"\"Standard model: top prediction = {gpt2.to_string(token_id_prediction)!r}, prob = {top_prob.item():.2%}\n",
    "SAE reconstruction: top prediction = {gpt2.to_string(token_id_prediction_sae)!r}, prob = {top_prob_sae.item():.2%}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Higher probability in this case!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running SAES and replicating dashboards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replacing the activations with SAE reconstructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits_no_saes, cache_no_saes = gpt2.run_with_cache(prompt)\n",
    "\n",
    "gpt2_sae.use_error_term = False\n",
    "logits_with_sae_recon, cache_with_sae_recon = gpt2.run_with_cache_with_saes(prompt, saes=[gpt2_sae])\n",
    "\n",
    "gpt2_sae.use_error_term = True\n",
    "logits_without_sae_recon, cache_without_sae_recon = gpt2.run_with_cache_with_saes(prompt, saes=[gpt2_sae])\n",
    "\n",
    "# Both SAE caches contain the hook values\n",
    "assert f\"{gpt2_sae.cfg.hook_name}.hook_sae_acts_post\" in cache_with_sae_recon\n",
    "assert f\"{gpt2_sae.cfg.hook_name}.hook_sae_acts_post\" in cache_without_sae_recon\n",
    "\n",
    "# But the final output will be different, because we don't use SAE reconstructions when use_error_term=True\n",
    "t.testing.assert_close(logits_no_saes, logits_without_sae_recon)\n",
    "logit_diff_from_sae = (logits_no_saes - logits_with_sae_recon).abs().mean()\n",
    "print(f\"Average logit diff from using SAE reconstruction: {logit_diff_from_sae:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using ActivationsStore to load a bunch of data. It streams in data from a given dataset that was used to train the SAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gpt2_sae.cfg.dataset_path)\n",
    "\n",
    "gpt2_act_store = ActivationsStore.from_sae(\n",
    "    model=gpt2,\n",
    "    sae=gpt2_sae,\n",
    "    streaming=True,\n",
    "    store_batch_size_prompts=16,\n",
    "    n_batches_in_buffer=32,\n",
    "    device=str(device),\n",
    ")\n",
    "\n",
    "# Example of how you can use this:\n",
    "tokens = gpt2_act_store.get_batch_tokens()\n",
    "assert tokens.shape == (gpt2_act_store.store_batch_size_prompts, gpt2_act_store.context_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Irrelevant to attention layer understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Activation Distribution**: The distribution of latent's activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_activation_histogram(\n",
    "    model: HookedSAETransformer,\n",
    "    sae: SAE,\n",
    "    act_store: ActivationsStore,\n",
    "    latent_idx: int,\n",
    "    total_batches: int = 200,\n",
    "):\n",
    "    \"\"\"\n",
    "    Displays the activation histogram for a particular latent, computed across `total_batches` batches from `act_store`.\n",
    "    \"\"\"\n",
    "    sae_acts_post_hook_name = f\"{sae.cfg.hook_name}.hook_sae_acts_post\"\n",
    "    all_positive_acts = []\n",
    "\n",
    "    for i in tqdm(range(total_batches), desc=\"Computing activations for histogram\"):\n",
    "        tokens = act_store.get_batch_tokens()\n",
    "        _, cache = model.run_with_cache_with_saes(\n",
    "            tokens,\n",
    "            saes=[sae],\n",
    "            stop_at_layer=sae.cfg.hook_layer + 1,\n",
    "            names_filter=[sae_acts_post_hook_name],\n",
    "        )\n",
    "        acts = cache[sae_acts_post_hook_name][..., latent_idx]\n",
    "        all_positive_acts.extend(acts[acts > 0].cpu().tolist())\n",
    "\n",
    "    frac_active = len(all_positive_acts) / (total_batches * act_store.store_batch_size_prompts * act_store.context_size)\n",
    "\n",
    "    px.histogram(\n",
    "        all_positive_acts,\n",
    "        nbins=50,\n",
    "        title=f\"ACTIVATIONS DENSITY {frac_active:.3%}\",\n",
    "        labels={\"value\": \"Activation\"},\n",
    "        width=800,\n",
    "        template=\"ggplot2\",\n",
    "        color_discrete_sequence=[\"darkorange\"],\n",
    "    ).update_layout(bargap=0.02, showlegend=False).show()\n",
    "    \n",
    "show_activation_histogram(gpt2, gpt2_sae, gpt2_act_store, latent_idx=9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Top/Bottom Logits**: Most +ve or -ve logits in the logit weight distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_top_logits(\n",
    "    model: HookedSAETransformer,\n",
    "    sae: SAE,\n",
    "    latent_idx: int,\n",
    "    k: int = 10,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Displays the top & bottom logits for a particular latent.\n",
    "    \"\"\"\n",
    "    logits = sae.W_dec[latent_idx] @ model.W_U\n",
    "\n",
    "    pos_logits, pos_token_ids = logits.topk(k)\n",
    "    pos_tokens = model.to_str_tokens(pos_token_ids)\n",
    "    neg_logits, neg_token_ids = logits.topk(k, largest=False)\n",
    "    neg_tokens = model.to_str_tokens(neg_token_ids)\n",
    "\n",
    "    print(\n",
    "        tabulate(\n",
    "            zip(map(repr, neg_tokens), neg_logits, map(repr, pos_tokens), pos_logits),\n",
    "            headers=[\"Bottom tokens\", \"Value\", \"Top tokens\", \"Value\"],\n",
    "            tablefmt=\"simple_outline\",\n",
    "            stralign=\"right\",\n",
    "            numalign=\"left\",\n",
    "            floatfmt=\"+.3f\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "show_top_logits(gpt2, gpt2_sae, latent_idx=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Max Activating Examples**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_k_largest_indices(x: Float[Tensor, \"batch seq\"], k: int, buffer: int = 0) -> Int[Tensor, \"k 2\"]:\n",
    "    \"\"\"\n",
    "    The indices of the top k elements in the input tensor, i.e. output[i, :] is the (batch, seqpos) value of the i-th\n",
    "    largest element in x.\n",
    "\n",
    "    Won't choose any elements within `buffer` from the start or end of their sequence.\n",
    "    \"\"\"\n",
    "    if buffer > 0:\n",
    "        x = x[:, buffer:-buffer]\n",
    "    indices = x.flatten().topk(k=k).indices\n",
    "    rows = indices // x.size(1)\n",
    "    cols = indices % x.size(1) + buffer\n",
    "    return t.stack((rows, cols), dim=1)\n",
    "\n",
    "\n",
    "x = t.arange(40, device=device).reshape((2, 20))\n",
    "x[0, 10] += 50  # 2nd highest value\n",
    "x[0, 11] += 100  # highest value\n",
    "x[1, 1] += 150  # not inside buffer (it's less than 3 from the start of the sequence)\n",
    "top_indices = get_k_largest_indices(x, k=2, buffer=3)\n",
    "assert top_indices.tolist() == [[0, 11], [0, 10]]\n",
    "\n",
    "\n",
    "def index_with_buffer(\n",
    "    x: Float[Tensor, \"batch seq\"], indices: Int[Tensor, \"k 2\"], buffer: int | None = None\n",
    ") -> Float[Tensor, \"k *buffer_x2_plus1\"]:\n",
    "    \"\"\"\n",
    "    Indexes into `x` with `indices` (which should have come from the `get_k_largest_indices` function), and takes a\n",
    "    +-buffer range around each indexed element. If `indices` are less than `buffer` away from the start of a sequence\n",
    "    then we just take the first `2*buffer+1` elems (same for at the end of a sequence).\n",
    "\n",
    "    If `buffer` is None, then we don't add any buffer and just return the elements at the given indices.\n",
    "    \"\"\"\n",
    "    rows, cols = indices.unbind(dim=-1)\n",
    "    if buffer is not None:\n",
    "        rows = einops.repeat(rows, \"k -> k buffer\", buffer=buffer * 2 + 1)\n",
    "        cols[cols < buffer] = buffer\n",
    "        cols[cols > x.size(1) - buffer - 1] = x.size(1) - buffer - 1\n",
    "        cols = einops.repeat(cols, \"k -> k buffer\", buffer=buffer * 2 + 1) + t.arange(\n",
    "            -buffer, buffer + 1, device=cols.device\n",
    "        )\n",
    "    return x[rows, cols]\n",
    "\n",
    "\n",
    "x_top_values_with_context = index_with_buffer(x, top_indices, buffer=3)\n",
    "assert x_top_values_with_context[0].tolist() == [8, 9, 10 + 50, 11 + 100, 12, 13, 14]  # highest value in the middle\n",
    "assert x_top_values_with_context[1].tolist() == [7, 8, 9, 10 + 50, 11 + 100, 12, 13]  # 2nd highest value in the middle\n",
    "\n",
    "\n",
    "def display_top_seqs(data: list[tuple[float, list[str], int]]):\n",
    "    \"\"\"\n",
    "    Given a list of (activation: float, str_toks: list[str], seq_pos: int), displays a table of these sequences, with\n",
    "    the relevant token highlighted.\n",
    "\n",
    "    We also turn newlines into \"\\\\n\", and remove unknown tokens � (usually weird quotation marks) for readability.\n",
    "    \"\"\"\n",
    "    table = Table(\"Act\", \"Sequence\", title=\"Max Activating Examples\", show_lines=True)\n",
    "    for act, str_toks, seq_pos in data:\n",
    "        formatted_seq = (\n",
    "            \"\".join([f\"[b u green]{str_tok}[/]\" if i == seq_pos else str_tok for i, str_tok in enumerate(str_toks)])\n",
    "            .replace(\"�\", \"\")\n",
    "            .replace(\"\\n\", \"↵\")\n",
    "        )\n",
    "        table.add_row(f\"{act:.3f}\", repr(formatted_seq))\n",
    "    rprint(table)\n",
    "\n",
    "\n",
    "example_data = [\n",
    "    (0.5, [\" one\", \" two\", \" three\"], 0),\n",
    "    (1.5, [\" one\", \" two\", \" three\"], 1),\n",
    "    (2.5, [\" one\", \" two\", \" three\"], 2),\n",
    "]\n",
    "display_top_seqs(example_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_max_activating_examples(\n",
    "    model: HookedSAETransformer,\n",
    "    sae: SAE,\n",
    "    act_store: ActivationsStore,\n",
    "    latent_idx: int,\n",
    "    total_batches: int = 100,\n",
    "    k: int = 10,\n",
    "    buffer: int = 10,\n",
    ") -> list[tuple[float, list[str], int]]:\n",
    "    \"\"\"\n",
    "    Returns the max activating examples across a number of batches from the activations store.\n",
    "    \"\"\"\n",
    "    sae_acts_post_hook_name = f\"{sae.cfg.hook_name}.hook_sae_acts_post\"\n",
    "\n",
    "    # Create list to store the top k activations for each batch. Once we're done,\n",
    "    # we'll filter this to only contain the top k over all batches\n",
    "    data = []\n",
    "\n",
    "    for _ in tqdm(range(total_batches), desc=\"Computing activations for max activating examples\"):\n",
    "        tokens = act_store.get_batch_tokens()\n",
    "        _, cache = model.run_with_cache_with_saes(\n",
    "            tokens,\n",
    "            saes=[sae],\n",
    "            stop_at_layer=sae.cfg.hook_layer + 1,\n",
    "            names_filter=[sae_acts_post_hook_name],\n",
    "        )\n",
    "        acts = cache[sae_acts_post_hook_name][..., latent_idx]\n",
    "\n",
    "        # Get largest indices, get the corresponding max acts, and get the surrounding indices\n",
    "        k_largest_indices = get_k_largest_indices(acts, k=k, buffer=buffer)\n",
    "        tokens_with_buffer = index_with_buffer(tokens, k_largest_indices, buffer=buffer)\n",
    "        str_toks = [model.to_str_tokens(toks) for toks in tokens_with_buffer]\n",
    "        top_acts = index_with_buffer(acts, k_largest_indices).tolist()\n",
    "        data.extend(list(zip(top_acts, str_toks, [buffer] * len(str_toks))))\n",
    "\n",
    "    return sorted(data, key=lambda x: x[0], reverse=True)[:k]\n",
    "\n",
    "\n",
    "# Fetch & display the results\n",
    "buffer = 10\n",
    "data = fetch_max_activating_examples(gpt2, gpt2_sae, gpt2_act_store, latent_idx=9, buffer=buffer, k=5)\n",
    "display_top_seqs(data)\n",
    "\n",
    "# Test one of the results, to see if it matches the expected output\n",
    "first_seq_str_tokens = data[0][1]\n",
    "print(first_seq_str_tokens)\n",
    "# assert first_seq_str_tokens[buffer] == \" new\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Relevant to attention layer evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_saes = gpt2_sae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class AttnSeqDFA:\n",
    "    act: float\n",
    "    str_toks_dest: list[str]\n",
    "    str_toks_src: list[str]\n",
    "    dest_pos: int\n",
    "    src_pos: int\n",
    "\n",
    "\n",
    "def display_top_seqs_attn(data: list[AttnSeqDFA]):\n",
    "    \"\"\"\n",
    "    Same as previous function, but we now have 2 str_tok lists and 2 sequence positions to highlight, the first being\n",
    "    for top activations (destination token) and the second for top DFA (src token). We've given you a dataclass to help\n",
    "    keep track of this.\n",
    "    \"\"\"\n",
    "    table = Table(\n",
    "        \"Top Act\",\n",
    "        \"Src token DFA (for top dest token)\",\n",
    "        \"Dest token\",\n",
    "        title=\"Max Activating Examples\",\n",
    "        show_lines=True,\n",
    "    )\n",
    "    for seq in data:\n",
    "        formatted_seqs = [\n",
    "            repr(\n",
    "                \"\".join(\n",
    "                    [f\"[b u {color}]{str_tok}[/]\" if i == seq_pos else str_tok for i, str_tok in enumerate(str_toks)]\n",
    "                )\n",
    "                .replace(\"�\", \"\")\n",
    "                .replace(\"\\n\", \"↵\")\n",
    "            )\n",
    "            for str_toks, seq_pos, color in [\n",
    "                (seq.str_toks_src, seq.src_pos, \"dark_orange\"),\n",
    "                (seq.str_toks_dest, seq.dest_pos, \"green\"),\n",
    "            ]\n",
    "        ]\n",
    "        table.add_row(f\"{seq.act:.3f}\", *formatted_seqs)\n",
    "    rprint(table)\n",
    "\n",
    "\n",
    "str_toks = [\" one\", \" two\", \" three\", \" four\"]\n",
    "example_data = [\n",
    "    AttnSeqDFA(act=0.5, str_toks_dest=str_toks[1:], str_toks_src=str_toks[:-1], dest_pos=0, src_pos=0),\n",
    "    AttnSeqDFA(act=1.5, str_toks_dest=str_toks[1:], str_toks_src=str_toks[:-1], dest_pos=1, src_pos=1),\n",
    "    AttnSeqDFA(act=2.5, str_toks_dest=str_toks[1:], str_toks_src=str_toks[:-1], dest_pos=2, src_pos=0),\n",
    "]\n",
    "display_top_seqs_attn(example_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_max_activating_examples_attn(\n",
    "    model: HookedSAETransformer,\n",
    "    sae: SAE,\n",
    "    act_store: ActivationsStore,\n",
    "    latent_idx: int,\n",
    "    total_batches: int = 250,\n",
    "    k: int = 10,\n",
    "    buffer: int = 10,\n",
    ") -> list[AttnSeqDFA]:\n",
    "    \"\"\"\n",
    "    Returns the max activating examples across a number of batches from the activations store.\n",
    "    \"\"\"\n",
    "    sae_acts_pre_hook_name = f\"{sae.cfg.hook_name}.hook_sae_acts_pre\"\n",
    "    v_hook_name = get_act_name(\"v\", sae.cfg.hook_layer)\n",
    "    pattern_hook_name = get_act_name(\"pattern\", sae.cfg.hook_layer)\n",
    "    data = []\n",
    "\n",
    "    for _ in tqdm(range(total_batches), desc=\"Computing activations for max activating examples (attn)\"):\n",
    "        tokens = act_store.get_batch_tokens()\n",
    "        _, cache = model.run_with_cache_with_saes(\n",
    "            tokens,\n",
    "            saes=[sae],\n",
    "            stop_at_layer=sae.cfg.hook_layer + 1,\n",
    "            names_filter=[sae_acts_pre_hook_name, v_hook_name, pattern_hook_name],\n",
    "        )\n",
    "        acts = cache[sae_acts_pre_hook_name][..., latent_idx]  # [batch seq]\n",
    "\n",
    "        # Get largest indices (i.e. dest tokens), and the tokens at those positions (plus buffer)\n",
    "        k_largest_indices = get_k_largest_indices(acts, k=k, buffer=buffer)\n",
    "        top_acts = index_with_buffer(acts, k_largest_indices).tolist()\n",
    "        dest_toks_with_buffer = index_with_buffer(tokens, k_largest_indices, buffer=buffer)\n",
    "        str_toks_dest_list = [model.to_str_tokens(toks) for toks in dest_toks_with_buffer]\n",
    "\n",
    "        # Get src token value vectors & dest-to-src attention patterns, for each of our chosen dest tokens\n",
    "        batch_indices, dest_pos_indices = k_largest_indices.unbind(-1)\n",
    "        v = cache[v_hook_name][batch_indices]  # shape [k src n_heads d_head]\n",
    "        pattern = cache[pattern_hook_name][batch_indices, :, dest_pos_indices]  # shape [k n_heads src]\n",
    "\n",
    "        # Multiply them together to get weighted value vectors, and reshape them to d_in = n_heads * d_head\n",
    "        v_weighted = (v * einops.rearrange(pattern, \"k n src -> k src n 1\")).flatten(-2, -1)  # shape [k src d_in]\n",
    "\n",
    "        # Map through our SAE encoder to get direct feature attribution for each src token, and argmax over src tokens\n",
    "        dfa = v_weighted @ sae.W_enc[:, latent_idx]  # shape [k src]\n",
    "        src_pos_indices = dfa.argmax(dim=-1)\n",
    "        src_toks_with_buffer = index_with_buffer(tokens, t.stack([batch_indices, src_pos_indices], -1), buffer=buffer)\n",
    "        str_toks_src_list = [model.to_str_tokens(toks) for toks in src_toks_with_buffer]\n",
    "\n",
    "        # Add all this data to our list\n",
    "        for act, str_toks_dest, str_toks_src, src_pos in zip(\n",
    "            top_acts, str_toks_dest_list, str_toks_src_list, src_pos_indices\n",
    "        ):\n",
    "            data.append(\n",
    "                AttnSeqDFA(\n",
    "                    act=act,\n",
    "                    str_toks_dest=str_toks_dest,  # top activating dest tokens, with buffer\n",
    "                    str_toks_src=str_toks_src,  # top DFA src tokens for the dest token, with buffer\n",
    "                    dest_pos=buffer,  # dest token is always in the middle of its buffer\n",
    "                    src_pos=min(src_pos, buffer),  # src token might be before the middle, if near start of sequence\n",
    "                )\n",
    "            )\n",
    "\n",
    "    return sorted(data, key=lambda x: x.act, reverse=True)[:k]\n",
    "\n",
    "# Test your function: compare it to dashboard above (max DFA should come from src toks like \" guns\", \" firearms\")\n",
    "data = fetch_max_activating_examples_attn(gpt2, attn_saes, gpt2_act_store, latent_idx=1)\n",
    "display_top_seqs_attn(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.set_grad_enabled(False)\n",
    "gpt2 = sae_lens.HookedSAETransformer.from_pretrained(\"gpt2-small\", device=device)\n",
    "\n",
    "# SAE for full gpt2-small\n",
    "hf_repo_id = \"suchitg/sae_test\"\n",
    "sae_id = 'blocks.9.attn.hook_z-base-v1'\n",
    "gpt2_sae = sae_lens.SAE.from_pretrained(release=hf_repo_id, sae_id=sae_id, device=str(device))[0]\n",
    "\n",
    "gpt2_act_store = ActivationsStore.from_sae(\n",
    "    model=gpt2,\n",
    "    sae=gpt2_sae,\n",
    "    streaming=True,\n",
    "    store_batch_size_prompts=16,\n",
    "    n_batches_in_buffer=32,\n",
    "    device=str(device),\n",
    ")\n",
    "\n",
    "\n",
    "example_prompt = \"After John and Mary went to the store, John gave a bottle of milk to\"\n",
    "example_answer = \" Mary\"\n",
    "tokens = gpt2.to_tokens(example_prompt)\n",
    "\n",
    "\n",
    "\n",
    "sae_acts_pre_hook_name = f\"{gpt2_sae.cfg.hook_name}.hook_sae_acts_pre\"\n",
    "v_hook_name = get_act_name(\"v\", gpt2_sae.cfg.hook_layer)\n",
    "pattern_hook_name = get_act_name(\"pattern\", gpt2_sae.cfg.hook_layer)\n",
    "print(sae_acts_pre_hook_name, v_hook_name, pattern_hook_name)\n",
    "\n",
    "\n",
    "_, cache = gpt2.run_with_cache_with_saes(\n",
    "    tokens,\n",
    "    saes=[gpt2_sae],\n",
    "    stop_at_layer=gpt2_sae.cfg.hook_layer + 1,\n",
    "    names_filter=[sae_acts_pre_hook_name, v_hook_name, pattern_hook_name],\n",
    ")\n",
    "acts = cache[sae_acts_pre_hook_name]\n",
    "print(acts.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2_wanda = sae_lens.HookedSAETransformer.from_pretrained(\"gpt2-small\", device=device)\n",
    "gpt2_wanda.load_state_dict(t.load('pruned/pruned_gpt2_wanda.pth'))\n",
    "# example_prompt = \"After John and Mary went to the store, John gave a bottle of milk to\"\n",
    "# example_answer = \" Mary\"\n",
    "# transformer_lens.utils.test_prompt(example_prompt, example_answer, gpt2_wanda, prepend_bos=True)\n",
    "\n",
    "# SAE for wanda-pruned gpt2-small\n",
    "hf_repo_id = \"suchitg/sae_wanda\"\n",
    "sae_id = 'blocks.9.attn.hook_z-v1'\n",
    "gpt2_wanda_sae = sae_lens.SAE.from_pretrained(release=hf_repo_id, sae_id=sae_id, device=str(device))[0]\n",
    "\n",
    "\n",
    "sae_acts_pre_hook_name = f\"{gpt2_wanda_sae.cfg.hook_name}.hook_sae_acts_pre\"\n",
    "v_hook_name = get_act_name(\"v\", gpt2_wanda_sae.cfg.hook_layer)\n",
    "pattern_hook_name = get_act_name(\"pattern\", gpt2_wanda_sae.cfg.hook_layer)\n",
    "print(sae_acts_pre_hook_name, v_hook_name, pattern_hook_name)\n",
    "\n",
    "\n",
    "_, cache = gpt2_wanda.run_with_cache_with_saes(\n",
    "    tokens,\n",
    "    saes=[gpt2_wanda_sae],\n",
    "    stop_at_layer=gpt2_wanda_sae.cfg.hook_layer + 1,\n",
    "    names_filter=[sae_acts_pre_hook_name, v_hook_name, pattern_hook_name],\n",
    ")\n",
    "acts = cache[sae_acts_pre_hook_name]\n",
    "print(acts.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.set_grad_enabled(False)\n",
    "\n",
    "# Helper functions\n",
    "def show_activation_histogram(\n",
    "    model: HookedSAETransformer,\n",
    "    sae: SAE,\n",
    "    act_store: ActivationsStore,\n",
    "    latent_idx: int,\n",
    "    total_batches: int = 200,\n",
    "):\n",
    "    \"\"\"\n",
    "    Displays the activation histogram for a particular latent, computed across `total_batches` batches from `act_store`.\n",
    "    \"\"\"\n",
    "    sae_acts_post_hook_name = f\"{sae.cfg.hook_name}.hook_sae_acts_post\"\n",
    "    all_positive_acts = []\n",
    "\n",
    "    for i in tqdm(range(total_batches), desc=\"Computing activations for histogram\"):\n",
    "        tokens = act_store.get_batch_tokens()\n",
    "        _, cache = model.run_with_cache_with_saes(\n",
    "            tokens,\n",
    "            saes=[sae],\n",
    "            stop_at_layer=sae.cfg.hook_layer + 1,\n",
    "            names_filter=[sae_acts_post_hook_name],\n",
    "        )\n",
    "        acts = cache[sae_acts_post_hook_name][..., latent_idx]\n",
    "        all_positive_acts.extend(acts[acts > 0].cpu().tolist())\n",
    "\n",
    "    frac_active = len(all_positive_acts) / (total_batches * act_store.store_batch_size_prompts * act_store.context_size)\n",
    "\n",
    "    px.histogram(\n",
    "        all_positive_acts,\n",
    "        nbins=50,\n",
    "        title=f\"ACTIVATIONS DENSITY {frac_active:.3%}\",\n",
    "        labels={\"value\": \"Activation\"},\n",
    "        width=800,\n",
    "        template=\"ggplot2\",\n",
    "        color_discrete_sequence=[\"darkorange\"],\n",
    "    ).update_layout(bargap=0.02, showlegend=False).show()\n",
    "\n",
    "def show_top_logits(\n",
    "    model: HookedSAETransformer,\n",
    "    sae: SAE,\n",
    "    latent_idx: int,\n",
    "    k: int = 10,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Displays the top & bottom logits for a particular latent.\n",
    "    \"\"\"\n",
    "    logits = sae.W_dec[latent_idx] @ model.W_U\n",
    "\n",
    "    pos_logits, pos_token_ids = logits.topk(k)\n",
    "    pos_tokens = model.to_str_tokens(pos_token_ids)\n",
    "    neg_logits, neg_token_ids = logits.topk(k, largest=False)\n",
    "    neg_tokens = model.to_str_tokens(neg_token_ids)\n",
    "\n",
    "    print(\n",
    "        tabulate(\n",
    "            zip(map(repr, neg_tokens), neg_logits, map(repr, pos_tokens), pos_logits),\n",
    "            headers=[\"Bottom tokens\", \"Value\", \"Top tokens\", \"Value\"],\n",
    "            tablefmt=\"simple_outline\",\n",
    "            stralign=\"right\",\n",
    "            numalign=\"left\",\n",
    "            floatfmt=\"+.3f\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "@dataclass\n",
    "class AttnSeqDFA:\n",
    "    act: float\n",
    "    str_toks_dest: list[str]\n",
    "    str_toks_src: list[str]\n",
    "    dest_pos: int\n",
    "    src_pos: int\n",
    "\n",
    "def display_top_seqs_attn(data: list[AttnSeqDFA]):\n",
    "    \"\"\"\n",
    "    Same as previous function, but we now have 2 str_tok lists and 2 sequence positions to highlight, the first being\n",
    "    for top activations (destination token) and the second for top DFA (src token). We've given you a dataclass to help\n",
    "    keep track of this.\n",
    "    \"\"\"\n",
    "    table = Table(\n",
    "        \"Top Act\",\n",
    "        \"Src token DFA (for top dest token)\",\n",
    "        \"Dest token\",\n",
    "        title=\"Max Activating Examples\",\n",
    "        show_lines=True,\n",
    "    )\n",
    "    for seq in data:\n",
    "        formatted_seqs = [\n",
    "            repr(\n",
    "                \"\".join(\n",
    "                    [f\"[b u {color}]{str_tok}[/]\" if i == seq_pos else str_tok for i, str_tok in enumerate(str_toks)]\n",
    "                )\n",
    "                .replace(\"�\", \"\")\n",
    "                .replace(\"\\n\", \"↵\")\n",
    "            )\n",
    "            for str_toks, seq_pos, color in [\n",
    "                (seq.str_toks_src, seq.src_pos, \"dark_orange\"),\n",
    "                (seq.str_toks_dest, seq.dest_pos, \"green\"),\n",
    "            ]\n",
    "        ]\n",
    "        table.add_row(f\"{seq.act:.3f}\", *formatted_seqs)\n",
    "    rprint(table)\n",
    "\n",
    "def fetch_max_activating_examples_attn(\n",
    "    model: HookedSAETransformer,\n",
    "    sae: SAE,\n",
    "    act_store: ActivationsStore,\n",
    "    latent_idx: int,\n",
    "    total_batches: int = 250,\n",
    "    k: int = 10,\n",
    "    buffer: int = 10,\n",
    ") -> list[AttnSeqDFA]:\n",
    "    \"\"\"\n",
    "    Returns the max activating examples across a number of batches from the activations store.\n",
    "    \"\"\"\n",
    "    sae_acts_pre_hook_name = f\"{sae.cfg.hook_name}.hook_sae_acts_pre\"\n",
    "    v_hook_name = get_act_name(\"v\", sae.cfg.hook_layer)\n",
    "    pattern_hook_name = get_act_name(\"pattern\", sae.cfg.hook_layer)\n",
    "    data = []\n",
    "\n",
    "    for _ in tqdm(range(total_batches), desc=\"Computing activations for max activating examples (attn)\"):\n",
    "        tokens = act_store.get_batch_tokens()\n",
    "        _, cache = model.run_with_cache_with_saes(\n",
    "            tokens,\n",
    "            saes=[sae],\n",
    "            stop_at_layer=sae.cfg.hook_layer + 1,\n",
    "            names_filter=[sae_acts_pre_hook_name, v_hook_name, pattern_hook_name],\n",
    "        )\n",
    "        acts = cache[sae_acts_pre_hook_name][..., latent_idx]  # [batch seq]\n",
    "\n",
    "        # Get largest indices (i.e. dest tokens), and the tokens at those positions (plus buffer)\n",
    "        k_largest_indices = get_k_largest_indices(acts, k=k, buffer=buffer)\n",
    "        top_acts = index_with_buffer(acts, k_largest_indices).tolist()\n",
    "        dest_toks_with_buffer = index_with_buffer(tokens, k_largest_indices, buffer=buffer)\n",
    "        str_toks_dest_list = [model.to_str_tokens(toks) for toks in dest_toks_with_buffer]\n",
    "\n",
    "        # Get src token value vectors & dest-to-src attention patterns, for each of our chosen dest tokens\n",
    "        batch_indices, dest_pos_indices = k_largest_indices.unbind(-1)\n",
    "        v = cache[v_hook_name][batch_indices]  # shape [k src n_heads d_head]\n",
    "        pattern = cache[pattern_hook_name][batch_indices, :, dest_pos_indices]  # shape [k n_heads src]\n",
    "\n",
    "        # Multiply them together to get weighted value vectors, and reshape them to d_in = n_heads * d_head\n",
    "        v_weighted = (v * einops.rearrange(pattern, \"k n src -> k src n 1\")).flatten(-2, -1)  # shape [k src d_in]\n",
    "\n",
    "        # Map through our SAE encoder to get direct feature attribution for each src token, and argmax over src tokens\n",
    "        dfa = v_weighted @ sae.W_enc[:, latent_idx]  # shape [k src]\n",
    "        src_pos_indices = dfa.argmax(dim=-1)\n",
    "        src_toks_with_buffer = index_with_buffer(tokens, t.stack([batch_indices, src_pos_indices], -1), buffer=buffer)\n",
    "        str_toks_src_list = [model.to_str_tokens(toks) for toks in src_toks_with_buffer]\n",
    "\n",
    "        # Add all this data to our list\n",
    "        for act, str_toks_dest, str_toks_src, src_pos in zip(\n",
    "            top_acts, str_toks_dest_list, str_toks_src_list, src_pos_indices\n",
    "        ):\n",
    "            data.append(\n",
    "                AttnSeqDFA(\n",
    "                    act=act,\n",
    "                    str_toks_dest=str_toks_dest,  # top activating dest tokens, with buffer\n",
    "                    str_toks_src=str_toks_src,  # top DFA src tokens for the dest token, with buffer\n",
    "                    dest_pos=buffer,  # dest token is always in the middle of its buffer\n",
    "                    src_pos=min(src_pos, buffer),  # src token might be before the middle, if near start of sequence\n",
    "                )\n",
    "            )\n",
    "\n",
    "    return sorted(data, key=lambda x: x.act, reverse=True)[:k]\n",
    "\n",
    "def get_k_largest_indices(x: Float[Tensor, \"batch seq\"], k: int, buffer: int = 0) -> Int[Tensor, \"k 2\"]:\n",
    "    \"\"\"\n",
    "    The indices of the top k elements in the input tensor, i.e. output[i, :] is the (batch, seqpos) value of the i-th\n",
    "    largest element in x.\n",
    "\n",
    "    Won't choose any elements within `buffer` from the start or end of their sequence.\n",
    "    \"\"\"\n",
    "    if buffer > 0:\n",
    "        x = x[:, buffer:-buffer]\n",
    "    indices = x.flatten().topk(k=k).indices\n",
    "    rows = indices // x.size(1)\n",
    "    cols = indices % x.size(1) + buffer\n",
    "    return t.stack((rows, cols), dim=1)\n",
    "\n",
    "def index_with_buffer(\n",
    "    x: Float[Tensor, \"batch seq\"], indices: Int[Tensor, \"k 2\"], buffer: int | None = None\n",
    ") -> Float[Tensor, \"k *buffer_x2_plus1\"]:\n",
    "    \"\"\"\n",
    "    Indexes into `x` with `indices` (which should have come from the `get_k_largest_indices` function), and takes a\n",
    "    +-buffer range around each indexed element. If `indices` are less than `buffer` away from the start of a sequence\n",
    "    then we just take the first `2*buffer+1` elems (same for at the end of a sequence).\n",
    "\n",
    "    If `buffer` is None, then we don't add any buffer and just return the elements at the given indices.\n",
    "    \"\"\"\n",
    "    rows, cols = indices.unbind(dim=-1)\n",
    "    if buffer is not None:\n",
    "        rows = einops.repeat(rows, \"k -> k buffer\", buffer=buffer * 2 + 1)\n",
    "        cols[cols < buffer] = buffer\n",
    "        cols[cols > x.size(1) - buffer - 1] = x.size(1) - buffer - 1\n",
    "        cols = einops.repeat(cols, \"k -> k buffer\", buffer=buffer * 2 + 1) + t.arange(\n",
    "            -buffer, buffer + 1, device=cols.device\n",
    "        )\n",
    "    return x[rows, cols]\n",
    "\n",
    "def display_top_seqs(data: list[tuple[float, list[str], int]]):\n",
    "    \"\"\"\n",
    "    Given a list of (activation: float, str_toks: list[str], seq_pos: int), displays a table of these sequences, with\n",
    "    the relevant token highlighted.\n",
    "\n",
    "    We also turn newlines into \"\\\\n\", and remove unknown tokens � (usually weird quotation marks) for readability.\n",
    "    \"\"\"\n",
    "    table = Table(\"Act\", \"Sequence\", title=\"Max Activating Examples\", show_lines=True)\n",
    "    for act, str_toks, seq_pos in data:\n",
    "        formatted_seq = (\n",
    "            \"\".join([f\"[b u green]{str_tok}[/]\" if i == seq_pos else str_tok for i, str_tok in enumerate(str_toks)])\n",
    "            .replace(\"�\", \"\")\n",
    "            .replace(\"\\n\", \"↵\")\n",
    "        )\n",
    "        table.add_row(f\"{act:.3f}\", repr(formatted_seq))\n",
    "    rprint(table)\n",
    "\n",
    "\n",
    "'''\n",
    "Full gpt2-small\n",
    "'''\n",
    "gpt2 = sae_lens.HookedSAETransformer.from_pretrained(\"gpt2-small\", device=device)\n",
    "# example_prompt = \"After John and Mary went to the store, John gave a bottle of milk to\"\n",
    "# example_answer = \" Mary\"\n",
    "# transformer_lens.utils.test_prompt(example_prompt, example_answer, gpt2, prepend_bos=True)\n",
    "\n",
    "# SAE for full gpt2-small\n",
    "hf_repo_id = \"suchitg/sae_test\"\n",
    "sae_id = 'blocks.9.attn.hook_z-base-v1'\n",
    "gpt2_sae = sae_lens.SAE.from_pretrained(release=hf_repo_id, sae_id=sae_id, device=str(device))[0]\n",
    "# print(tabulate(gpt2_sae.cfg.__dict__.items(), headers=[\"name\", \"value\"], tablefmt=\"simple_outline\"))\n",
    "\n",
    "gpt2_act_store = ActivationsStore.from_sae(\n",
    "    model=gpt2,\n",
    "    sae=gpt2_sae,\n",
    "    streaming=True,\n",
    "    store_batch_size_prompts=16,\n",
    "    n_batches_in_buffer=32,\n",
    "    device=str(device),\n",
    ")\n",
    "\n",
    "# Example of how you can use this:\n",
    "tokens = gpt2_act_store.get_batch_tokens()\n",
    "assert tokens.shape == (gpt2_act_store.store_batch_size_prompts, gpt2_act_store.context_size)\n",
    "show_activation_histogram(gpt2, gpt2_sae, gpt2_act_store, latent_idx=9)\n",
    "show_top_logits(gpt2, gpt2_sae, latent_idx=9)\n",
    "data = fetch_max_activating_examples_attn(gpt2, gpt2_sae, gpt2_act_store, latent_idx=1)\n",
    "display_top_seqs_attn(data)\n",
    "\n",
    "'''\n",
    "WANDA PRUNED\n",
    "'''\n",
    "gpt2_wanda = sae_lens.HookedSAETransformer.from_pretrained(\"gpt2-small\", device=device)\n",
    "gpt2_wanda.load_state_dict(t.load('pruned/pruned_gpt2_wanda.pth'))\n",
    "# example_prompt = \"After John and Mary went to the store, John gave a bottle of milk to\"\n",
    "# example_answer = \" Mary\"\n",
    "# transformer_lens.utils.test_prompt(example_prompt, example_answer, gpt2_wanda, prepend_bos=True)\n",
    "\n",
    "# SAE for wanda-pruned gpt2-small\n",
    "hf_repo_id = \"suchitg/sae_wanda\"\n",
    "sae_id = 'blocks.9.attn.hook_z-v1'\n",
    "gpt2_wanda_sae = sae_lens.SAE.from_pretrained(release=hf_repo_id, sae_id=sae_id, device=str(device))[0]\n",
    "\n",
    "# print(tabulate(gpt2_wanda_sae.cfg.__dict__.items(), headers=[\"name\", \"value\"], tablefmt=\"simple_outline\"))\n",
    "\n",
    "gpt2_wanda_act_store = ActivationsStore.from_sae(\n",
    "    model=gpt2_wanda,\n",
    "    sae=gpt2_wanda_sae,\n",
    "    streaming=True,\n",
    "    store_batch_size_prompts=16,\n",
    "    n_batches_in_buffer=32,\n",
    "    device=str(device),\n",
    ")\n",
    "\n",
    "# Example of how you can use this:\n",
    "tokens = gpt2_wanda_act_store.get_batch_tokens()\n",
    "assert tokens.shape == (gpt2_wanda_act_store.store_batch_size_prompts, gpt2_wanda_act_store.context_size)\n",
    "show_activation_histogram(gpt2_wanda, gpt2_wanda_sae, gpt2_wanda_act_store, latent_idx=9)\n",
    "show_top_logits(gpt2_wanda, gpt2_wanda_sae, latent_idx=9)\n",
    "\n",
    "data = fetch_max_activating_examples_attn(gpt2_wanda, gpt2_wanda_sae, gpt2_wanda_act_store, latent_idx=9)\n",
    "display_top_seqs_attn(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Research Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Q. How different are the neuron activations of an SAE trained on wanda pruned gpt2-small as compared to an SAE trained on full gpt2-small? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Q. Does an SAE trained on wanda pruned gpt2-small replicate a wanda pruned SAE trained on full gpt2-small?\n",
    "\n",
    "Procedure: Wanda prune the SAE trained on full gpt2\n",
    "\n",
    "* How to get the input activations of the SAE to apply Wanda pruning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2 = sae_lens.HookedSAETransformer.from_pretrained(\"gpt2-small\", device=device)\n",
    "example_prompt = \"After John and Mary went to the store, John gave a bottle of milk to\"\n",
    "example_answer = \" Mary\"\n",
    "# transformer_lens.utils.test_prompt(example_prompt, example_answer, gpt2, prepend_bos=True)\n",
    "\n",
    "# SAE for full gpt2-small\n",
    "hf_repo_id = \"suchitg/sae_test\"\n",
    "sae_id = 'blocks.9.attn.hook_z-base-v1'\n",
    "gpt2_sae = sae_lens.SAE.from_pretrained(release=hf_repo_id, sae_id=sae_id, device=str(device))[0]\n",
    "# print(tabulate(gpt2_sae.cfg.__dict__.items(), headers=[\"name\", \"value\"], tablefmt=\"simple_outline\"))\n",
    "\n",
    "_, cache = gpt2.run_with_cache_with_saes(example_prompt, saes=[gpt2_sae])\n",
    "\n",
    "for name, param in cache.items():\n",
    "    if \"hook_sae\" in name:\n",
    "        print(f\"{name:<43}: {tuple(param.shape)}\")\n",
    "\n",
    "\n",
    "prompt = \"In the beginning, God created the heavens and the\"\n",
    "answer = \"earth\"\n",
    "\n",
    "# Show that the model can confidently predict the next token.\n",
    "test_prompt(prompt, answer, gpt2)\n",
    "print(gpt2_sae.use_error_term)  \n",
    "\n",
    "_, cache = gpt2.run_with_cache_with_saes(prompt, saes=[gpt2_sae])\n",
    "\n",
    "print([(k, v.shape) for k, v in cache.items() if \"sae\" in k])\n",
    "\n",
    "px.line(\n",
    "    cache[f\"{gpt2_sae.cfg.hook_name}.hook_sae_acts_post\"][0, -1, :].cpu().numpy(),\n",
    "    title=\"Feature activations at the final token position\",\n",
    "    labels={\"index\": \"Feature\", \"value\": \"Activation\"},\n",
    ").show()\n",
    "\n",
    "prompt = \"In the beginning, God created the cat and the\"\n",
    "answer = \"earth\"\n",
    "\n",
    "# here we see that removing the word \"Heavens\" is very effective at making the model no longer predict \"earth\".\n",
    "# instead the model predicts a bunch of different animals.\n",
    "# Can we work out which features fire differently which might explain this? (This is a toy example not meant to be super interesting)\n",
    "test_prompt(prompt, answer, gpt2)\n",
    "\n",
    "prompt = [\n",
    "    \"In the beginning, God created the heavens and the\",\n",
    "    \"In the beginning, God created the cat and the\",\n",
    "]\n",
    "_, cache = gpt2.run_with_cache_with_saes(prompt, saes=[gpt2_sae])\n",
    "print([(k, v.shape) for k, v in cache.items() if \"sae\" in k])\n",
    "\n",
    "feature_activation_df = pd.DataFrame(\n",
    "    cache[f\"{gpt2_sae.cfg.hook_name}.hook_sae_acts_post\"][0, -1, :].cpu().numpy(),\n",
    "    index=[f\"feature_{i}\" for i in range(gpt2_sae.cfg.d_sae)],\n",
    ")\n",
    "feature_activation_df.columns = [\"heavens_and_the\"]\n",
    "feature_activation_df[\"cat_and_the\"] = (\n",
    "    cache[f\"{gpt2_sae.cfg.hook_name}.hook_sae_acts_post\"][1, -1, :].cpu().numpy()\n",
    ")\n",
    "feature_activation_df[\"diff\"] = (\n",
    "    feature_activation_df[\"heavens_and_the\"] - feature_activation_df[\"cat_and_the\"]\n",
    ")\n",
    "\n",
    "fig = px.line(\n",
    "    feature_activation_df,\n",
    "    title=\"Feature activations for the prompt\",\n",
    "    labels={\"index\": \"Feature\", \"value\": \"Activation\"},\n",
    ")\n",
    "\n",
    "# hide the x-ticks\n",
    "fig.update_xaxes(showticklabels=False)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2 = sae_lens.HookedSAETransformer.from_pretrained(\"gpt2-small\", device=device)\n",
    "example_prompt = \"After John and Mary went to the store, John gave a bottle of milk to\"\n",
    "example_answer = \" Mary\"\n",
    "# transformer_lens.utils.test_prompt(example_prompt, example_answer, gpt2, prepend_bos=True)\n",
    "\n",
    "# SAE for full gpt2-small\n",
    "hf_repo_id = \"suchitg/sae_test\"\n",
    "sae_id = 'blocks.9.attn.hook_z-base-v1'\n",
    "gpt2_sae = sae_lens.SAE.from_pretrained(release=hf_repo_id, sae_id=sae_id, device=str(device))[0]\n",
    "\n",
    "prompt = \"Mitigating the risk of extinction from AI should be a global\"\n",
    "answer = \" priority\"\n",
    "\n",
    "# First see how the model does without SAEs\n",
    "test_prompt(prompt, answer, gpt2)\n",
    "\n",
    "# Test our prompt, to see what the model says\n",
    "with gpt2.saes(saes=[gpt2_sae]):\n",
    "    test_prompt(prompt, answer, gpt2)\n",
    "\n",
    "# Using `run_with_saes` method in place of standard forward pass\n",
    "logits = gpt2(prompt, return_type=\"logits\")\n",
    "logits_sae = gpt2.run_with_saes(prompt, saes=[gpt2_sae], return_type=\"logits\")\n",
    "answer_token_id = gpt2.to_single_token(answer)\n",
    "\n",
    "# Getting model's prediction\n",
    "top_prob, token_id_prediction = logits[0, -1].softmax(-1).max(-1)\n",
    "top_prob_sae, token_id_prediction_sae = logits_sae[0, -1].softmax(-1).max(-1)\n",
    "\n",
    "print(f\"\"\"Standard model: top prediction = {gpt2.to_string(token_id_prediction)!r}, prob = {top_prob.item():.2%}\n",
    "SAE reconstruction: top prediction = {gpt2.to_string(token_id_prediction_sae)!r}, prob = {top_prob_sae.item():.2%}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Mitigating the risk of extinction from AI should be a global\"\n",
    "answer = \" priority\"\n",
    "\n",
    "gpt2_sae.load_state_dict(t.load('pruned_gpt2_sae_wanda.pth'))\n",
    "\n",
    "# First see how the model does without SAEs\n",
    "test_prompt(prompt, answer, gpt2)\n",
    "\n",
    "# Test our prompt, to see what the model says\n",
    "with gpt2.saes(saes=[gpt2_sae]):\n",
    "    test_prompt(prompt, answer, gpt2)\n",
    "\n",
    "# Using `run_with_saes` method in place of standard forward pass\n",
    "logits = gpt2(prompt, return_type=\"logits\")\n",
    "logits_sae = gpt2.run_with_saes(prompt, saes=[gpt2_sae], return_type=\"logits\")\n",
    "answer_token_id = gpt2.to_single_token(answer)\n",
    "\n",
    "# Getting model's prediction\n",
    "top_prob, token_id_prediction = logits[0, -1].softmax(-1).max(-1)\n",
    "top_prob_sae, token_id_prediction_sae = logits_sae[0, -1].softmax(-1).max(-1)\n",
    "\n",
    "print(f\"\"\"Standard model: top prediction = {gpt2.to_string(token_id_prediction)!r}, prob = {top_prob.item():.2%}\n",
    "SAE reconstruction: top prediction = {gpt2.to_string(token_id_prediction_sae)!r}, prob = {top_prob_sae.item():.2%}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sae_lens.HookedSAETransformer.from_pretrained(\"gpt2-small\", device=device)\n",
    "hf_repo_id = \"suchitg/sae_test\"\n",
    "sae_id = 'blocks.9.attn.hook_z-base-v1'\n",
    "sae = sae_lens.SAE.from_pretrained(release=hf_repo_id, sae_id=sae_id, device=str(device))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sae.load_state_dict(t.load('pruned_gpt2_sae_wanda.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer_lens.utils import tokenize_and_concatenate\n",
    "\n",
    "dataset = load_dataset(\n",
    "    path=\"NeelNanda/pile-10k\",\n",
    "    split=\"train\",\n",
    "    streaming=False,\n",
    ")\n",
    "\n",
    "token_dataset = tokenize_and_concatenate(\n",
    "    dataset=dataset,  # type: ignore\n",
    "    tokenizer=model.tokenizer,  # type: ignore\n",
    "    streaming=True,\n",
    "    max_length=sae.cfg.context_size,\n",
    "    add_bos_token=sae.cfg.prepend_bos,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sae.eval()  # prevents error if we're expecting a dead neuron mask for who grads\n",
    "\n",
    "with t.no_grad():\n",
    "\n",
    "    # activation store can give us tokens.\n",
    "    batch_tokens = token_dataset[:8][\"tokens\"]\n",
    "    _, cache = model.run_with_cache(batch_tokens, prepend_bos=True)\n",
    "\n",
    "    # Use the SAE\n",
    "    feature_acts = sae.encode(cache[sae.cfg.hook_name])\n",
    "    sae_out = sae.decode(feature_acts)\n",
    "\n",
    "    # save some room\n",
    "    del cache\n",
    "\n",
    "    # ignore the bos token, get the number of features that activated in each token, averaged accross batch and position\n",
    "    l0 = (feature_acts[:, 1:] > 0).float().sum(-1).detach()\n",
    "    print(\"average l0\", l0.mean().item())\n",
    "    px.histogram(l0.flatten().cpu().numpy()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer_lens import utils\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "# next we want to do a reconstruction test.\n",
    "def reconstr_hook(activation, hook, sae_out):\n",
    "    return sae_out\n",
    "\n",
    "\n",
    "def zero_abl_hook(activation, hook):\n",
    "    return t.zeros_like(activation)\n",
    "\n",
    "\n",
    "print(\"Orig\", model(batch_tokens, return_type=\"loss\").item())\n",
    "print(\n",
    "    \"reconstr\",\n",
    "    model.run_with_hooks(\n",
    "        batch_tokens,\n",
    "        fwd_hooks=[\n",
    "            (\n",
    "                sae.cfg.hook_name,\n",
    "                partial(reconstr_hook, sae_out=sae_out),\n",
    "            )\n",
    "        ],\n",
    "        return_type=\"loss\",\n",
    "    ).item(),\n",
    ")\n",
    "print(\n",
    "    \"Zero\",\n",
    "    model.run_with_hooks(\n",
    "        batch_tokens,\n",
    "        return_type=\"loss\",\n",
    "        fwd_hooks=[(sae.cfg.hook_name, zero_abl_hook)],\n",
    "    ).item(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sae_lens.HookedSAETransformer.from_pretrained(\"gpt2-small\", device=device)\n",
    "model.load_state_dict(t.load('pruned/pruned_gpt2_wanda.pth'))\n",
    "\n",
    "# SAE for wanda-pruned gpt2-small\n",
    "hf_repo_id = \"suchitg/sae_wanda\"\n",
    "sae_id = 'blocks.9.attn.hook_z-v1'\n",
    "sae = sae_lens.SAE.from_pretrained(release=hf_repo_id, sae_id=sae_id, device=str(device))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sae.eval()  # prevents error if we're expecting a dead neuron mask for who grads\n",
    "\n",
    "with t.no_grad():\n",
    "\n",
    "    # activation store can give us tokens.\n",
    "    batch_tokens = token_dataset[:8][\"tokens\"]\n",
    "    _, cache = model.run_with_cache(batch_tokens, prepend_bos=True)\n",
    "\n",
    "    # Use the SAE\n",
    "    feature_acts = sae.encode(cache[sae.cfg.hook_name])\n",
    "    sae_out = sae.decode(feature_acts)\n",
    "\n",
    "    # save some room\n",
    "    del cache\n",
    "\n",
    "    # ignore the bos token, get the number of features that activated in each token, averaged accross batch and position\n",
    "    l0 = (feature_acts[:, 1:] > 0).float().sum(-1).detach()\n",
    "    print(\"average l0\", l0.mean().item())\n",
    "    px.histogram(l0.flatten().cpu().numpy()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer_lens import utils\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "# next we want to do a reconstruction test.\n",
    "def reconstr_hook(activation, hook, sae_out):\n",
    "    return sae_out\n",
    "\n",
    "\n",
    "def zero_abl_hook(activation, hook):\n",
    "    return t.zeros_like(activation)\n",
    "\n",
    "\n",
    "print(\"Orig\", model(batch_tokens, return_type=\"loss\").item())\n",
    "print(\n",
    "    \"reconstr\",\n",
    "    model.run_with_hooks(\n",
    "        batch_tokens,\n",
    "        fwd_hooks=[\n",
    "            (\n",
    "                sae.cfg.hook_name,\n",
    "                partial(reconstr_hook, sae_out=sae_out),\n",
    "            )\n",
    "        ],\n",
    "        return_type=\"loss\",\n",
    "    ).item(),\n",
    ")\n",
    "print(\n",
    "    \"Zero\",\n",
    "    model.run_with_hooks(\n",
    "        batch_tokens,\n",
    "        return_type=\"loss\",\n",
    "        fwd_hooks=[(sae.cfg.hook_name, zero_abl_hook)],\n",
    "    ).item(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2_wanda = sae_lens.HookedSAETransformer.from_pretrained(\"gpt2-small\", device=device)\n",
    "gpt2_wanda.load_state_dict(t.load('pruned/pruned_gpt2_wanda.pth'))\n",
    "\n",
    "# SAE for wanda-pruned gpt2-small\n",
    "hf_repo_id = \"suchitg/sae_wanda\"\n",
    "sae_id = 'blocks.9.attn.hook_z-v1'\n",
    "gpt2_wanda_sae = sae_lens.SAE.from_pretrained(release=hf_repo_id, sae_id=sae_id, device=str(device))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Mitigating the risk of extinction from AI should be a global\"\n",
    "answer = \" priority\"\n",
    "\n",
    "\n",
    "# First see how the model does without SAEs\n",
    "test_prompt(prompt, answer, gpt2_wanda)\n",
    "\n",
    "# Test our prompt, to see what the model says\n",
    "with gpt2_wanda.saes(saes=[gpt2_wanda_sae]):\n",
    "    test_prompt(prompt, answer, gpt2_wanda)\n",
    "\n",
    "# Using `run_with_saes` method in place of standard forward pass\n",
    "logits = gpt2_wanda(prompt, return_type=\"logits\")\n",
    "logits_sae = gpt2_wanda.run_with_saes(prompt, saes=[gpt2_wanda_sae], return_type=\"logits\")\n",
    "answer_token_id = gpt2_wanda.to_single_token(answer)\n",
    "\n",
    "# Getting model's prediction\n",
    "top_prob, token_id_prediction = logits[0, -1].softmax(-1).max(-1)\n",
    "top_prob_sae, token_id_prediction_sae = logits_sae[0, -1].softmax(-1).max(-1)\n",
    "\n",
    "print(f\"\"\"Standard model: top prediction = {gpt2_wanda.to_string(token_id_prediction)!r}, prob = {top_prob.item():.2%}\n",
    "SAE reconstruction: top prediction = {gpt2_wanda.to_string(token_id_prediction_sae)!r}, prob = {top_prob_sae.item():.2%}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Q. Can I predict either SAES based on the other?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gemma-2-2b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You tried to specify center_unembed=True for a model using logit softcap, but this can't be done! Softcapping is not invariant upon adding a constant Setting center_unembed=False instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09bd71ab34db4088a05852eaff98acfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gemma-2-2b into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "import torch as t\n",
    "from datasets import load_dataset\n",
    "import sae_lens\n",
    "import transformer_lens\n",
    "from sae_lens import (\n",
    "    SAE,\n",
    "    HookedSAETransformer,\n",
    ")\n",
    "from transformer_lens.utils import get_act_name, test_prompt, to_numpy\n",
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "model = HookedSAETransformer.from_pretrained(\"gemma-2-2b\")\n",
    "model.load_state_dict(t.load('/home/gupte.31/COLM/sae-compression/gemma2b/pruned/gemma-2-2b_wanda.pth'))\n",
    "\n",
    "t.set_grad_enabled(False)\n",
    "\n",
    "# next we want to do a reconstruction test.\n",
    "def reconstr_hook(activation, hook, sae_out):\n",
    "    return sae_out\n",
    "\n",
    "dataset = load_dataset(\n",
    "    path=\"NeelNanda/pile-10k\",\n",
    "    split=\"train\",\n",
    "    streaming=False,\n",
    ")\n",
    "\n",
    "from transformer_lens.utils import tokenize_and_concatenate\n",
    "\n",
    "token_dataset = tokenize_and_concatenate(\n",
    "    dataset=dataset,  # type: ignore\n",
    "    tokenizer=model.tokenizer,  # type: ignore\n",
    "    streaming=True,\n",
    "    max_length=1024,\n",
    "    add_bos_token=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard model: top prediction = ' priority', prob = 43.13%\n",
      "SAE reconstruction: top prediction = ' priority', prob = 38.53%\n",
      "\n",
      "Standard model: top prediction = ' Mary', prob = 41.80%\n",
      "SAE reconstruction: top prediction = ' Mary', prob = 79.80%\n",
      "\n",
      "Reconstuction loss: 3.3389153480529785\n"
     ]
    }
   ],
   "source": [
    "from sae_lens import SAE\n",
    "\n",
    "release = \"gemma-scope-2b-pt-res-canonical\"\n",
    "sae_id = \"layer_12/width_16k/canonical\"\n",
    "sae = SAE.from_pretrained(release, sae_id, device=\"cuda\")[0]\n",
    "\n",
    "prompt = \"Mitigating the risk of extinction from AI should be a global\"\n",
    "answer = \" priority\"\n",
    "\n",
    "# # First see how the model does without SAEs\n",
    "# test_prompt(prompt, answer, model)\n",
    "\n",
    "# # Test our prompt, to see what the model says\n",
    "# with model.saes(saes=[sae]):\n",
    "#     test_prompt(prompt, answer, model)\n",
    "\n",
    "# Using `run_with_saes` method in place of standard forward pass\n",
    "logits = model(prompt, return_type=\"logits\")\n",
    "logits_sae = model.run_with_saes(prompt, saes=[sae], return_type=\"logits\")\n",
    "answer_token_id = model.to_single_token(answer)\n",
    "\n",
    "# Getting model's prediction\n",
    "top_prob, token_id_prediction = logits[0, -1].softmax(-1).max(-1)\n",
    "top_prob_sae, token_id_prediction_sae = logits_sae[0, -1].softmax(-1).max(-1)\n",
    "\n",
    "print(f\"\"\"Standard model: top prediction = {model.to_string(token_id_prediction)!r}, prob = {top_prob.item():.2%}\n",
    "SAE reconstruction: top prediction = {model.to_string(token_id_prediction_sae)!r}, prob = {top_prob_sae.item():.2%}\n",
    "\"\"\")\n",
    "\n",
    "del logits, logits_sae, token_id_prediction, token_id_prediction_sae, top_prob, top_prob_sae\n",
    "t.cuda.empty_cache()\n",
    "\n",
    "# IOI task\n",
    "prompt = \"After John and Mary went to the store, John gave a bottle of milk to\"\n",
    "answer = \" Mary\"\n",
    "\n",
    "# # First see how the model does without SAEs\n",
    "# test_prompt(prompt, answer, model)\n",
    "\n",
    "# # Test our prompt, to see what the model says\n",
    "# with model.saes(saes=[sae]):\n",
    "#     test_prompt(prompt, answer, model)\n",
    "\n",
    "# Using `run_with_saes` method in place of standard forward pass\n",
    "logits = model(prompt, return_type=\"logits\")\n",
    "logits_sae = model.run_with_saes(prompt, saes=[sae], return_type=\"logits\")\n",
    "answer_token_id = model.to_single_token(answer)\n",
    "\n",
    "# Getting model's prediction\n",
    "top_prob, token_id_prediction = logits[0, -1].softmax(-1).max(-1)\n",
    "top_prob_sae, token_id_prediction_sae = logits_sae[0, -1].softmax(-1).max(-1)\n",
    "\n",
    "print(f\"\"\"Standard model: top prediction = {model.to_string(token_id_prediction)!r}, prob = {top_prob.item():.2%}\n",
    "SAE reconstruction: top prediction = {model.to_string(token_id_prediction_sae)!r}, prob = {top_prob_sae.item():.2%}\n",
    "\"\"\")\n",
    "\n",
    "del sae, logits, logits_sae, token_id_prediction, token_id_prediction_sae, top_prob, top_prob_sae\n",
    "t.cuda.empty_cache()\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "sae = SAE.from_pretrained(release, sae_id, device=\"cuda\")[0]\n",
    "\n",
    "sae.eval()  # prevents error if we're expecting a dead neuron mask for who grads\n",
    "with t.no_grad():\n",
    "\n",
    "    # activation store can give us tokens.\n",
    "    batch_tokens = token_dataset[:2][\"tokens\"]\n",
    "    _, cache = model.run_with_cache(batch_tokens, prepend_bos=True)\n",
    "\n",
    "    # Use the SAE\n",
    "    feature_acts = sae.encode(cache[sae.cfg.hook_name])\n",
    "    sae_out = sae.decode(feature_acts)\n",
    "\n",
    "    # save some room\n",
    "    del cache\n",
    "\n",
    "    print(\n",
    "    \"Reconstuction loss:\",\n",
    "    model.run_with_hooks(\n",
    "        batch_tokens,\n",
    "        fwd_hooks=[\n",
    "            (\n",
    "                sae.cfg.hook_name,\n",
    "                partial(reconstr_hook, sae_out=sae_out),\n",
    "            )\n",
    "        ],\n",
    "        return_type=\"loss\",\n",
    "    ).item(),\n",
    "    )\n",
    "\n",
    "del sae, sae_out, feature_acts, batch_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a1030bca2ce483d9ed1dbcdcd80c1c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "cfg.json:   0%|          | 0.00/1.02k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66b83cb57b2b485fbc4d8840a6bcd049",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sae_weights.safetensors:   0%|          | 0.00/302M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gupte.31/.conda/envs/SAE/lib/python3.10/site-packages/sae_lens/sae.py:146: UserWarning: \n",
      "This SAE has non-empty model_from_pretrained_kwargs. \n",
      "For optimal performance, load the model like so:\n",
      "model = HookedSAETransformer.from_pretrained_no_processing(..., **cfg.model_from_pretrained_kwargs)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard model: top prediction = ' priority', prob = 43.13%\n",
      "SAE reconstruction: top prediction = ' priority', prob = 30.36%\n",
      "\n",
      "Standard model: top prediction = ' Mary', prob = 41.80%\n",
      "SAE reconstruction: top prediction = ' Mary', prob = 49.08%\n",
      "\n",
      "Reconstuction loss: 3.224670886993408\n"
     ]
    }
   ],
   "source": [
    "release = \"suchitg/sae-compression-gemma-2-2b\"\n",
    "sae_id = \"blocks.12.hook_resid_post\"\n",
    "sae = SAE.from_pretrained(release, sae_id, device=\"cuda\")[0]\n",
    "\n",
    "\n",
    "prompt = \"Mitigating the risk of extinction from AI should be a global\"\n",
    "answer = \" priority\"\n",
    "\n",
    "# # First see how the model does without SAEs\n",
    "# test_prompt(prompt, answer, model)\n",
    "\n",
    "# # Test our prompt, to see what the model says\n",
    "# with model.saes(saes=[sae]):\n",
    "#     test_prompt(prompt, answer, model)\n",
    "\n",
    "# Using `run_with_saes` method in place of standard forward pass\n",
    "logits = model(prompt, return_type=\"logits\")\n",
    "logits_sae = model.run_with_saes(prompt, saes=[sae], return_type=\"logits\")\n",
    "answer_token_id = model.to_single_token(answer)\n",
    "\n",
    "# Getting model's prediction\n",
    "top_prob, token_id_prediction = logits[0, -1].softmax(-1).max(-1)\n",
    "top_prob_sae, token_id_prediction_sae = logits_sae[0, -1].softmax(-1).max(-1)\n",
    "\n",
    "print(f\"\"\"Standard model: top prediction = {model.to_string(token_id_prediction)!r}, prob = {top_prob.item():.2%}\n",
    "SAE reconstruction: top prediction = {model.to_string(token_id_prediction_sae)!r}, prob = {top_prob_sae.item():.2%}\n",
    "\"\"\")\n",
    "\n",
    "del logits, logits_sae, token_id_prediction, token_id_prediction_sae, top_prob, top_prob_sae\n",
    "\n",
    "\n",
    "# IOI task\n",
    "prompt = \"After John and Mary went to the store, John gave a bottle of milk to\"\n",
    "answer = \" Mary\"\n",
    "\n",
    "# # First see how the model does without SAEs\n",
    "# test_prompt(prompt, answer, model)\n",
    "\n",
    "# # Test our prompt, to see what the model says\n",
    "# with model.saes(saes=[sae]):\n",
    "#     test_prompt(prompt, answer, model)\n",
    "\n",
    "# Using `run_with_saes` method in place of standard forward pass\n",
    "logits = model(prompt, return_type=\"logits\")\n",
    "logits_sae = model.run_with_saes(prompt, saes=[sae], return_type=\"logits\")\n",
    "answer_token_id = model.to_single_token(answer)\n",
    "\n",
    "# Getting model's prediction\n",
    "top_prob, token_id_prediction = logits[0, -1].softmax(-1).max(-1)\n",
    "top_prob_sae, token_id_prediction_sae = logits_sae[0, -1].softmax(-1).max(-1)\n",
    "\n",
    "print(f\"\"\"Standard model: top prediction = {model.to_string(token_id_prediction)!r}, prob = {top_prob.item():.2%}\n",
    "SAE reconstruction: top prediction = {model.to_string(token_id_prediction_sae)!r}, prob = {top_prob_sae.item():.2%}\n",
    "\"\"\")\n",
    "del sae, logits, logits_sae, token_id_prediction, token_id_prediction_sae, top_prob, top_prob_sae\n",
    "t.cuda.empty_cache()\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "sae = SAE.from_pretrained(release, sae_id, device=\"cuda\")[0]\n",
    "\n",
    "sae.eval()  # prevents error if we're expecting a dead neuron mask for who grads\n",
    "with t.no_grad():\n",
    "\n",
    "    # activation store can give us tokens.\n",
    "    batch_tokens = token_dataset[:2][\"tokens\"]\n",
    "    _, cache = model.run_with_cache(batch_tokens, prepend_bos=True)\n",
    "\n",
    "    # Use the SAE\n",
    "    feature_acts = sae.encode(cache[sae.cfg.hook_name])\n",
    "    sae_out = sae.decode(feature_acts)\n",
    "\n",
    "    # save some room\n",
    "    del cache\n",
    "\n",
    "    print(\n",
    "    \"Reconstuction loss:\",\n",
    "    model.run_with_hooks(\n",
    "        batch_tokens,\n",
    "        fwd_hooks=[\n",
    "            (\n",
    "                sae.cfg.hook_name,\n",
    "                partial(reconstr_hook, sae_out=sae_out),\n",
    "            )\n",
    "        ],\n",
    "        return_type=\"loss\",\n",
    "    ).item(),\n",
    "    )\n",
    "\n",
    "del sae, sae_out, feature_acts, batch_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard model: top prediction = ' priority', prob = 43.13%\n",
      "SAE reconstruction: top prediction = ' priority', prob = 35.15%\n",
      "\n",
      "Standard model: top prediction = ' Mary', prob = 41.80%\n",
      "SAE reconstruction: top prediction = ' Mary', prob = 74.69%\n",
      "\n",
      "Reconstuction loss: 3.3414523601531982\n"
     ]
    }
   ],
   "source": [
    "from sae_lens import SAE\n",
    "\n",
    "release = \"gemma-scope-2b-pt-res-canonical\"\n",
    "sae_id = \"layer_12/width_16k/canonical\"\n",
    "sae = SAE.from_pretrained(release, sae_id, device=\"cuda\")[0]\n",
    "sae.load_state_dict(t.load('/local/scratch/suchit/COLM/pruned_saes/gemma-2-2b/wanda/pile/hook_resid_post/blocks.12.hook_resid_post.pth'))\n",
    "\n",
    "prompt = \"Mitigating the risk of extinction from AI should be a global\"\n",
    "answer = \" priority\"\n",
    "\n",
    "# # First see how the model does without SAEs\n",
    "# test_prompt(prompt, answer, model)\n",
    "\n",
    "# # Test our prompt, to see what the model says\n",
    "# with model.saes(saes=[sae]):\n",
    "#     test_prompt(prompt, answer, model)\n",
    "\n",
    "# Using `run_with_saes` method in place of standard forward pass\n",
    "logits = model(prompt, return_type=\"logits\")\n",
    "logits_sae = model.run_with_saes(prompt, saes=[sae], return_type=\"logits\")\n",
    "answer_token_id = model.to_single_token(answer)\n",
    "\n",
    "# Getting model's prediction\n",
    "top_prob, token_id_prediction = logits[0, -1].softmax(-1).max(-1)\n",
    "top_prob_sae, token_id_prediction_sae = logits_sae[0, -1].softmax(-1).max(-1)\n",
    "\n",
    "print(f\"\"\"Standard model: top prediction = {model.to_string(token_id_prediction)!r}, prob = {top_prob.item():.2%}\n",
    "SAE reconstruction: top prediction = {model.to_string(token_id_prediction_sae)!r}, prob = {top_prob_sae.item():.2%}\n",
    "\"\"\")\n",
    "\n",
    "del logits, logits_sae, token_id_prediction, token_id_prediction_sae, top_prob, top_prob_sae\n",
    "t.cuda.empty_cache()\n",
    "\n",
    "# IOI task\n",
    "prompt = \"After John and Mary went to the store, John gave a bottle of milk to\"\n",
    "answer = \" Mary\"\n",
    "\n",
    "# # First see how the model does without SAEs\n",
    "# test_prompt(prompt, answer, model)\n",
    "\n",
    "# # Test our prompt, to see what the model says\n",
    "# with model.saes(saes=[sae]):\n",
    "#     test_prompt(prompt, answer, model)\n",
    "\n",
    "# Using `run_with_saes` method in place of standard forward pass\n",
    "logits = model(prompt, return_type=\"logits\")\n",
    "logits_sae = model.run_with_saes(prompt, saes=[sae], return_type=\"logits\")\n",
    "answer_token_id = model.to_single_token(answer)\n",
    "\n",
    "# Getting model's prediction\n",
    "top_prob, token_id_prediction = logits[0, -1].softmax(-1).max(-1)\n",
    "top_prob_sae, token_id_prediction_sae = logits_sae[0, -1].softmax(-1).max(-1)\n",
    "\n",
    "print(f\"\"\"Standard model: top prediction = {model.to_string(token_id_prediction)!r}, prob = {top_prob.item():.2%}\n",
    "SAE reconstruction: top prediction = {model.to_string(token_id_prediction_sae)!r}, prob = {top_prob_sae.item():.2%}\n",
    "\"\"\")\n",
    "\n",
    "del sae, logits, logits_sae, token_id_prediction, token_id_prediction_sae, top_prob, top_prob_sae\n",
    "t.cuda.empty_cache()\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "sae = SAE.from_pretrained(release, sae_id, device=\"cuda\")[0]\n",
    "sae.load_state_dict(t.load('/local/scratch/suchit/COLM/pruned_saes/gemma-2-2b/wanda/pile/hook_resid_post/blocks.12.hook_resid_post.pth'))\n",
    "\n",
    "\n",
    "sae.eval()  # prevents error if we're expecting a dead neuron mask for who grads\n",
    "with t.no_grad():\n",
    "\n",
    "    # activation store can give us tokens.\n",
    "    batch_tokens = token_dataset[:2][\"tokens\"]\n",
    "    _, cache = model.run_with_cache(batch_tokens, prepend_bos=True)\n",
    "\n",
    "    # Use the SAE\n",
    "    feature_acts = sae.encode(cache[sae.cfg.hook_name])\n",
    "    sae_out = sae.decode(feature_acts)\n",
    "\n",
    "    # save some room\n",
    "    del cache\n",
    "\n",
    "    print(\n",
    "    \"Reconstuction loss:\",\n",
    "    model.run_with_hooks(\n",
    "        batch_tokens,\n",
    "        fwd_hooks=[\n",
    "            (\n",
    "                sae.cfg.hook_name,\n",
    "                partial(reconstr_hook, sae_out=sae_out),\n",
    "            )\n",
    "        ],\n",
    "        return_type=\"loss\",\n",
    "    ).item(),\n",
    "    )\n",
    "\n",
    "del sae, sae_out, feature_acts, batch_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard model: top prediction = ' priority', prob = 43.13%\n",
      "SAE reconstruction: top prediction = ' priority', prob = 36.63%\n",
      "\n",
      "Standard model: top prediction = ' Mary', prob = 41.80%\n",
      "SAE reconstruction: top prediction = ' Mary', prob = 67.17%\n",
      "\n",
      "Reconstuction loss: 3.4749696254730225\n"
     ]
    }
   ],
   "source": [
    "from sae_lens import SAE\n",
    "\n",
    "release = \"gemma-scope-2b-pt-res-canonical\"\n",
    "sae_id = \"layer_12/width_16k/canonical\"\n",
    "sae = SAE.from_pretrained(release, sae_id, device=\"cuda\")[0]\n",
    "sae.load_state_dict(t.load('/local/scratch/suchit/COLM/pruned_saes/gemma-2-2b/wanda/pile/hook_resid_post_ratio=0.5/blocks.12.hook_resid_post.pth'))\n",
    "\n",
    "prompt = \"Mitigating the risk of extinction from AI should be a global\"\n",
    "answer = \" priority\"\n",
    "\n",
    "# # First see how the model does without SAEs\n",
    "# test_prompt(prompt, answer, model)\n",
    "\n",
    "# # Test our prompt, to see what the model says\n",
    "# with model.saes(saes=[sae]):\n",
    "#     test_prompt(prompt, answer, model)\n",
    "\n",
    "# Using `run_with_saes` method in place of standard forward pass\n",
    "logits = model(prompt, return_type=\"logits\")\n",
    "logits_sae = model.run_with_saes(prompt, saes=[sae], return_type=\"logits\")\n",
    "answer_token_id = model.to_single_token(answer)\n",
    "\n",
    "# Getting model's prediction\n",
    "top_prob, token_id_prediction = logits[0, -1].softmax(-1).max(-1)\n",
    "top_prob_sae, token_id_prediction_sae = logits_sae[0, -1].softmax(-1).max(-1)\n",
    "\n",
    "print(f\"\"\"Standard model: top prediction = {model.to_string(token_id_prediction)!r}, prob = {top_prob.item():.2%}\n",
    "SAE reconstruction: top prediction = {model.to_string(token_id_prediction_sae)!r}, prob = {top_prob_sae.item():.2%}\n",
    "\"\"\")\n",
    "\n",
    "del logits, logits_sae, token_id_prediction, token_id_prediction_sae, top_prob, top_prob_sae\n",
    "t.cuda.empty_cache()\n",
    "\n",
    "# IOI task\n",
    "prompt = \"After John and Mary went to the store, John gave a bottle of milk to\"\n",
    "answer = \" Mary\"\n",
    "\n",
    "# # First see how the model does without SAEs\n",
    "# test_prompt(prompt, answer, model)\n",
    "\n",
    "# # Test our prompt, to see what the model says\n",
    "# with model.saes(saes=[sae]):\n",
    "#     test_prompt(prompt, answer, model)\n",
    "\n",
    "# Using `run_with_saes` method in place of standard forward pass\n",
    "logits = model(prompt, return_type=\"logits\")\n",
    "logits_sae = model.run_with_saes(prompt, saes=[sae], return_type=\"logits\")\n",
    "answer_token_id = model.to_single_token(answer)\n",
    "\n",
    "# Getting model's prediction\n",
    "top_prob, token_id_prediction = logits[0, -1].softmax(-1).max(-1)\n",
    "top_prob_sae, token_id_prediction_sae = logits_sae[0, -1].softmax(-1).max(-1)\n",
    "\n",
    "print(f\"\"\"Standard model: top prediction = {model.to_string(token_id_prediction)!r}, prob = {top_prob.item():.2%}\n",
    "SAE reconstruction: top prediction = {model.to_string(token_id_prediction_sae)!r}, prob = {top_prob_sae.item():.2%}\n",
    "\"\"\")\n",
    "\n",
    "del sae, logits, logits_sae, token_id_prediction, token_id_prediction_sae, top_prob, top_prob_sae\n",
    "t.cuda.empty_cache()\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "sae = SAE.from_pretrained(release, sae_id, device=\"cuda\")[0]\n",
    "sae.load_state_dict(t.load('/local/scratch/suchit/COLM/pruned_saes/gemma-2-2b/wanda/pile/hook_resid_post_ratio=0.5/blocks.12.hook_resid_post.pth'))\n",
    "\n",
    "\n",
    "\n",
    "sae.eval()  # prevents error if we're expecting a dead neuron mask for who grads\n",
    "with t.no_grad():\n",
    "\n",
    "    # activation store can give us tokens.\n",
    "    batch_tokens = token_dataset[:2][\"tokens\"]\n",
    "    _, cache = model.run_with_cache(batch_tokens, prepend_bos=True)\n",
    "\n",
    "    # Use the SAE\n",
    "    feature_acts = sae.encode(cache[sae.cfg.hook_name])\n",
    "    sae_out = sae.decode(feature_acts)\n",
    "\n",
    "    # save some room\n",
    "    del cache\n",
    "\n",
    "    print(\n",
    "    \"Reconstuction loss:\",\n",
    "    model.run_with_hooks(\n",
    "        batch_tokens,\n",
    "        fwd_hooks=[\n",
    "            (\n",
    "                sae.cfg.hook_name,\n",
    "                partial(reconstr_hook, sae_out=sae_out),\n",
    "            )\n",
    "        ],\n",
    "        return_type=\"loss\",\n",
    "    ).item(),\n",
    "    )\n",
    "\n",
    "del sae, sae_out, feature_acts, batch_tokens\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SAE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

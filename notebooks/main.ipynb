{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import itertools\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "from collections import Counter, defaultdict\n",
    "from copy import deepcopy\n",
    "from dataclasses import dataclass\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "from typing import Any, Callable, Literal, TypeAlias\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch as t\n",
    "from datasets import load_dataset\n",
    "import sae_lens\n",
    "import transformer_lens\n",
    "from sae_lens import (\n",
    "    SAE,\n",
    "    ActivationsStore,\n",
    "    HookedSAETransformer,\n",
    "    LanguageModelSAERunnerConfig,\n",
    "    SAEConfig,\n",
    "    SAETrainingRunner,\n",
    "    upload_saes_to_huggingface,\n",
    ")\n",
    "from sae_lens.toolkit.pretrained_saes_directory import get_pretrained_saes_directory\n",
    "from sae_vis import SaeVisConfig, SaeVisData, SaeVisLayoutConfig\n",
    "from tabulate import tabulate\n",
    "from torch import Tensor, nn\n",
    "from torch.distributions.categorical import Categorical\n",
    "from torch.nn import functional as F\n",
    "from tqdm.auto import tqdm\n",
    "from transformer_lens import ActivationCache, HookedTransformer\n",
    "from transformer_lens.hook_points import HookPoint\n",
    "from transformer_lens.utils import get_act_name, test_prompt, to_numpy\n",
    "\n",
    "import einops\n",
    "import circuitsvis as cv\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from IPython.display import HTML, IFrame, clear_output, display\n",
    "from jaxtyping import Float, Int\n",
    "from openai import OpenAI\n",
    "from rich import print as rprint\n",
    "from rich.table import Table\n",
    "from tabulate import tabulate\n",
    "\n",
    "from tqdm import tqdm\n",
    "device = t.device(\"cuda:1\" if t.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "# Hugging face: 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\n",
    "# Wandb: 6b549d940e7a29c79c184f27f25606e94a48a966"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attention SAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_saes = {\n",
    "    layer: sae_lens.SAE.from_pretrained(\n",
    "        \"gpt2-small-hook-z-kk\",\n",
    "        f\"blocks.{layer}.hook_z\",\n",
    "        device=device,\n",
    "    )[0]\n",
    "    for layer in range(12)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for attn_sae in attn_saes.values():\n",
    "    print(attn_sae.cfg)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(attn_saes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_dashboard(\n",
    "    sae_release=\"gpt2-small-res-jb\",\n",
    "    sae_id=\"blocks.7.hook_resid_pre\",\n",
    "    latent_idx=0,\n",
    "    width=800,\n",
    "    height=600,\n",
    "):\n",
    "    release = sae_lens.toolkit.pretrained_saes_directory.get_pretrained_saes_directory()[sae_release]\n",
    "    neuronpedia_id = release.neuronpedia_id[sae_id]\n",
    "\n",
    "    url = f\"https://neuronpedia.org/{neuronpedia_id}/{latent_idx}?embed=true&embedexplanation=true&embedplots=true&embedtest=true&height=300\"\n",
    "\n",
    "    print(url)\n",
    "    display(IFrame(url, width=width, height=height))\n",
    "\n",
    "\n",
    "layer = 9\n",
    "\n",
    "display_dashboard(\n",
    "    sae_release=\"gpt2-small-hook-z-kk\",\n",
    "    sae_id=f\"blocks.{layer}.hook_z\",\n",
    "    latent_idx=2,  # or you can try `random.randint(0, attn_saes[layer].cfg.d_sae)`\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2.cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Magnitude pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_magnitude(W, sparse_ratio=0.5):\n",
    "    W_abs = W.abs()\n",
    "    k = int(W_abs.numel() * sparse_ratio)\n",
    "    _, indices = W_abs.view(-1).topk(k)\n",
    "    mask = t.zeros_like(W_abs)\n",
    "    mask.view(-1)[indices] = 1\n",
    "    return mask*W\n",
    "\n",
    "def prune_model(model):\n",
    "    wts = ['W_Q', 'W_K', 'W_V', 'W_O', 'W_in', 'W_out']\n",
    "    for name, param in gpt2.named_parameters():\n",
    "        # print(f\"Layer: {name}, Shape: {param.shape}\")\n",
    "        if name.split('.')[-1] in wts:\n",
    "            if param.dim() == 3:    \n",
    "                for i in range(param.shape[0]):\n",
    "                    param[i] = prune_magnitude(param[i])\n",
    "            else:\n",
    "                param = prune_magnitude(param)\n",
    "    \n",
    "    return model\n",
    "\n",
    "gpt2: sae_lens.HookedSAETransformer = sae_lens.HookedSAETransformer.from_pretrained(\"gpt2-small\", device=device)\n",
    "pruned_gpt2 = prune_model(gpt2)\n",
    "gpt2: sae_lens.HookedSAETransformer = sae_lens.HookedSAETransformer.from_pretrained(\"gpt2-small\", device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"I\"\n",
    "answer = \" am\"\n",
    "transformer_lens.utils.test_prompt(prompt, answer, gpt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"I\"\n",
    "answer = \" am\"\n",
    "transformer_lens.utils.test_prompt(prompt, answer, pruned_gpt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_lens.evals.sanity_check(gpt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_lens.evals.sanity_check(pruned_gpt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_prompt = \"After John and Mary went to the store, John gave a bottle of milk to\"\n",
    "example_answer = \" Mary\"\n",
    "transformer_lens.utils.test_prompt(example_prompt, example_answer, gpt2, prepend_bos=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_prompt = \"After John and Mary went to the store, John gave a bottle of milk to\"\n",
    "example_answer = \" Mary\"\n",
    "transformer_lens.utils.test_prompt(example_prompt, example_answer, pruned_gpt2, prepend_bos=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wanda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_wanda(W, X_norm, sparse_ratio=0.5):\n",
    "    W_metric = W.abs() * X_norm\n",
    "    _, sorted_idx = W_metric.sort(dim=1)\n",
    "    pruned_idx = sorted_idx[:, :int(W.shape[1] * sparse_ratio)]\n",
    "    \n",
    "    W_clone = W.detach().clone()    \n",
    "    W_clone.scatter_(dim=1, index=pruned_idx, src=t.zeros_like(pruned_idx, dtype=W.dtype))\n",
    "    return W_clone\n",
    "\n",
    "W = t.tensor([\n",
    "    [4, 0, 1, -1],\n",
    "    [3, -2, -1, -3],\n",
    "    [-3, 1, 0, 2]\n",
    "])\n",
    "X = t.tensor([\n",
    "    [1, 2, 8, 3]\n",
    "])\n",
    "\n",
    "prune_wanda(W, X, sparse_ratio=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_wanda(W, X_norm, sparse_ratio=0.5):\n",
    "    W_metric = W.abs() * X_norm\n",
    "    _, sorted_idx = W_metric.sort(dim=1)\n",
    "    pruned_idx = sorted_idx[:, :int(W.shape[1] * sparse_ratio)]\n",
    "    \n",
    "    W_clone = W.detach().clone()    \n",
    "    W_clone.scatter_(dim=1, index=pruned_idx, src=t.zeros_like(pruned_idx, dtype=W.dtype))\n",
    "    return W_clone\n",
    "\n",
    "def prune_model(model, tokens):\n",
    "    wts_act = {\n",
    "    'attn.W_Q': 'attn.hook_q',\n",
    "    'attn.W_K': 'attn.hook_k',\n",
    "    'attn.W_V': 'attn.hook_v',\n",
    "    'attn.W_O': 'hook_attn_out',\n",
    "    'mlp.W_in': 'mlp.hook_pre',\n",
    "    'mlp.W_out': 'hook_mlp_out'\n",
    "    }\n",
    "    for layer in range(model.cfg.n_layers):\n",
    "        logits, cache = model.run_with_cache(tokens, remove_batch_dim=True)\n",
    "        for wt, act in wts_act.items():\n",
    "            W = model.get_parameter(f'blocks.{layer}.{wt}')\n",
    "            X = cache[f'blocks.{layer}.{act}']\n",
    "\n",
    "            if W.dim() == 3:\n",
    "                if 'W_O' in wt:\n",
    "                    X_norm = X.norm(p=2, dim=0)\n",
    "                    for head in range(W.shape[0]):\n",
    "                        W[head] = prune_wanda(W[head], X_norm, sparse_ratio=0.5)\n",
    "                        \n",
    "                else:\n",
    "                    for head in range(W.shape[0]):\n",
    "                        X_norm = X[:, head, :].norm(p=2, dim=0)\n",
    "                        W[head] = prune_wanda(W[head], X_norm, sparse_ratio=0.5)\n",
    "            else:\n",
    "                X_norm = X.norm(p=2, dim=0)\n",
    "                W = prune_wanda(W, X_norm, sparse_ratio=0.5)\n",
    "            \n",
    "    return model\n",
    "\n",
    "gpt2: sae_lens.HookedSAETransformer = sae_lens.HookedSAETransformer.from_pretrained(\"gpt2-small\", device=device)\n",
    "gpt2_text = \"A quick brown fox jumps over the lazy dog.\"\n",
    "gpt2_tokens = gpt2.to_tokens(gpt2_text)\n",
    "print(gpt2_tokens.shape)\n",
    "pruned_gpt2 = prune_model(gpt2, gpt2_tokens)\n",
    "gpt2: sae_lens.HookedSAETransformer = sae_lens.HookedSAETransformer.from_pretrained(\"gpt2-small\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"I\"\n",
    "answer = \" am\"\n",
    "transformer_lens.utils.test_prompt(prompt, answer, pruned_gpt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_prompt = \"After John and Mary went to the store, John gave a bottle of milk to\"\n",
    "example_answer = \" Mary\"\n",
    "transformer_lens.utils.test_prompt(example_prompt, example_answer, gpt2, prepend_bos=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_prompt = \"After John and Mary went to the store, John gave a bottle of milk to\"\n",
    "example_answer = \" Mary\"\n",
    "transformer_lens.utils.test_prompt(example_prompt, example_answer, pruned_gpt2, prepend_bos=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SAEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.set_grad_enabled(False)\n",
    "gpt2 = sae_lens.HookedSAETransformer.from_pretrained(\"gpt2-small\", device=device)\n",
    "gpt2.load_state_dict(t.load('/home/gupte.31/COLM/sae-compression/gpt2-small/pruned/pruned_gpt2_wanda.pth'))\n",
    "example_prompt = \"After John and Mary went to the store, John gave a bottle of milk to\"\n",
    "example_answer = \" Mary\"\n",
    "transformer_lens.utils.test_prompt(example_prompt, example_answer, gpt2, prepend_bos=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tabulate(gpt2_sae.cfg.__dict__.items(), headers=[\"name\", \"value\"], tablefmt=\"simple_outline\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Mitigating the risk of extinction from AI should be a global\"\n",
    "answer = \" priority\"\n",
    "\n",
    "# First see how the model does without SAEs\n",
    "test_prompt(prompt, answer, gpt2)\n",
    "\n",
    "# Test our prompt, to see what the model says\n",
    "with gpt2.saes(saes=[gpt2_sae]):\n",
    "    test_prompt(prompt, answer, gpt2)\n",
    "\n",
    "# # Same thing, done in a different way\n",
    "# model.add_sae(attn_sae)\n",
    "# test_prompt(prompt, answer, model)\n",
    "# model.reset_saes()  # Remember to always do this!\n",
    "\n",
    "# Using `run_with_saes` method in place of standard forward pass\n",
    "logits = gpt2(prompt, return_type=\"logits\")\n",
    "logits_sae = gpt2.run_with_saes(prompt, saes=[gpt2_sae], return_type=\"logits\")\n",
    "answer_token_id = gpt2.to_single_token(answer)\n",
    "\n",
    "# Getting model's prediction\n",
    "top_prob, token_id_prediction = logits[0, -1].softmax(-1).max(-1)\n",
    "top_prob_sae, token_id_prediction_sae = logits_sae[0, -1].softmax(-1).max(-1)\n",
    "\n",
    "print(f\"\"\"Standard model: top prediction = {gpt2.to_string(token_id_prediction)!r}, prob = {top_prob.item():.2%}\n",
    "SAE reconstruction: top prediction = {gpt2.to_string(token_id_prediction_sae)!r}, prob = {top_prob_sae.item():.2%}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Difference: In the demo, priority is the token reconstructed by the SAE. Does this mean we need more epochs? Or does this mean pruning has affected the SAE? Or possible reason\n",
    "\n",
    "**prepend_bos=False?** Check what is the default value of the parameter - Thats not the reason\n",
    "Also architecture is not the reason - standard or gated give the same output\n",
    "Standard has lower probability for priority as compared to gated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"After John and Mary went to the store, John gave a bottle of milk to\"\n",
    "answer = \" Mary\"\n",
    "\n",
    "# First see how the model does without SAEs\n",
    "test_prompt(prompt, answer, gpt2)\n",
    "\n",
    "# Test our prompt, to see what the model says\n",
    "with gpt2.saes(saes=[gpt2_sae]):\n",
    "    test_prompt(prompt, answer, gpt2)\n",
    "\n",
    "# Using `run_with_saes` method in place of standard forward pass\n",
    "logits = gpt2(prompt, return_type=\"logits\")\n",
    "logits_sae = gpt2.run_with_saes(prompt, saes=[gpt2_sae], return_type=\"logits\")\n",
    "answer_token_id = gpt2.to_single_token(answer)\n",
    "\n",
    "# Getting model's prediction\n",
    "top_prob, token_id_prediction = logits[0, -1].softmax(-1).max(-1)\n",
    "top_prob_sae, token_id_prediction_sae = logits_sae[0, -1].softmax(-1).max(-1)\n",
    "\n",
    "print(f\"\"\"Standard model: top prediction = {gpt2.to_string(token_id_prediction)!r}, prob = {top_prob.item():.2%}\n",
    "SAE reconstruction: top prediction = {gpt2.to_string(token_id_prediction_sae)!r}, prob = {top_prob_sae.item():.2%}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Higher probability in this case!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running SAES and replicating dashboards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replacing the activations with SAE reconstructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits_no_saes, cache_no_saes = gpt2.run_with_cache(prompt)\n",
    "\n",
    "gpt2_sae.use_error_term = False\n",
    "logits_with_sae_recon, cache_with_sae_recon = gpt2.run_with_cache_with_saes(prompt, saes=[gpt2_sae])\n",
    "\n",
    "gpt2_sae.use_error_term = True\n",
    "logits_without_sae_recon, cache_without_sae_recon = gpt2.run_with_cache_with_saes(prompt, saes=[gpt2_sae])\n",
    "\n",
    "# Both SAE caches contain the hook values\n",
    "assert f\"{gpt2_sae.cfg.hook_name}.hook_sae_acts_post\" in cache_with_sae_recon\n",
    "assert f\"{gpt2_sae.cfg.hook_name}.hook_sae_acts_post\" in cache_without_sae_recon\n",
    "\n",
    "# But the final output will be different, because we don't use SAE reconstructions when use_error_term=True\n",
    "t.testing.assert_close(logits_no_saes, logits_without_sae_recon)\n",
    "logit_diff_from_sae = (logits_no_saes - logits_with_sae_recon).abs().mean()\n",
    "print(f\"Average logit diff from using SAE reconstruction: {logit_diff_from_sae:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using ActivationsStore to load a bunch of data. It streams in data from a given dataset that was used to train the SAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gpt2_sae.cfg.dataset_path)\n",
    "\n",
    "gpt2_act_store = ActivationsStore.from_sae(\n",
    "    model=gpt2,\n",
    "    sae=gpt2_sae,\n",
    "    streaming=True,\n",
    "    store_batch_size_prompts=16,\n",
    "    n_batches_in_buffer=32,\n",
    "    device=str(device),\n",
    ")\n",
    "\n",
    "# Example of how you can use this:\n",
    "tokens = gpt2_act_store.get_batch_tokens()\n",
    "assert tokens.shape == (gpt2_act_store.store_batch_size_prompts, gpt2_act_store.context_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Irrelevant to attention layer understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Activation Distribution**: The distribution of latent's activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_activation_histogram(\n",
    "    model: HookedSAETransformer,\n",
    "    sae: SAE,\n",
    "    act_store: ActivationsStore,\n",
    "    latent_idx: int,\n",
    "    total_batches: int = 200,\n",
    "):\n",
    "    \"\"\"\n",
    "    Displays the activation histogram for a particular latent, computed across `total_batches` batches from `act_store`.\n",
    "    \"\"\"\n",
    "    sae_acts_post_hook_name = f\"{sae.cfg.hook_name}.hook_sae_acts_post\"\n",
    "    all_positive_acts = []\n",
    "\n",
    "    for i in tqdm(range(total_batches), desc=\"Computing activations for histogram\"):\n",
    "        tokens = act_store.get_batch_tokens()\n",
    "        _, cache = model.run_with_cache_with_saes(\n",
    "            tokens,\n",
    "            saes=[sae],\n",
    "            stop_at_layer=sae.cfg.hook_layer + 1,\n",
    "            names_filter=[sae_acts_post_hook_name],\n",
    "        )\n",
    "        acts = cache[sae_acts_post_hook_name][..., latent_idx]\n",
    "        all_positive_acts.extend(acts[acts > 0].cpu().tolist())\n",
    "\n",
    "    frac_active = len(all_positive_acts) / (total_batches * act_store.store_batch_size_prompts * act_store.context_size)\n",
    "\n",
    "    px.histogram(\n",
    "        all_positive_acts,\n",
    "        nbins=50,\n",
    "        title=f\"ACTIVATIONS DENSITY {frac_active:.3%}\",\n",
    "        labels={\"value\": \"Activation\"},\n",
    "        width=800,\n",
    "        template=\"ggplot2\",\n",
    "        color_discrete_sequence=[\"darkorange\"],\n",
    "    ).update_layout(bargap=0.02, showlegend=False).show()\n",
    "    \n",
    "show_activation_histogram(gpt2, gpt2_sae, gpt2_act_store, latent_idx=9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Top/Bottom Logits**: Most +ve or -ve logits in the logit weight distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_top_logits(\n",
    "    model: HookedSAETransformer,\n",
    "    sae: SAE,\n",
    "    latent_idx: int,\n",
    "    k: int = 10,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Displays the top & bottom logits for a particular latent.\n",
    "    \"\"\"\n",
    "    logits = sae.W_dec[latent_idx] @ model.W_U\n",
    "\n",
    "    pos_logits, pos_token_ids = logits.topk(k)\n",
    "    pos_tokens = model.to_str_tokens(pos_token_ids)\n",
    "    neg_logits, neg_token_ids = logits.topk(k, largest=False)\n",
    "    neg_tokens = model.to_str_tokens(neg_token_ids)\n",
    "\n",
    "    print(\n",
    "        tabulate(\n",
    "            zip(map(repr, neg_tokens), neg_logits, map(repr, pos_tokens), pos_logits),\n",
    "            headers=[\"Bottom tokens\", \"Value\", \"Top tokens\", \"Value\"],\n",
    "            tablefmt=\"simple_outline\",\n",
    "            stralign=\"right\",\n",
    "            numalign=\"left\",\n",
    "            floatfmt=\"+.3f\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "show_top_logits(gpt2, gpt2_sae, latent_idx=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Max Activating Examples**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_k_largest_indices(x: Float[Tensor, \"batch seq\"], k: int, buffer: int = 0) -> Int[Tensor, \"k 2\"]:\n",
    "    \"\"\"\n",
    "    The indices of the top k elements in the input tensor, i.e. output[i, :] is the (batch, seqpos) value of the i-th\n",
    "    largest element in x.\n",
    "\n",
    "    Won't choose any elements within `buffer` from the start or end of their sequence.\n",
    "    \"\"\"\n",
    "    if buffer > 0:\n",
    "        x = x[:, buffer:-buffer]\n",
    "    indices = x.flatten().topk(k=k).indices\n",
    "    rows = indices // x.size(1)\n",
    "    cols = indices % x.size(1) + buffer\n",
    "    return t.stack((rows, cols), dim=1)\n",
    "\n",
    "\n",
    "x = t.arange(40, device=device).reshape((2, 20))\n",
    "x[0, 10] += 50  # 2nd highest value\n",
    "x[0, 11] += 100  # highest value\n",
    "x[1, 1] += 150  # not inside buffer (it's less than 3 from the start of the sequence)\n",
    "top_indices = get_k_largest_indices(x, k=2, buffer=3)\n",
    "assert top_indices.tolist() == [[0, 11], [0, 10]]\n",
    "\n",
    "\n",
    "def index_with_buffer(\n",
    "    x: Float[Tensor, \"batch seq\"], indices: Int[Tensor, \"k 2\"], buffer: int | None = None\n",
    ") -> Float[Tensor, \"k *buffer_x2_plus1\"]:\n",
    "    \"\"\"\n",
    "    Indexes into `x` with `indices` (which should have come from the `get_k_largest_indices` function), and takes a\n",
    "    +-buffer range around each indexed element. If `indices` are less than `buffer` away from the start of a sequence\n",
    "    then we just take the first `2*buffer+1` elems (same for at the end of a sequence).\n",
    "\n",
    "    If `buffer` is None, then we don't add any buffer and just return the elements at the given indices.\n",
    "    \"\"\"\n",
    "    rows, cols = indices.unbind(dim=-1)\n",
    "    if buffer is not None:\n",
    "        rows = einops.repeat(rows, \"k -> k buffer\", buffer=buffer * 2 + 1)\n",
    "        cols[cols < buffer] = buffer\n",
    "        cols[cols > x.size(1) - buffer - 1] = x.size(1) - buffer - 1\n",
    "        cols = einops.repeat(cols, \"k -> k buffer\", buffer=buffer * 2 + 1) + t.arange(\n",
    "            -buffer, buffer + 1, device=cols.device\n",
    "        )\n",
    "    return x[rows, cols]\n",
    "\n",
    "\n",
    "x_top_values_with_context = index_with_buffer(x, top_indices, buffer=3)\n",
    "assert x_top_values_with_context[0].tolist() == [8, 9, 10 + 50, 11 + 100, 12, 13, 14]  # highest value in the middle\n",
    "assert x_top_values_with_context[1].tolist() == [7, 8, 9, 10 + 50, 11 + 100, 12, 13]  # 2nd highest value in the middle\n",
    "\n",
    "\n",
    "def display_top_seqs(data: list[tuple[float, list[str], int]]):\n",
    "    \"\"\"\n",
    "    Given a list of (activation: float, str_toks: list[str], seq_pos: int), displays a table of these sequences, with\n",
    "    the relevant token highlighted.\n",
    "\n",
    "    We also turn newlines into \"\\\\n\", and remove unknown tokens � (usually weird quotation marks) for readability.\n",
    "    \"\"\"\n",
    "    table = Table(\"Act\", \"Sequence\", title=\"Max Activating Examples\", show_lines=True)\n",
    "    for act, str_toks, seq_pos in data:\n",
    "        formatted_seq = (\n",
    "            \"\".join([f\"[b u green]{str_tok}[/]\" if i == seq_pos else str_tok for i, str_tok in enumerate(str_toks)])\n",
    "            .replace(\"�\", \"\")\n",
    "            .replace(\"\\n\", \"↵\")\n",
    "        )\n",
    "        table.add_row(f\"{act:.3f}\", repr(formatted_seq))\n",
    "    rprint(table)\n",
    "\n",
    "\n",
    "example_data = [\n",
    "    (0.5, [\" one\", \" two\", \" three\"], 0),\n",
    "    (1.5, [\" one\", \" two\", \" three\"], 1),\n",
    "    (2.5, [\" one\", \" two\", \" three\"], 2),\n",
    "]\n",
    "display_top_seqs(example_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_max_activating_examples(\n",
    "    model: HookedSAETransformer,\n",
    "    sae: SAE,\n",
    "    act_store: ActivationsStore,\n",
    "    latent_idx: int,\n",
    "    total_batches: int = 100,\n",
    "    k: int = 10,\n",
    "    buffer: int = 10,\n",
    ") -> list[tuple[float, list[str], int]]:\n",
    "    \"\"\"\n",
    "    Returns the max activating examples across a number of batches from the activations store.\n",
    "    \"\"\"\n",
    "    sae_acts_post_hook_name = f\"{sae.cfg.hook_name}.hook_sae_acts_post\"\n",
    "\n",
    "    # Create list to store the top k activations for each batch. Once we're done,\n",
    "    # we'll filter this to only contain the top k over all batches\n",
    "    data = []\n",
    "\n",
    "    for _ in tqdm(range(total_batches), desc=\"Computing activations for max activating examples\"):\n",
    "        tokens = act_store.get_batch_tokens()\n",
    "        _, cache = model.run_with_cache_with_saes(\n",
    "            tokens,\n",
    "            saes=[sae],\n",
    "            stop_at_layer=sae.cfg.hook_layer + 1,\n",
    "            names_filter=[sae_acts_post_hook_name],\n",
    "        )\n",
    "        acts = cache[sae_acts_post_hook_name][..., latent_idx]\n",
    "\n",
    "        # Get largest indices, get the corresponding max acts, and get the surrounding indices\n",
    "        k_largest_indices = get_k_largest_indices(acts, k=k, buffer=buffer)\n",
    "        tokens_with_buffer = index_with_buffer(tokens, k_largest_indices, buffer=buffer)\n",
    "        str_toks = [model.to_str_tokens(toks) for toks in tokens_with_buffer]\n",
    "        top_acts = index_with_buffer(acts, k_largest_indices).tolist()\n",
    "        data.extend(list(zip(top_acts, str_toks, [buffer] * len(str_toks))))\n",
    "\n",
    "    return sorted(data, key=lambda x: x[0], reverse=True)[:k]\n",
    "\n",
    "\n",
    "# Fetch & display the results\n",
    "buffer = 10\n",
    "data = fetch_max_activating_examples(gpt2, gpt2_sae, gpt2_act_store, latent_idx=9, buffer=buffer, k=5)\n",
    "display_top_seqs(data)\n",
    "\n",
    "# Test one of the results, to see if it matches the expected output\n",
    "first_seq_str_tokens = data[0][1]\n",
    "print(first_seq_str_tokens)\n",
    "# assert first_seq_str_tokens[buffer] == \" new\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Relevant to attention layer evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_saes = gpt2_sae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class AttnSeqDFA:\n",
    "    act: float\n",
    "    str_toks_dest: list[str]\n",
    "    str_toks_src: list[str]\n",
    "    dest_pos: int\n",
    "    src_pos: int\n",
    "\n",
    "\n",
    "def display_top_seqs_attn(data: list[AttnSeqDFA]):\n",
    "    \"\"\"\n",
    "    Same as previous function, but we now have 2 str_tok lists and 2 sequence positions to highlight, the first being\n",
    "    for top activations (destination token) and the second for top DFA (src token). We've given you a dataclass to help\n",
    "    keep track of this.\n",
    "    \"\"\"\n",
    "    table = Table(\n",
    "        \"Top Act\",\n",
    "        \"Src token DFA (for top dest token)\",\n",
    "        \"Dest token\",\n",
    "        title=\"Max Activating Examples\",\n",
    "        show_lines=True,\n",
    "    )\n",
    "    for seq in data:\n",
    "        formatted_seqs = [\n",
    "            repr(\n",
    "                \"\".join(\n",
    "                    [f\"[b u {color}]{str_tok}[/]\" if i == seq_pos else str_tok for i, str_tok in enumerate(str_toks)]\n",
    "                )\n",
    "                .replace(\"�\", \"\")\n",
    "                .replace(\"\\n\", \"↵\")\n",
    "            )\n",
    "            for str_toks, seq_pos, color in [\n",
    "                (seq.str_toks_src, seq.src_pos, \"dark_orange\"),\n",
    "                (seq.str_toks_dest, seq.dest_pos, \"green\"),\n",
    "            ]\n",
    "        ]\n",
    "        table.add_row(f\"{seq.act:.3f}\", *formatted_seqs)\n",
    "    rprint(table)\n",
    "\n",
    "\n",
    "str_toks = [\" one\", \" two\", \" three\", \" four\"]\n",
    "example_data = [\n",
    "    AttnSeqDFA(act=0.5, str_toks_dest=str_toks[1:], str_toks_src=str_toks[:-1], dest_pos=0, src_pos=0),\n",
    "    AttnSeqDFA(act=1.5, str_toks_dest=str_toks[1:], str_toks_src=str_toks[:-1], dest_pos=1, src_pos=1),\n",
    "    AttnSeqDFA(act=2.5, str_toks_dest=str_toks[1:], str_toks_src=str_toks[:-1], dest_pos=2, src_pos=0),\n",
    "]\n",
    "display_top_seqs_attn(example_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_max_activating_examples_attn(\n",
    "    model: HookedSAETransformer,\n",
    "    sae: SAE,\n",
    "    act_store: ActivationsStore,\n",
    "    latent_idx: int,\n",
    "    total_batches: int = 250,\n",
    "    k: int = 10,\n",
    "    buffer: int = 10,\n",
    ") -> list[AttnSeqDFA]:\n",
    "    \"\"\"\n",
    "    Returns the max activating examples across a number of batches from the activations store.\n",
    "    \"\"\"\n",
    "    sae_acts_pre_hook_name = f\"{sae.cfg.hook_name}.hook_sae_acts_pre\"\n",
    "    v_hook_name = get_act_name(\"v\", sae.cfg.hook_layer)\n",
    "    pattern_hook_name = get_act_name(\"pattern\", sae.cfg.hook_layer)\n",
    "    data = []\n",
    "\n",
    "    for _ in tqdm(range(total_batches), desc=\"Computing activations for max activating examples (attn)\"):\n",
    "        tokens = act_store.get_batch_tokens()\n",
    "        _, cache = model.run_with_cache_with_saes(\n",
    "            tokens,\n",
    "            saes=[sae],\n",
    "            stop_at_layer=sae.cfg.hook_layer + 1,\n",
    "            names_filter=[sae_acts_pre_hook_name, v_hook_name, pattern_hook_name],\n",
    "        )\n",
    "        acts = cache[sae_acts_pre_hook_name][..., latent_idx]  # [batch seq]\n",
    "\n",
    "        # Get largest indices (i.e. dest tokens), and the tokens at those positions (plus buffer)\n",
    "        k_largest_indices = get_k_largest_indices(acts, k=k, buffer=buffer)\n",
    "        top_acts = index_with_buffer(acts, k_largest_indices).tolist()\n",
    "        dest_toks_with_buffer = index_with_buffer(tokens, k_largest_indices, buffer=buffer)\n",
    "        str_toks_dest_list = [model.to_str_tokens(toks) for toks in dest_toks_with_buffer]\n",
    "\n",
    "        # Get src token value vectors & dest-to-src attention patterns, for each of our chosen dest tokens\n",
    "        batch_indices, dest_pos_indices = k_largest_indices.unbind(-1)\n",
    "        v = cache[v_hook_name][batch_indices]  # shape [k src n_heads d_head]\n",
    "        pattern = cache[pattern_hook_name][batch_indices, :, dest_pos_indices]  # shape [k n_heads src]\n",
    "\n",
    "        # Multiply them together to get weighted value vectors, and reshape them to d_in = n_heads * d_head\n",
    "        v_weighted = (v * einops.rearrange(pattern, \"k n src -> k src n 1\")).flatten(-2, -1)  # shape [k src d_in]\n",
    "\n",
    "        # Map through our SAE encoder to get direct feature attribution for each src token, and argmax over src tokens\n",
    "        dfa = v_weighted @ sae.W_enc[:, latent_idx]  # shape [k src]\n",
    "        src_pos_indices = dfa.argmax(dim=-1)\n",
    "        src_toks_with_buffer = index_with_buffer(tokens, t.stack([batch_indices, src_pos_indices], -1), buffer=buffer)\n",
    "        str_toks_src_list = [model.to_str_tokens(toks) for toks in src_toks_with_buffer]\n",
    "\n",
    "        # Add all this data to our list\n",
    "        for act, str_toks_dest, str_toks_src, src_pos in zip(\n",
    "            top_acts, str_toks_dest_list, str_toks_src_list, src_pos_indices\n",
    "        ):\n",
    "            data.append(\n",
    "                AttnSeqDFA(\n",
    "                    act=act,\n",
    "                    str_toks_dest=str_toks_dest,  # top activating dest tokens, with buffer\n",
    "                    str_toks_src=str_toks_src,  # top DFA src tokens for the dest token, with buffer\n",
    "                    dest_pos=buffer,  # dest token is always in the middle of its buffer\n",
    "                    src_pos=min(src_pos, buffer),  # src token might be before the middle, if near start of sequence\n",
    "                )\n",
    "            )\n",
    "\n",
    "    return sorted(data, key=lambda x: x.act, reverse=True)[:k]\n",
    "\n",
    "# Test your function: compare it to dashboard above (max DFA should come from src toks like \" guns\", \" firearms\")\n",
    "data = fetch_max_activating_examples_attn(gpt2, attn_saes, gpt2_act_store, latent_idx=1)\n",
    "display_top_seqs_attn(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.set_grad_enabled(False)\n",
    "gpt2 = sae_lens.HookedSAETransformer.from_pretrained(\"gpt2-small\", device=device)\n",
    "\n",
    "# SAE for full gpt2-small\n",
    "hf_repo_id = \"suchitg/sae_test\"\n",
    "sae_id = 'blocks.9.attn.hook_z-base-v1'\n",
    "gpt2_sae = sae_lens.SAE.from_pretrained(release=hf_repo_id, sae_id=sae_id, device=str(device))[0]\n",
    "\n",
    "gpt2_act_store = ActivationsStore.from_sae(\n",
    "    model=gpt2,\n",
    "    sae=gpt2_sae,\n",
    "    streaming=True,\n",
    "    store_batch_size_prompts=16,\n",
    "    n_batches_in_buffer=32,\n",
    "    device=str(device),\n",
    ")\n",
    "\n",
    "\n",
    "example_prompt = \"After John and Mary went to the store, John gave a bottle of milk to\"\n",
    "example_answer = \" Mary\"\n",
    "tokens = gpt2.to_tokens(example_prompt)\n",
    "\n",
    "\n",
    "\n",
    "sae_acts_pre_hook_name = f\"{gpt2_sae.cfg.hook_name}.hook_sae_acts_pre\"\n",
    "v_hook_name = get_act_name(\"v\", gpt2_sae.cfg.hook_layer)\n",
    "pattern_hook_name = get_act_name(\"pattern\", gpt2_sae.cfg.hook_layer)\n",
    "print(sae_acts_pre_hook_name, v_hook_name, pattern_hook_name)\n",
    "\n",
    "\n",
    "_, cache = gpt2.run_with_cache_with_saes(\n",
    "    tokens,\n",
    "    saes=[gpt2_sae],\n",
    "    stop_at_layer=gpt2_sae.cfg.hook_layer + 1,\n",
    "    names_filter=[sae_acts_pre_hook_name, v_hook_name, pattern_hook_name],\n",
    ")\n",
    "acts = cache[sae_acts_pre_hook_name]\n",
    "print(acts.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2_wanda = sae_lens.HookedSAETransformer.from_pretrained(\"gpt2-small\", device=device)\n",
    "gpt2_wanda.load_state_dict(t.load('pruned/pruned_gpt2_wanda.pth'))\n",
    "# example_prompt = \"After John and Mary went to the store, John gave a bottle of milk to\"\n",
    "# example_answer = \" Mary\"\n",
    "# transformer_lens.utils.test_prompt(example_prompt, example_answer, gpt2_wanda, prepend_bos=True)\n",
    "\n",
    "# SAE for wanda-pruned gpt2-small\n",
    "hf_repo_id = \"suchitg/sae_wanda\"\n",
    "sae_id = 'blocks.9.attn.hook_z-v1'\n",
    "gpt2_wanda_sae = sae_lens.SAE.from_pretrained(release=hf_repo_id, sae_id=sae_id, device=str(device))[0]\n",
    "\n",
    "\n",
    "sae_acts_pre_hook_name = f\"{gpt2_wanda_sae.cfg.hook_name}.hook_sae_acts_pre\"\n",
    "v_hook_name = get_act_name(\"v\", gpt2_wanda_sae.cfg.hook_layer)\n",
    "pattern_hook_name = get_act_name(\"pattern\", gpt2_wanda_sae.cfg.hook_layer)\n",
    "print(sae_acts_pre_hook_name, v_hook_name, pattern_hook_name)\n",
    "\n",
    "\n",
    "_, cache = gpt2_wanda.run_with_cache_with_saes(\n",
    "    tokens,\n",
    "    saes=[gpt2_wanda_sae],\n",
    "    stop_at_layer=gpt2_wanda_sae.cfg.hook_layer + 1,\n",
    "    names_filter=[sae_acts_pre_hook_name, v_hook_name, pattern_hook_name],\n",
    ")\n",
    "acts = cache[sae_acts_pre_hook_name]\n",
    "print(acts.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.set_grad_enabled(False)\n",
    "\n",
    "# Helper functions\n",
    "def show_activation_histogram(\n",
    "    model: HookedSAETransformer,\n",
    "    sae: SAE,\n",
    "    act_store: ActivationsStore,\n",
    "    latent_idx: int,\n",
    "    total_batches: int = 200,\n",
    "):\n",
    "    \"\"\"\n",
    "    Displays the activation histogram for a particular latent, computed across `total_batches` batches from `act_store`.\n",
    "    \"\"\"\n",
    "    sae_acts_post_hook_name = f\"{sae.cfg.hook_name}.hook_sae_acts_post\"\n",
    "    all_positive_acts = []\n",
    "\n",
    "    for i in tqdm(range(total_batches), desc=\"Computing activations for histogram\"):\n",
    "        tokens = act_store.get_batch_tokens()\n",
    "        _, cache = model.run_with_cache_with_saes(\n",
    "            tokens,\n",
    "            saes=[sae],\n",
    "            stop_at_layer=sae.cfg.hook_layer + 1,\n",
    "            names_filter=[sae_acts_post_hook_name],\n",
    "        )\n",
    "        acts = cache[sae_acts_post_hook_name][..., latent_idx]\n",
    "        all_positive_acts.extend(acts[acts > 0].cpu().tolist())\n",
    "\n",
    "    frac_active = len(all_positive_acts) / (total_batches * act_store.store_batch_size_prompts * act_store.context_size)\n",
    "\n",
    "    px.histogram(\n",
    "        all_positive_acts,\n",
    "        nbins=50,\n",
    "        title=f\"ACTIVATIONS DENSITY {frac_active:.3%}\",\n",
    "        labels={\"value\": \"Activation\"},\n",
    "        width=800,\n",
    "        template=\"ggplot2\",\n",
    "        color_discrete_sequence=[\"darkorange\"],\n",
    "    ).update_layout(bargap=0.02, showlegend=False).show()\n",
    "\n",
    "def show_top_logits(\n",
    "    model: HookedSAETransformer,\n",
    "    sae: SAE,\n",
    "    latent_idx: int,\n",
    "    k: int = 10,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Displays the top & bottom logits for a particular latent.\n",
    "    \"\"\"\n",
    "    logits = sae.W_dec[latent_idx] @ model.W_U\n",
    "\n",
    "    pos_logits, pos_token_ids = logits.topk(k)\n",
    "    pos_tokens = model.to_str_tokens(pos_token_ids)\n",
    "    neg_logits, neg_token_ids = logits.topk(k, largest=False)\n",
    "    neg_tokens = model.to_str_tokens(neg_token_ids)\n",
    "\n",
    "    print(\n",
    "        tabulate(\n",
    "            zip(map(repr, neg_tokens), neg_logits, map(repr, pos_tokens), pos_logits),\n",
    "            headers=[\"Bottom tokens\", \"Value\", \"Top tokens\", \"Value\"],\n",
    "            tablefmt=\"simple_outline\",\n",
    "            stralign=\"right\",\n",
    "            numalign=\"left\",\n",
    "            floatfmt=\"+.3f\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "@dataclass\n",
    "class AttnSeqDFA:\n",
    "    act: float\n",
    "    str_toks_dest: list[str]\n",
    "    str_toks_src: list[str]\n",
    "    dest_pos: int\n",
    "    src_pos: int\n",
    "\n",
    "def display_top_seqs_attn(data: list[AttnSeqDFA]):\n",
    "    \"\"\"\n",
    "    Same as previous function, but we now have 2 str_tok lists and 2 sequence positions to highlight, the first being\n",
    "    for top activations (destination token) and the second for top DFA (src token). We've given you a dataclass to help\n",
    "    keep track of this.\n",
    "    \"\"\"\n",
    "    table = Table(\n",
    "        \"Top Act\",\n",
    "        \"Src token DFA (for top dest token)\",\n",
    "        \"Dest token\",\n",
    "        title=\"Max Activating Examples\",\n",
    "        show_lines=True,\n",
    "    )\n",
    "    for seq in data:\n",
    "        formatted_seqs = [\n",
    "            repr(\n",
    "                \"\".join(\n",
    "                    [f\"[b u {color}]{str_tok}[/]\" if i == seq_pos else str_tok for i, str_tok in enumerate(str_toks)]\n",
    "                )\n",
    "                .replace(\"�\", \"\")\n",
    "                .replace(\"\\n\", \"↵\")\n",
    "            )\n",
    "            for str_toks, seq_pos, color in [\n",
    "                (seq.str_toks_src, seq.src_pos, \"dark_orange\"),\n",
    "                (seq.str_toks_dest, seq.dest_pos, \"green\"),\n",
    "            ]\n",
    "        ]\n",
    "        table.add_row(f\"{seq.act:.3f}\", *formatted_seqs)\n",
    "    rprint(table)\n",
    "\n",
    "def fetch_max_activating_examples_attn(\n",
    "    model: HookedSAETransformer,\n",
    "    sae: SAE,\n",
    "    act_store: ActivationsStore,\n",
    "    latent_idx: int,\n",
    "    total_batches: int = 250,\n",
    "    k: int = 10,\n",
    "    buffer: int = 10,\n",
    ") -> list[AttnSeqDFA]:\n",
    "    \"\"\"\n",
    "    Returns the max activating examples across a number of batches from the activations store.\n",
    "    \"\"\"\n",
    "    sae_acts_pre_hook_name = f\"{sae.cfg.hook_name}.hook_sae_acts_pre\"\n",
    "    v_hook_name = get_act_name(\"v\", sae.cfg.hook_layer)\n",
    "    pattern_hook_name = get_act_name(\"pattern\", sae.cfg.hook_layer)\n",
    "    data = []\n",
    "\n",
    "    for _ in tqdm(range(total_batches), desc=\"Computing activations for max activating examples (attn)\"):\n",
    "        tokens = act_store.get_batch_tokens()\n",
    "        _, cache = model.run_with_cache_with_saes(\n",
    "            tokens,\n",
    "            saes=[sae],\n",
    "            stop_at_layer=sae.cfg.hook_layer + 1,\n",
    "            names_filter=[sae_acts_pre_hook_name, v_hook_name, pattern_hook_name],\n",
    "        )\n",
    "        acts = cache[sae_acts_pre_hook_name][..., latent_idx]  # [batch seq]\n",
    "\n",
    "        # Get largest indices (i.e. dest tokens), and the tokens at those positions (plus buffer)\n",
    "        k_largest_indices = get_k_largest_indices(acts, k=k, buffer=buffer)\n",
    "        top_acts = index_with_buffer(acts, k_largest_indices).tolist()\n",
    "        dest_toks_with_buffer = index_with_buffer(tokens, k_largest_indices, buffer=buffer)\n",
    "        str_toks_dest_list = [model.to_str_tokens(toks) for toks in dest_toks_with_buffer]\n",
    "\n",
    "        # Get src token value vectors & dest-to-src attention patterns, for each of our chosen dest tokens\n",
    "        batch_indices, dest_pos_indices = k_largest_indices.unbind(-1)\n",
    "        v = cache[v_hook_name][batch_indices]  # shape [k src n_heads d_head]\n",
    "        pattern = cache[pattern_hook_name][batch_indices, :, dest_pos_indices]  # shape [k n_heads src]\n",
    "\n",
    "        # Multiply them together to get weighted value vectors, and reshape them to d_in = n_heads * d_head\n",
    "        v_weighted = (v * einops.rearrange(pattern, \"k n src -> k src n 1\")).flatten(-2, -1)  # shape [k src d_in]\n",
    "\n",
    "        # Map through our SAE encoder to get direct feature attribution for each src token, and argmax over src tokens\n",
    "        dfa = v_weighted @ sae.W_enc[:, latent_idx]  # shape [k src]\n",
    "        src_pos_indices = dfa.argmax(dim=-1)\n",
    "        src_toks_with_buffer = index_with_buffer(tokens, t.stack([batch_indices, src_pos_indices], -1), buffer=buffer)\n",
    "        str_toks_src_list = [model.to_str_tokens(toks) for toks in src_toks_with_buffer]\n",
    "\n",
    "        # Add all this data to our list\n",
    "        for act, str_toks_dest, str_toks_src, src_pos in zip(\n",
    "            top_acts, str_toks_dest_list, str_toks_src_list, src_pos_indices\n",
    "        ):\n",
    "            data.append(\n",
    "                AttnSeqDFA(\n",
    "                    act=act,\n",
    "                    str_toks_dest=str_toks_dest,  # top activating dest tokens, with buffer\n",
    "                    str_toks_src=str_toks_src,  # top DFA src tokens for the dest token, with buffer\n",
    "                    dest_pos=buffer,  # dest token is always in the middle of its buffer\n",
    "                    src_pos=min(src_pos, buffer),  # src token might be before the middle, if near start of sequence\n",
    "                )\n",
    "            )\n",
    "\n",
    "    return sorted(data, key=lambda x: x.act, reverse=True)[:k]\n",
    "\n",
    "def get_k_largest_indices(x: Float[Tensor, \"batch seq\"], k: int, buffer: int = 0) -> Int[Tensor, \"k 2\"]:\n",
    "    \"\"\"\n",
    "    The indices of the top k elements in the input tensor, i.e. output[i, :] is the (batch, seqpos) value of the i-th\n",
    "    largest element in x.\n",
    "\n",
    "    Won't choose any elements within `buffer` from the start or end of their sequence.\n",
    "    \"\"\"\n",
    "    if buffer > 0:\n",
    "        x = x[:, buffer:-buffer]\n",
    "    indices = x.flatten().topk(k=k).indices\n",
    "    rows = indices // x.size(1)\n",
    "    cols = indices % x.size(1) + buffer\n",
    "    return t.stack((rows, cols), dim=1)\n",
    "\n",
    "def index_with_buffer(\n",
    "    x: Float[Tensor, \"batch seq\"], indices: Int[Tensor, \"k 2\"], buffer: int | None = None\n",
    ") -> Float[Tensor, \"k *buffer_x2_plus1\"]:\n",
    "    \"\"\"\n",
    "    Indexes into `x` with `indices` (which should have come from the `get_k_largest_indices` function), and takes a\n",
    "    +-buffer range around each indexed element. If `indices` are less than `buffer` away from the start of a sequence\n",
    "    then we just take the first `2*buffer+1` elems (same for at the end of a sequence).\n",
    "\n",
    "    If `buffer` is None, then we don't add any buffer and just return the elements at the given indices.\n",
    "    \"\"\"\n",
    "    rows, cols = indices.unbind(dim=-1)\n",
    "    if buffer is not None:\n",
    "        rows = einops.repeat(rows, \"k -> k buffer\", buffer=buffer * 2 + 1)\n",
    "        cols[cols < buffer] = buffer\n",
    "        cols[cols > x.size(1) - buffer - 1] = x.size(1) - buffer - 1\n",
    "        cols = einops.repeat(cols, \"k -> k buffer\", buffer=buffer * 2 + 1) + t.arange(\n",
    "            -buffer, buffer + 1, device=cols.device\n",
    "        )\n",
    "    return x[rows, cols]\n",
    "\n",
    "def display_top_seqs(data: list[tuple[float, list[str], int]]):\n",
    "    \"\"\"\n",
    "    Given a list of (activation: float, str_toks: list[str], seq_pos: int), displays a table of these sequences, with\n",
    "    the relevant token highlighted.\n",
    "\n",
    "    We also turn newlines into \"\\\\n\", and remove unknown tokens � (usually weird quotation marks) for readability.\n",
    "    \"\"\"\n",
    "    table = Table(\"Act\", \"Sequence\", title=\"Max Activating Examples\", show_lines=True)\n",
    "    for act, str_toks, seq_pos in data:\n",
    "        formatted_seq = (\n",
    "            \"\".join([f\"[b u green]{str_tok}[/]\" if i == seq_pos else str_tok for i, str_tok in enumerate(str_toks)])\n",
    "            .replace(\"�\", \"\")\n",
    "            .replace(\"\\n\", \"↵\")\n",
    "        )\n",
    "        table.add_row(f\"{act:.3f}\", repr(formatted_seq))\n",
    "    rprint(table)\n",
    "\n",
    "\n",
    "'''\n",
    "Full gpt2-small\n",
    "'''\n",
    "gpt2 = sae_lens.HookedSAETransformer.from_pretrained(\"gpt2-small\", device=device)\n",
    "# example_prompt = \"After John and Mary went to the store, John gave a bottle of milk to\"\n",
    "# example_answer = \" Mary\"\n",
    "# transformer_lens.utils.test_prompt(example_prompt, example_answer, gpt2, prepend_bos=True)\n",
    "\n",
    "# SAE for full gpt2-small\n",
    "hf_repo_id = \"suchitg/sae_test\"\n",
    "sae_id = 'blocks.9.attn.hook_z-base-v1'\n",
    "gpt2_sae = sae_lens.SAE.from_pretrained(release=hf_repo_id, sae_id=sae_id, device=str(device))[0]\n",
    "# print(tabulate(gpt2_sae.cfg.__dict__.items(), headers=[\"name\", \"value\"], tablefmt=\"simple_outline\"))\n",
    "\n",
    "gpt2_act_store = ActivationsStore.from_sae(\n",
    "    model=gpt2,\n",
    "    sae=gpt2_sae,\n",
    "    streaming=True,\n",
    "    store_batch_size_prompts=16,\n",
    "    n_batches_in_buffer=32,\n",
    "    device=str(device),\n",
    ")\n",
    "\n",
    "# Example of how you can use this:\n",
    "tokens = gpt2_act_store.get_batch_tokens()\n",
    "assert tokens.shape == (gpt2_act_store.store_batch_size_prompts, gpt2_act_store.context_size)\n",
    "show_activation_histogram(gpt2, gpt2_sae, gpt2_act_store, latent_idx=9)\n",
    "show_top_logits(gpt2, gpt2_sae, latent_idx=9)\n",
    "data = fetch_max_activating_examples_attn(gpt2, gpt2_sae, gpt2_act_store, latent_idx=1)\n",
    "display_top_seqs_attn(data)\n",
    "\n",
    "'''\n",
    "WANDA PRUNED\n",
    "'''\n",
    "gpt2_wanda = sae_lens.HookedSAETransformer.from_pretrained(\"gpt2-small\", device=device)\n",
    "gpt2_wanda.load_state_dict(t.load('pruned/pruned_gpt2_wanda.pth'))\n",
    "# example_prompt = \"After John and Mary went to the store, John gave a bottle of milk to\"\n",
    "# example_answer = \" Mary\"\n",
    "# transformer_lens.utils.test_prompt(example_prompt, example_answer, gpt2_wanda, prepend_bos=True)\n",
    "\n",
    "# SAE for wanda-pruned gpt2-small\n",
    "hf_repo_id = \"suchitg/sae_wanda\"\n",
    "sae_id = 'blocks.9.attn.hook_z-v1'\n",
    "gpt2_wanda_sae = sae_lens.SAE.from_pretrained(release=hf_repo_id, sae_id=sae_id, device=str(device))[0]\n",
    "\n",
    "# print(tabulate(gpt2_wanda_sae.cfg.__dict__.items(), headers=[\"name\", \"value\"], tablefmt=\"simple_outline\"))\n",
    "\n",
    "gpt2_wanda_act_store = ActivationsStore.from_sae(\n",
    "    model=gpt2_wanda,\n",
    "    sae=gpt2_wanda_sae,\n",
    "    streaming=True,\n",
    "    store_batch_size_prompts=16,\n",
    "    n_batches_in_buffer=32,\n",
    "    device=str(device),\n",
    ")\n",
    "\n",
    "# Example of how you can use this:\n",
    "tokens = gpt2_wanda_act_store.get_batch_tokens()\n",
    "assert tokens.shape == (gpt2_wanda_act_store.store_batch_size_prompts, gpt2_wanda_act_store.context_size)\n",
    "show_activation_histogram(gpt2_wanda, gpt2_wanda_sae, gpt2_wanda_act_store, latent_idx=9)\n",
    "show_top_logits(gpt2_wanda, gpt2_wanda_sae, latent_idx=9)\n",
    "\n",
    "data = fetch_max_activating_examples_attn(gpt2_wanda, gpt2_wanda_sae, gpt2_wanda_act_store, latent_idx=9)\n",
    "display_top_seqs_attn(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Research Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Q. How different are the neuron activations of an SAE trained on wanda pruned gpt2-small as compared to an SAE trained on full gpt2-small? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Q. Does an SAE trained on wanda pruned gpt2-small replicate a wanda pruned SAE trained on full gpt2-small?\n",
    "\n",
    "Procedure: Wanda prune the SAE trained on full gpt2\n",
    "\n",
    "* How to get the input activations of the SAE to apply Wanda pruning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2 = sae_lens.HookedSAETransformer.from_pretrained(\"gpt2-small\", device=device)\n",
    "example_prompt = \"After John and Mary went to the store, John gave a bottle of milk to\"\n",
    "example_answer = \" Mary\"\n",
    "# transformer_lens.utils.test_prompt(example_prompt, example_answer, gpt2, prepend_bos=True)\n",
    "\n",
    "# SAE for full gpt2-small\n",
    "hf_repo_id = \"suchitg/sae_test\"\n",
    "sae_id = 'blocks.9.attn.hook_z-base-v1'\n",
    "gpt2_sae = sae_lens.SAE.from_pretrained(release=hf_repo_id, sae_id=sae_id, device=str(device))[0]\n",
    "# print(tabulate(gpt2_sae.cfg.__dict__.items(), headers=[\"name\", \"value\"], tablefmt=\"simple_outline\"))\n",
    "\n",
    "_, cache = gpt2.run_with_cache_with_saes(example_prompt, saes=[gpt2_sae])\n",
    "\n",
    "for name, param in cache.items():\n",
    "    if \"hook_sae\" in name:\n",
    "        print(f\"{name:<43}: {tuple(param.shape)}\")\n",
    "\n",
    "\n",
    "prompt = \"In the beginning, God created the heavens and the\"\n",
    "answer = \"earth\"\n",
    "\n",
    "# Show that the model can confidently predict the next token.\n",
    "test_prompt(prompt, answer, gpt2)\n",
    "print(gpt2_sae.use_error_term)  \n",
    "\n",
    "_, cache = gpt2.run_with_cache_with_saes(prompt, saes=[gpt2_sae])\n",
    "\n",
    "print([(k, v.shape) for k, v in cache.items() if \"sae\" in k])\n",
    "\n",
    "px.line(\n",
    "    cache[f\"{gpt2_sae.cfg.hook_name}.hook_sae_acts_post\"][0, -1, :].cpu().numpy(),\n",
    "    title=\"Feature activations at the final token position\",\n",
    "    labels={\"index\": \"Feature\", \"value\": \"Activation\"},\n",
    ").show()\n",
    "\n",
    "prompt = \"In the beginning, God created the cat and the\"\n",
    "answer = \"earth\"\n",
    "\n",
    "# here we see that removing the word \"Heavens\" is very effective at making the model no longer predict \"earth\".\n",
    "# instead the model predicts a bunch of different animals.\n",
    "# Can we work out which features fire differently which might explain this? (This is a toy example not meant to be super interesting)\n",
    "test_prompt(prompt, answer, gpt2)\n",
    "\n",
    "prompt = [\n",
    "    \"In the beginning, God created the heavens and the\",\n",
    "    \"In the beginning, God created the cat and the\",\n",
    "]\n",
    "_, cache = gpt2.run_with_cache_with_saes(prompt, saes=[gpt2_sae])\n",
    "print([(k, v.shape) for k, v in cache.items() if \"sae\" in k])\n",
    "\n",
    "feature_activation_df = pd.DataFrame(\n",
    "    cache[f\"{gpt2_sae.cfg.hook_name}.hook_sae_acts_post\"][0, -1, :].cpu().numpy(),\n",
    "    index=[f\"feature_{i}\" for i in range(gpt2_sae.cfg.d_sae)],\n",
    ")\n",
    "feature_activation_df.columns = [\"heavens_and_the\"]\n",
    "feature_activation_df[\"cat_and_the\"] = (\n",
    "    cache[f\"{gpt2_sae.cfg.hook_name}.hook_sae_acts_post\"][1, -1, :].cpu().numpy()\n",
    ")\n",
    "feature_activation_df[\"diff\"] = (\n",
    "    feature_activation_df[\"heavens_and_the\"] - feature_activation_df[\"cat_and_the\"]\n",
    ")\n",
    "\n",
    "fig = px.line(\n",
    "    feature_activation_df,\n",
    "    title=\"Feature activations for the prompt\",\n",
    "    labels={\"index\": \"Feature\", \"value\": \"Activation\"},\n",
    ")\n",
    "\n",
    "# hide the x-ticks\n",
    "fig.update_xaxes(showticklabels=False)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2 = sae_lens.HookedSAETransformer.from_pretrained(\"gpt2-small\", device=device)\n",
    "example_prompt = \"After John and Mary went to the store, John gave a bottle of milk to\"\n",
    "example_answer = \" Mary\"\n",
    "# transformer_lens.utils.test_prompt(example_prompt, example_answer, gpt2, prepend_bos=True)\n",
    "\n",
    "# SAE for full gpt2-small\n",
    "hf_repo_id = \"suchitg/sae_test\"\n",
    "sae_id = 'blocks.9.attn.hook_z-base-v1'\n",
    "gpt2_sae = sae_lens.SAE.from_pretrained(release=hf_repo_id, sae_id=sae_id, device=str(device))[0]\n",
    "\n",
    "prompt = \"Mitigating the risk of extinction from AI should be a global\"\n",
    "answer = \" priority\"\n",
    "\n",
    "# First see how the model does without SAEs\n",
    "test_prompt(prompt, answer, gpt2)\n",
    "\n",
    "# Test our prompt, to see what the model says\n",
    "with gpt2.saes(saes=[gpt2_sae]):\n",
    "    test_prompt(prompt, answer, gpt2)\n",
    "\n",
    "# Using `run_with_saes` method in place of standard forward pass\n",
    "logits = gpt2(prompt, return_type=\"logits\")\n",
    "logits_sae = gpt2.run_with_saes(prompt, saes=[gpt2_sae], return_type=\"logits\")\n",
    "answer_token_id = gpt2.to_single_token(answer)\n",
    "\n",
    "# Getting model's prediction\n",
    "top_prob, token_id_prediction = logits[0, -1].softmax(-1).max(-1)\n",
    "top_prob_sae, token_id_prediction_sae = logits_sae[0, -1].softmax(-1).max(-1)\n",
    "\n",
    "print(f\"\"\"Standard model: top prediction = {gpt2.to_string(token_id_prediction)!r}, prob = {top_prob.item():.2%}\n",
    "SAE reconstruction: top prediction = {gpt2.to_string(token_id_prediction_sae)!r}, prob = {top_prob_sae.item():.2%}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Mitigating the risk of extinction from AI should be a global\"\n",
    "answer = \" priority\"\n",
    "\n",
    "gpt2_sae.load_state_dict(t.load('pruned_gpt2_sae_wanda.pth'))\n",
    "\n",
    "# First see how the model does without SAEs\n",
    "test_prompt(prompt, answer, gpt2)\n",
    "\n",
    "# Test our prompt, to see what the model says\n",
    "with gpt2.saes(saes=[gpt2_sae]):\n",
    "    test_prompt(prompt, answer, gpt2)\n",
    "\n",
    "# Using `run_with_saes` method in place of standard forward pass\n",
    "logits = gpt2(prompt, return_type=\"logits\")\n",
    "logits_sae = gpt2.run_with_saes(prompt, saes=[gpt2_sae], return_type=\"logits\")\n",
    "answer_token_id = gpt2.to_single_token(answer)\n",
    "\n",
    "# Getting model's prediction\n",
    "top_prob, token_id_prediction = logits[0, -1].softmax(-1).max(-1)\n",
    "top_prob_sae, token_id_prediction_sae = logits_sae[0, -1].softmax(-1).max(-1)\n",
    "\n",
    "print(f\"\"\"Standard model: top prediction = {gpt2.to_string(token_id_prediction)!r}, prob = {top_prob.item():.2%}\n",
    "SAE reconstruction: top prediction = {gpt2.to_string(token_id_prediction_sae)!r}, prob = {top_prob_sae.item():.2%}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sae_lens.HookedSAETransformer.from_pretrained(\"gpt2-small\", device=device)\n",
    "hf_repo_id = \"suchitg/sae_test\"\n",
    "sae_id = 'blocks.9.attn.hook_z-base-v1'\n",
    "sae = sae_lens.SAE.from_pretrained(release=hf_repo_id, sae_id=sae_id, device=str(device))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sae.load_state_dict(t.load('pruned_gpt2_sae_wanda.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer_lens.utils import tokenize_and_concatenate\n",
    "\n",
    "dataset = load_dataset(\n",
    "    path=\"NeelNanda/pile-10k\",\n",
    "    split=\"train\",\n",
    "    streaming=False,\n",
    ")\n",
    "\n",
    "token_dataset = tokenize_and_concatenate(\n",
    "    dataset=dataset,  # type: ignore\n",
    "    tokenizer=model.tokenizer,  # type: ignore\n",
    "    streaming=True,\n",
    "    max_length=sae.cfg.context_size,\n",
    "    add_bos_token=sae.cfg.prepend_bos,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sae.eval()  # prevents error if we're expecting a dead neuron mask for who grads\n",
    "\n",
    "with t.no_grad():\n",
    "\n",
    "    # activation store can give us tokens.\n",
    "    batch_tokens = token_dataset[:8][\"tokens\"]\n",
    "    _, cache = model.run_with_cache(batch_tokens, prepend_bos=True)\n",
    "\n",
    "    # Use the SAE\n",
    "    feature_acts = sae.encode(cache[sae.cfg.hook_name])\n",
    "    sae_out = sae.decode(feature_acts)\n",
    "\n",
    "    # save some room\n",
    "    del cache\n",
    "\n",
    "    # ignore the bos token, get the number of features that activated in each token, averaged accross batch and position\n",
    "    l0 = (feature_acts[:, 1:] > 0).float().sum(-1).detach()\n",
    "    print(\"average l0\", l0.mean().item())\n",
    "    px.histogram(l0.flatten().cpu().numpy()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer_lens import utils\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "# next we want to do a reconstruction test.\n",
    "def reconstr_hook(activation, hook, sae_out):\n",
    "    return sae_out\n",
    "\n",
    "\n",
    "def zero_abl_hook(activation, hook):\n",
    "    return t.zeros_like(activation)\n",
    "\n",
    "\n",
    "print(\"Orig\", model(batch_tokens, return_type=\"loss\").item())\n",
    "print(\n",
    "    \"reconstr\",\n",
    "    model.run_with_hooks(\n",
    "        batch_tokens,\n",
    "        fwd_hooks=[\n",
    "            (\n",
    "                sae.cfg.hook_name,\n",
    "                partial(reconstr_hook, sae_out=sae_out),\n",
    "            )\n",
    "        ],\n",
    "        return_type=\"loss\",\n",
    "    ).item(),\n",
    ")\n",
    "print(\n",
    "    \"Zero\",\n",
    "    model.run_with_hooks(\n",
    "        batch_tokens,\n",
    "        return_type=\"loss\",\n",
    "        fwd_hooks=[(sae.cfg.hook_name, zero_abl_hook)],\n",
    "    ).item(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sae_lens.HookedSAETransformer.from_pretrained(\"gpt2-small\", device=device)\n",
    "model.load_state_dict(t.load('pruned/pruned_gpt2_wanda.pth'))\n",
    "\n",
    "# SAE for wanda-pruned gpt2-small\n",
    "hf_repo_id = \"suchitg/sae_wanda\"\n",
    "sae_id = 'blocks.9.attn.hook_z-v1'\n",
    "sae = sae_lens.SAE.from_pretrained(release=hf_repo_id, sae_id=sae_id, device=str(device))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sae.eval()  # prevents error if we're expecting a dead neuron mask for who grads\n",
    "\n",
    "with t.no_grad():\n",
    "\n",
    "    # activation store can give us tokens.\n",
    "    batch_tokens = token_dataset[:8][\"tokens\"]\n",
    "    _, cache = model.run_with_cache(batch_tokens, prepend_bos=True)\n",
    "\n",
    "    # Use the SAE\n",
    "    feature_acts = sae.encode(cache[sae.cfg.hook_name])\n",
    "    sae_out = sae.decode(feature_acts)\n",
    "\n",
    "    # save some room\n",
    "    del cache\n",
    "\n",
    "    # ignore the bos token, get the number of features that activated in each token, averaged accross batch and position\n",
    "    l0 = (feature_acts[:, 1:] > 0).float().sum(-1).detach()\n",
    "    print(\"average l0\", l0.mean().item())\n",
    "    px.histogram(l0.flatten().cpu().numpy()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer_lens import utils\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "# next we want to do a reconstruction test.\n",
    "def reconstr_hook(activation, hook, sae_out):\n",
    "    return sae_out\n",
    "\n",
    "\n",
    "def zero_abl_hook(activation, hook):\n",
    "    return t.zeros_like(activation)\n",
    "\n",
    "\n",
    "print(\"Orig\", model(batch_tokens, return_type=\"loss\").item())\n",
    "print(\n",
    "    \"reconstr\",\n",
    "    model.run_with_hooks(\n",
    "        batch_tokens,\n",
    "        fwd_hooks=[\n",
    "            (\n",
    "                sae.cfg.hook_name,\n",
    "                partial(reconstr_hook, sae_out=sae_out),\n",
    "            )\n",
    "        ],\n",
    "        return_type=\"loss\",\n",
    "    ).item(),\n",
    ")\n",
    "print(\n",
    "    \"Zero\",\n",
    "    model.run_with_hooks(\n",
    "        batch_tokens,\n",
    "        return_type=\"loss\",\n",
    "        fwd_hooks=[(sae.cfg.hook_name, zero_abl_hook)],\n",
    "    ).item(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2_wanda = sae_lens.HookedSAETransformer.from_pretrained(\"gpt2-small\", device=device)\n",
    "gpt2_wanda.load_state_dict(t.load('pruned/pruned_gpt2_wanda.pth'))\n",
    "\n",
    "# SAE for wanda-pruned gpt2-small\n",
    "hf_repo_id = \"suchitg/sae_wanda\"\n",
    "sae_id = 'blocks.9.attn.hook_z-v1'\n",
    "gpt2_wanda_sae = sae_lens.SAE.from_pretrained(release=hf_repo_id, sae_id=sae_id, device=str(device))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Mitigating the risk of extinction from AI should be a global\"\n",
    "answer = \" priority\"\n",
    "\n",
    "\n",
    "# First see how the model does without SAEs\n",
    "test_prompt(prompt, answer, gpt2_wanda)\n",
    "\n",
    "# Test our prompt, to see what the model says\n",
    "with gpt2_wanda.saes(saes=[gpt2_wanda_sae]):\n",
    "    test_prompt(prompt, answer, gpt2_wanda)\n",
    "\n",
    "# Using `run_with_saes` method in place of standard forward pass\n",
    "logits = gpt2_wanda(prompt, return_type=\"logits\")\n",
    "logits_sae = gpt2_wanda.run_with_saes(prompt, saes=[gpt2_wanda_sae], return_type=\"logits\")\n",
    "answer_token_id = gpt2_wanda.to_single_token(answer)\n",
    "\n",
    "# Getting model's prediction\n",
    "top_prob, token_id_prediction = logits[0, -1].softmax(-1).max(-1)\n",
    "top_prob_sae, token_id_prediction_sae = logits_sae[0, -1].softmax(-1).max(-1)\n",
    "\n",
    "print(f\"\"\"Standard model: top prediction = {gpt2_wanda.to_string(token_id_prediction)!r}, prob = {top_prob.item():.2%}\n",
    "SAE reconstruction: top prediction = {gpt2_wanda.to_string(token_id_prediction_sae)!r}, prob = {top_prob_sae.item():.2%}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Q. Can I predict either SAES based on the other?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gemma-2-2b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "from datasets import load_dataset\n",
    "import sae_lens\n",
    "import transformer_lens\n",
    "from sae_lens import (\n",
    "    SAE,\n",
    "    HookedSAETransformer,\n",
    ")\n",
    "from transformer_lens.utils import get_act_name, test_prompt, to_numpy\n",
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "model = HookedSAETransformer.from_pretrained(\"gemma-2-2b\")\n",
    "model.load_state_dict(t.load('/home/gupte.31/COLM/sae-compression/gemma2b/pruned/gemma-2-2b_wanda.pth'))\n",
    "\n",
    "t.set_grad_enabled(False)\n",
    "\n",
    "# next we want to do a reconstruction test.\n",
    "def reconstr_hook(activation, hook, sae_out):\n",
    "    return sae_out\n",
    "\n",
    "dataset = load_dataset(\n",
    "    path=\"NeelNanda/pile-10k\",\n",
    "    split=\"train\",\n",
    "    streaming=False,\n",
    ")\n",
    "\n",
    "from transformer_lens.utils import tokenize_and_concatenate\n",
    "\n",
    "token_dataset = tokenize_and_concatenate(\n",
    "    dataset=dataset,  # type: ignore\n",
    "    tokenizer=model.tokenizer,  # type: ignore\n",
    "    streaming=True,\n",
    "    max_length=1024,\n",
    "    add_bos_token=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sae_lens import SAE\n",
    "\n",
    "release = \"gemma-scope-2b-pt-res-canonical\"\n",
    "sae_id = \"layer_12/width_16k/canonical\"\n",
    "sae = SAE.from_pretrained(release, sae_id, device=\"cuda\")[0]\n",
    "\n",
    "prompt = \"Mitigating the risk of extinction from AI should be a global\"\n",
    "answer = \" priority\"\n",
    "\n",
    "# # First see how the model does without SAEs\n",
    "# test_prompt(prompt, answer, model)\n",
    "\n",
    "# # Test our prompt, to see what the model says\n",
    "# with model.saes(saes=[sae]):\n",
    "#     test_prompt(prompt, answer, model)\n",
    "\n",
    "# Using `run_with_saes` method in place of standard forward pass\n",
    "logits = model(prompt, return_type=\"logits\")\n",
    "logits_sae = model.run_with_saes(prompt, saes=[sae], return_type=\"logits\")\n",
    "answer_token_id = model.to_single_token(answer)\n",
    "\n",
    "# Getting model's prediction\n",
    "top_prob, token_id_prediction = logits[0, -1].softmax(-1).max(-1)\n",
    "top_prob_sae, token_id_prediction_sae = logits_sae[0, -1].softmax(-1).max(-1)\n",
    "\n",
    "print(f\"\"\"Standard model: top prediction = {model.to_string(token_id_prediction)!r}, prob = {top_prob.item():.2%}\n",
    "SAE reconstruction: top prediction = {model.to_string(token_id_prediction_sae)!r}, prob = {top_prob_sae.item():.2%}\n",
    "\"\"\")\n",
    "\n",
    "del logits, logits_sae, token_id_prediction, token_id_prediction_sae, top_prob, top_prob_sae\n",
    "t.cuda.empty_cache()\n",
    "\n",
    "# IOI task\n",
    "prompt = \"After John and Mary went to the store, John gave a bottle of milk to\"\n",
    "answer = \" Mary\"\n",
    "\n",
    "# # First see how the model does without SAEs\n",
    "# test_prompt(prompt, answer, model)\n",
    "\n",
    "# # Test our prompt, to see what the model says\n",
    "# with model.saes(saes=[sae]):\n",
    "#     test_prompt(prompt, answer, model)\n",
    "\n",
    "# Using `run_with_saes` method in place of standard forward pass\n",
    "logits = model(prompt, return_type=\"logits\")\n",
    "logits_sae = model.run_with_saes(prompt, saes=[sae], return_type=\"logits\")\n",
    "answer_token_id = model.to_single_token(answer)\n",
    "\n",
    "# Getting model's prediction\n",
    "top_prob, token_id_prediction = logits[0, -1].softmax(-1).max(-1)\n",
    "top_prob_sae, token_id_prediction_sae = logits_sae[0, -1].softmax(-1).max(-1)\n",
    "\n",
    "print(f\"\"\"Standard model: top prediction = {model.to_string(token_id_prediction)!r}, prob = {top_prob.item():.2%}\n",
    "SAE reconstruction: top prediction = {model.to_string(token_id_prediction_sae)!r}, prob = {top_prob_sae.item():.2%}\n",
    "\"\"\")\n",
    "\n",
    "del sae, logits, logits_sae, token_id_prediction, token_id_prediction_sae, top_prob, top_prob_sae\n",
    "t.cuda.empty_cache()\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "sae = SAE.from_pretrained(release, sae_id, device=\"cuda\")[0]\n",
    "\n",
    "sae.eval()  # prevents error if we're expecting a dead neuron mask for who grads\n",
    "with t.no_grad():\n",
    "\n",
    "    # activation store can give us tokens.\n",
    "    batch_tokens = token_dataset[:2][\"tokens\"]\n",
    "    _, cache = model.run_with_cache(batch_tokens, prepend_bos=True)\n",
    "\n",
    "    # Use the SAE\n",
    "    feature_acts = sae.encode(cache[sae.cfg.hook_name])\n",
    "    sae_out = sae.decode(feature_acts)\n",
    "\n",
    "    # save some room\n",
    "    del cache\n",
    "\n",
    "    print(\n",
    "    \"Reconstuction loss:\",\n",
    "    model.run_with_hooks(\n",
    "        batch_tokens,\n",
    "        fwd_hooks=[\n",
    "            (\n",
    "                sae.cfg.hook_name,\n",
    "                partial(reconstr_hook, sae_out=sae_out),\n",
    "            )\n",
    "        ],\n",
    "        return_type=\"loss\",\n",
    "    ).item(),\n",
    "    )\n",
    "\n",
    "del sae, sae_out, feature_acts, batch_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "release = \"suchitg/sae-compression-gemma-2-2b\"\n",
    "sae_id = \"blocks.12.hook_resid_post\"\n",
    "sae = SAE.from_pretrained(release, sae_id, device=\"cuda\")[0]\n",
    "\n",
    "\n",
    "prompt = \"Mitigating the risk of extinction from AI should be a global\"\n",
    "answer = \" priority\"\n",
    "\n",
    "# # First see how the model does without SAEs\n",
    "# test_prompt(prompt, answer, model)\n",
    "\n",
    "# # Test our prompt, to see what the model says\n",
    "# with model.saes(saes=[sae]):\n",
    "#     test_prompt(prompt, answer, model)\n",
    "\n",
    "# Using `run_with_saes` method in place of standard forward pass\n",
    "logits = model(prompt, return_type=\"logits\")\n",
    "logits_sae = model.run_with_saes(prompt, saes=[sae], return_type=\"logits\")\n",
    "answer_token_id = model.to_single_token(answer)\n",
    "\n",
    "# Getting model's prediction\n",
    "top_prob, token_id_prediction = logits[0, -1].softmax(-1).max(-1)\n",
    "top_prob_sae, token_id_prediction_sae = logits_sae[0, -1].softmax(-1).max(-1)\n",
    "\n",
    "print(f\"\"\"Standard model: top prediction = {model.to_string(token_id_prediction)!r}, prob = {top_prob.item():.2%}\n",
    "SAE reconstruction: top prediction = {model.to_string(token_id_prediction_sae)!r}, prob = {top_prob_sae.item():.2%}\n",
    "\"\"\")\n",
    "\n",
    "del logits, logits_sae, token_id_prediction, token_id_prediction_sae, top_prob, top_prob_sae\n",
    "\n",
    "\n",
    "# IOI task\n",
    "prompt = \"After John and Mary went to the store, John gave a bottle of milk to\"\n",
    "answer = \" Mary\"\n",
    "\n",
    "# # First see how the model does without SAEs\n",
    "# test_prompt(prompt, answer, model)\n",
    "\n",
    "# # Test our prompt, to see what the model says\n",
    "# with model.saes(saes=[sae]):\n",
    "#     test_prompt(prompt, answer, model)\n",
    "\n",
    "# Using `run_with_saes` method in place of standard forward pass\n",
    "logits = model(prompt, return_type=\"logits\")\n",
    "logits_sae = model.run_with_saes(prompt, saes=[sae], return_type=\"logits\")\n",
    "answer_token_id = model.to_single_token(answer)\n",
    "\n",
    "# Getting model's prediction\n",
    "top_prob, token_id_prediction = logits[0, -1].softmax(-1).max(-1)\n",
    "top_prob_sae, token_id_prediction_sae = logits_sae[0, -1].softmax(-1).max(-1)\n",
    "\n",
    "print(f\"\"\"Standard model: top prediction = {model.to_string(token_id_prediction)!r}, prob = {top_prob.item():.2%}\n",
    "SAE reconstruction: top prediction = {model.to_string(token_id_prediction_sae)!r}, prob = {top_prob_sae.item():.2%}\n",
    "\"\"\")\n",
    "del sae, logits, logits_sae, token_id_prediction, token_id_prediction_sae, top_prob, top_prob_sae\n",
    "t.cuda.empty_cache()\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "sae = SAE.from_pretrained(release, sae_id, device=\"cuda\")[0]\n",
    "\n",
    "sae.eval()  # prevents error if we're expecting a dead neuron mask for who grads\n",
    "with t.no_grad():\n",
    "\n",
    "    # activation store can give us tokens.\n",
    "    batch_tokens = token_dataset[:2][\"tokens\"]\n",
    "    _, cache = model.run_with_cache(batch_tokens, prepend_bos=True)\n",
    "\n",
    "    # Use the SAE\n",
    "    feature_acts = sae.encode(cache[sae.cfg.hook_name])\n",
    "    sae_out = sae.decode(feature_acts)\n",
    "\n",
    "    # save some room\n",
    "    del cache\n",
    "\n",
    "    print(\n",
    "    \"Reconstuction loss:\",\n",
    "    model.run_with_hooks(\n",
    "        batch_tokens,\n",
    "        fwd_hooks=[\n",
    "            (\n",
    "                sae.cfg.hook_name,\n",
    "                partial(reconstr_hook, sae_out=sae_out),\n",
    "            )\n",
    "        ],\n",
    "        return_type=\"loss\",\n",
    "    ).item(),\n",
    "    )\n",
    "\n",
    "del sae, sae_out, feature_acts, batch_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sae_lens import SAE\n",
    "\n",
    "release = \"gemma-scope-2b-pt-res-canonical\"\n",
    "sae_id = \"layer_12/width_16k/canonical\"\n",
    "sae = SAE.from_pretrained(release, sae_id, device=\"cuda\")[0]\n",
    "sae.load_state_dict(t.load('/local/scratch/suchit/COLM/pruned_saes/gemma-2-2b/wanda/pile/hook_resid_post/blocks.12.hook_resid_post.pth'))\n",
    "\n",
    "prompt = \"Mitigating the risk of extinction from AI should be a global\"\n",
    "answer = \" priority\"\n",
    "\n",
    "# # First see how the model does without SAEs\n",
    "# test_prompt(prompt, answer, model)\n",
    "\n",
    "# # Test our prompt, to see what the model says\n",
    "# with model.saes(saes=[sae]):\n",
    "#     test_prompt(prompt, answer, model)\n",
    "\n",
    "# Using `run_with_saes` method in place of standard forward pass\n",
    "logits = model(prompt, return_type=\"logits\")\n",
    "logits_sae = model.run_with_saes(prompt, saes=[sae], return_type=\"logits\")\n",
    "answer_token_id = model.to_single_token(answer)\n",
    "\n",
    "# Getting model's prediction\n",
    "top_prob, token_id_prediction = logits[0, -1].softmax(-1).max(-1)\n",
    "top_prob_sae, token_id_prediction_sae = logits_sae[0, -1].softmax(-1).max(-1)\n",
    "\n",
    "print(f\"\"\"Standard model: top prediction = {model.to_string(token_id_prediction)!r}, prob = {top_prob.item():.2%}\n",
    "SAE reconstruction: top prediction = {model.to_string(token_id_prediction_sae)!r}, prob = {top_prob_sae.item():.2%}\n",
    "\"\"\")\n",
    "\n",
    "del logits, logits_sae, token_id_prediction, token_id_prediction_sae, top_prob, top_prob_sae\n",
    "t.cuda.empty_cache()\n",
    "\n",
    "# IOI task\n",
    "prompt = \"After John and Mary went to the store, John gave a bottle of milk to\"\n",
    "answer = \" Mary\"\n",
    "\n",
    "# # First see how the model does without SAEs\n",
    "# test_prompt(prompt, answer, model)\n",
    "\n",
    "# # Test our prompt, to see what the model says\n",
    "# with model.saes(saes=[sae]):\n",
    "#     test_prompt(prompt, answer, model)\n",
    "\n",
    "# Using `run_with_saes` method in place of standard forward pass\n",
    "logits = model(prompt, return_type=\"logits\")\n",
    "logits_sae = model.run_with_saes(prompt, saes=[sae], return_type=\"logits\")\n",
    "answer_token_id = model.to_single_token(answer)\n",
    "\n",
    "# Getting model's prediction\n",
    "top_prob, token_id_prediction = logits[0, -1].softmax(-1).max(-1)\n",
    "top_prob_sae, token_id_prediction_sae = logits_sae[0, -1].softmax(-1).max(-1)\n",
    "\n",
    "print(f\"\"\"Standard model: top prediction = {model.to_string(token_id_prediction)!r}, prob = {top_prob.item():.2%}\n",
    "SAE reconstruction: top prediction = {model.to_string(token_id_prediction_sae)!r}, prob = {top_prob_sae.item():.2%}\n",
    "\"\"\")\n",
    "\n",
    "del sae, logits, logits_sae, token_id_prediction, token_id_prediction_sae, top_prob, top_prob_sae\n",
    "t.cuda.empty_cache()\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "sae = SAE.from_pretrained(release, sae_id, device=\"cuda\")[0]\n",
    "sae.load_state_dict(t.load('/local/scratch/suchit/COLM/pruned_saes/gemma-2-2b/wanda/pile/hook_resid_post/blocks.12.hook_resid_post.pth'))\n",
    "\n",
    "\n",
    "sae.eval()  # prevents error if we're expecting a dead neuron mask for who grads\n",
    "with t.no_grad():\n",
    "\n",
    "    # activation store can give us tokens.\n",
    "    batch_tokens = token_dataset[:2][\"tokens\"]\n",
    "    _, cache = model.run_with_cache(batch_tokens, prepend_bos=True)\n",
    "\n",
    "    # Use the SAE\n",
    "    feature_acts = sae.encode(cache[sae.cfg.hook_name])\n",
    "    sae_out = sae.decode(feature_acts)\n",
    "\n",
    "    # save some room\n",
    "    del cache\n",
    "\n",
    "    print(\n",
    "    \"Reconstuction loss:\",\n",
    "    model.run_with_hooks(\n",
    "        batch_tokens,\n",
    "        fwd_hooks=[\n",
    "            (\n",
    "                sae.cfg.hook_name,\n",
    "                partial(reconstr_hook, sae_out=sae_out),\n",
    "            )\n",
    "        ],\n",
    "        return_type=\"loss\",\n",
    "    ).item(),\n",
    "    )\n",
    "\n",
    "del sae, sae_out, feature_acts, batch_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sae_lens import SAE\n",
    "\n",
    "release = \"gemma-scope-2b-pt-res-canonical\"\n",
    "sae_id = \"layer_12/width_16k/canonical\"\n",
    "sae = SAE.from_pretrained(release, sae_id, device=\"cuda\")[0]\n",
    "sae.load_state_dict(t.load('/local/scratch/suchit/COLM/pruned_saes/gemma-2-2b/wanda/pile/hook_resid_post_ratio=0.5/blocks.12.hook_resid_post.pth'))\n",
    "\n",
    "prompt = \"Mitigating the risk of extinction from AI should be a global\"\n",
    "answer = \" priority\"\n",
    "\n",
    "# # First see how the model does without SAEs\n",
    "# test_prompt(prompt, answer, model)\n",
    "\n",
    "# # Test our prompt, to see what the model says\n",
    "# with model.saes(saes=[sae]):\n",
    "#     test_prompt(prompt, answer, model)\n",
    "\n",
    "# Using `run_with_saes` method in place of standard forward pass\n",
    "logits = model(prompt, return_type=\"logits\")\n",
    "logits_sae = model.run_with_saes(prompt, saes=[sae], return_type=\"logits\")\n",
    "answer_token_id = model.to_single_token(answer)\n",
    "\n",
    "# Getting model's prediction\n",
    "top_prob, token_id_prediction = logits[0, -1].softmax(-1).max(-1)\n",
    "top_prob_sae, token_id_prediction_sae = logits_sae[0, -1].softmax(-1).max(-1)\n",
    "\n",
    "print(f\"\"\"Standard model: top prediction = {model.to_string(token_id_prediction)!r}, prob = {top_prob.item():.2%}\n",
    "SAE reconstruction: top prediction = {model.to_string(token_id_prediction_sae)!r}, prob = {top_prob_sae.item():.2%}\n",
    "\"\"\")\n",
    "\n",
    "del logits, logits_sae, token_id_prediction, token_id_prediction_sae, top_prob, top_prob_sae\n",
    "t.cuda.empty_cache()\n",
    "\n",
    "# IOI task\n",
    "prompt = \"After John and Mary went to the store, John gave a bottle of milk to\"\n",
    "answer = \" Mary\"\n",
    "\n",
    "# # First see how the model does without SAEs\n",
    "# test_prompt(prompt, answer, model)\n",
    "\n",
    "# # Test our prompt, to see what the model says\n",
    "# with model.saes(saes=[sae]):\n",
    "#     test_prompt(prompt, answer, model)\n",
    "\n",
    "# Using `run_with_saes` method in place of standard forward pass\n",
    "logits = model(prompt, return_type=\"logits\")\n",
    "logits_sae = model.run_with_saes(prompt, saes=[sae], return_type=\"logits\")\n",
    "answer_token_id = model.to_single_token(answer)\n",
    "\n",
    "# Getting model's prediction\n",
    "top_prob, token_id_prediction = logits[0, -1].softmax(-1).max(-1)\n",
    "top_prob_sae, token_id_prediction_sae = logits_sae[0, -1].softmax(-1).max(-1)\n",
    "\n",
    "print(f\"\"\"Standard model: top prediction = {model.to_string(token_id_prediction)!r}, prob = {top_prob.item():.2%}\n",
    "SAE reconstruction: top prediction = {model.to_string(token_id_prediction_sae)!r}, prob = {top_prob_sae.item():.2%}\n",
    "\"\"\")\n",
    "\n",
    "del sae, logits, logits_sae, token_id_prediction, token_id_prediction_sae, top_prob, top_prob_sae\n",
    "t.cuda.empty_cache()\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "sae = SAE.from_pretrained(release, sae_id, device=\"cuda\")[0]\n",
    "sae.load_state_dict(t.load('/local/scratch/suchit/COLM/pruned_saes/gemma-2-2b/wanda/pile/hook_resid_post_ratio=0.5/blocks.12.hook_resid_post.pth'))\n",
    "\n",
    "\n",
    "\n",
    "sae.eval()  # prevents error if we're expecting a dead neuron mask for who grads\n",
    "with t.no_grad():\n",
    "\n",
    "    # activation store can give us tokens.\n",
    "    batch_tokens = token_dataset[:2][\"tokens\"]\n",
    "    _, cache = model.run_with_cache(batch_tokens, prepend_bos=True)\n",
    "\n",
    "    # Use the SAE\n",
    "    feature_acts = sae.encode(cache[sae.cfg.hook_name])\n",
    "    sae_out = sae.decode(feature_acts)\n",
    "\n",
    "    # save some room\n",
    "    del cache\n",
    "\n",
    "    print(\n",
    "    \"Reconstuction loss:\",\n",
    "    model.run_with_hooks(\n",
    "        batch_tokens,\n",
    "        fwd_hooks=[\n",
    "            (\n",
    "                sae.cfg.hook_name,\n",
    "                partial(reconstr_hook, sae_out=sae_out),\n",
    "            )\n",
    "        ],\n",
    "        return_type=\"loss\",\n",
    "    ).item(),\n",
    "    )\n",
    "\n",
    "del sae, sae_out, feature_acts, batch_tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8a1e23640ea4df8ada74546608eecc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "model_id = \"google/gemma-2-2b\"\n",
    "model_id = \"meta-llama/Llama-3.1-8B\"\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# Define quantization config\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,                   # Set to True for 4-bit quantization\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",           # Options: \"fp4\" or \"nf4\"\n",
    "    bnb_4bit_compute_dtype=\"float16\"     # Can also be bfloat16 or float32\n",
    ")\n",
    "\n",
    "\n",
    "# Load quantized model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"cuda:5\",             # Automatically selects GPU\n",
    "    quantization_config=quant_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "036d9a3ec32840919fd7694a8b282d5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4096, 4096]) torch.Size([1024, 4096]) torch.Size([1024, 4096])\n",
      "torch.Size([32, 4096, 128]) torch.Size([8, 4096, 128]) torch.Size([8, 4096, 128])\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'exit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformer_lens\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m HookedTransformer\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# hookedmodel = HookedTransformer.from_pretrained(\"gemma-2-2b\", device=\"cuda:5\", dtype=\"float16\")\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m hookedmodel \u001b[38;5;241m=\u001b[39m \u001b[43mHookedTransformer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda:5\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfloat16\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/SAE/lib/python3.10/site-packages/transformer_lens/HookedTransformer.py:1325\u001b[0m, in \u001b[0;36mHookedTransformer.from_pretrained\u001b[0;34m(cls, model_name, fold_ln, center_writing_weights, center_unembed, refactor_factored_attn_matrices, checkpoint_index, checkpoint_value, hf_model, device, n_devices, tokenizer, move_to_device, fold_value_biases, default_prepend_bos, default_padding_side, dtype, first_n_layers, **from_pretrained_kwargs)\u001b[0m\n\u001b[1;32m   1321\u001b[0m     center_unembed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1323\u001b[0m \u001b[38;5;66;03m# Get the state dict of the model (ie a mapping of parameter names to tensors), processed to\u001b[39;00m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;66;03m# match the HookedTransformer parameter names.\u001b[39;00m\n\u001b[0;32m-> 1325\u001b[0m state_dict \u001b[38;5;241m=\u001b[39m \u001b[43mloading\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_pretrained_state_dict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1326\u001b[0m \u001b[43m    \u001b[49m\u001b[43mofficial_model_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhf_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfrom_pretrained_kwargs\u001b[49m\n\u001b[1;32m   1327\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1329\u001b[0m \u001b[38;5;66;03m# Create the HookedTransformer object\u001b[39;00m\n\u001b[1;32m   1330\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m(\n\u001b[1;32m   1331\u001b[0m     cfg,\n\u001b[1;32m   1332\u001b[0m     tokenizer,\n\u001b[1;32m   1333\u001b[0m     move_to_device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1334\u001b[0m     default_padding_side\u001b[38;5;241m=\u001b[39mdefault_padding_side,\n\u001b[1;32m   1335\u001b[0m )\n",
      "File \u001b[0;32m~/.conda/envs/SAE/lib/python3.10/site-packages/transformer_lens/loading_from_pretrained.py:1861\u001b[0m, in \u001b[0;36mget_pretrained_state_dict\u001b[0;34m(official_model_name, cfg, hf_model, dtype, **kwargs)\u001b[0m\n\u001b[1;32m   1859\u001b[0m     state_dict \u001b[38;5;241m=\u001b[39m convert_neox_weights(hf_model, cfg)\n\u001b[1;32m   1860\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m cfg\u001b[38;5;241m.\u001b[39moriginal_architecture \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLlamaForCausalLM\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 1861\u001b[0m     state_dict \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_llama_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhf_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1862\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m cfg\u001b[38;5;241m.\u001b[39moriginal_architecture \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBertForMaskedLM\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1863\u001b[0m     state_dict \u001b[38;5;241m=\u001b[39m convert_bert_weights(hf_model, cfg)\n",
      "File \u001b[0;32m~/.conda/envs/SAE/lib/python3.10/site-packages/transformer_lens/pretrained/weight_conversions/llama.py:41\u001b[0m, in \u001b[0;36mconvert_llama_weights\u001b[0;34m(llama, cfg)\u001b[0m\n\u001b[1;32m     39\u001b[0m     W_V \u001b[38;5;241m=\u001b[39m einops\u001b[38;5;241m.\u001b[39mrearrange(W_V, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(n h) m->n m h\u001b[39m\u001b[38;5;124m\"\u001b[39m, n\u001b[38;5;241m=\u001b[39mn_kv_heads)\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28mprint\u001b[39m(W_Q\u001b[38;5;241m.\u001b[39mshape, W_K\u001b[38;5;241m.\u001b[39mshape, W_V\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 41\u001b[0m     \u001b[43mexit\u001b[49m()\n\u001b[1;32m     43\u001b[0m state_dict[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblocks.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00ml\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.attn.W_Q\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m W_Q\n\u001b[1;32m     44\u001b[0m state_dict[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblocks.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00ml\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.attn.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgqa_uscore\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mW_K\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m W_K\n",
      "\u001b[0;31mNameError\u001b[0m: name 'exit' is not defined"
     ]
    }
   ],
   "source": [
    "from transformer_lens import HookedTransformer\n",
    "\n",
    "# hookedmodel = HookedTransformer.from_pretrained(\"gemma-2-2b\", device=\"cuda:5\", dtype=\"float16\")\n",
    "hookedmodel = HookedTransformer.from_pretrained(model_id, device=\"cuda:5\", dtype=\"float16\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta-llama/llama-3.1-8b True False\n",
      "torch.Size([8388608, 1]) torch.Size([2097152, 1]) torch.Size([2097152, 1])\n",
      "torch.Size([8388608, 1]) torch.Size([2097152, 1]) torch.Size([2097152, 1])\n",
      "torch.Size([8388608, 1]) torch.Size([2097152, 1]) torch.Size([2097152, 1])\n",
      "torch.Size([8388608, 1]) torch.Size([2097152, 1]) torch.Size([2097152, 1])\n",
      "torch.Size([8388608, 1]) torch.Size([2097152, 1]) torch.Size([2097152, 1])\n",
      "torch.Size([8388608, 1]) torch.Size([2097152, 1]) torch.Size([2097152, 1])\n",
      "torch.Size([8388608, 1]) torch.Size([2097152, 1]) torch.Size([2097152, 1])\n",
      "torch.Size([8388608, 1]) torch.Size([2097152, 1]) torch.Size([2097152, 1])\n",
      "torch.Size([8388608, 1]) torch.Size([2097152, 1]) torch.Size([2097152, 1])\n",
      "torch.Size([8388608, 1]) torch.Size([2097152, 1]) torch.Size([2097152, 1])\n",
      "torch.Size([8388608, 1]) torch.Size([2097152, 1]) torch.Size([2097152, 1])\n",
      "torch.Size([8388608, 1]) torch.Size([2097152, 1]) torch.Size([2097152, 1])\n",
      "torch.Size([8388608, 1]) torch.Size([2097152, 1]) torch.Size([2097152, 1])\n",
      "torch.Size([8388608, 1]) torch.Size([2097152, 1]) torch.Size([2097152, 1])\n",
      "torch.Size([8388608, 1]) torch.Size([2097152, 1]) torch.Size([2097152, 1])\n",
      "torch.Size([8388608, 1]) torch.Size([2097152, 1]) torch.Size([2097152, 1])\n",
      "torch.Size([8388608, 1]) torch.Size([2097152, 1]) torch.Size([2097152, 1])\n",
      "torch.Size([8388608, 1]) torch.Size([2097152, 1]) torch.Size([2097152, 1])\n",
      "torch.Size([8388608, 1]) torch.Size([2097152, 1]) torch.Size([2097152, 1])\n",
      "torch.Size([8388608, 1]) torch.Size([2097152, 1]) torch.Size([2097152, 1])\n",
      "torch.Size([8388608, 1]) torch.Size([2097152, 1]) torch.Size([2097152, 1])\n",
      "torch.Size([8388608, 1]) torch.Size([2097152, 1]) torch.Size([2097152, 1])\n",
      "torch.Size([8388608, 1]) torch.Size([2097152, 1]) torch.Size([2097152, 1])\n",
      "torch.Size([8388608, 1]) torch.Size([2097152, 1]) torch.Size([2097152, 1])\n",
      "torch.Size([8388608, 1]) torch.Size([2097152, 1]) torch.Size([2097152, 1])\n",
      "torch.Size([8388608, 1]) torch.Size([2097152, 1]) torch.Size([2097152, 1])\n",
      "torch.Size([8388608, 1]) torch.Size([2097152, 1]) torch.Size([2097152, 1])\n",
      "torch.Size([8388608, 1]) torch.Size([2097152, 1]) torch.Size([2097152, 1])\n",
      "torch.Size([8388608, 1]) torch.Size([2097152, 1]) torch.Size([2097152, 1])\n",
      "torch.Size([8388608, 1]) torch.Size([2097152, 1]) torch.Size([2097152, 1])\n",
      "torch.Size([8388608, 1]) torch.Size([2097152, 1]) torch.Size([2097152, 1])\n",
      "torch.Size([8388608, 1]) torch.Size([2097152, 1]) torch.Size([2097152, 1])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for HookedTransformer:\n\tsize mismatch for blocks.0.attn._W_K: copying a param with shape torch.Size([2097152, 1]) from checkpoint, the shape in current model is torch.Size([8, 4096, 128]).\n\tsize mismatch for blocks.0.attn._W_V: copying a param with shape torch.Size([2097152, 1]) from checkpoint, the shape in current model is torch.Size([8, 4096, 128]).\n\tsize mismatch for blocks.1.attn._W_K: copying a param with shape torch.Size([2097152, 1]) from checkpoint, the shape in current model is torch.Size([8, 4096, 128]).\n\tsize mismatch for blocks.1.attn._W_V: copying a param with shape torch.Size([2097152, 1]) from checkpoint, the shape in current model is torch.Size([8, 4096, 128]).\n\tsize mismatch for blocks.2.attn._W_K: copying a param with shape torch.Size([2097152, 1]) from checkpoint, the shape in current model is torch.Size([8, 4096, 128]).\n\tsize mismatch for blocks.2.attn._W_V: copying a param with shape torch.Size([2097152, 1]) from checkpoint, the shape in current model is torch.Size([8, 4096, 128]).\n\tsize mismatch for blocks.3.attn._W_K: copying a param with shape torch.Size([2097152, 1]) from checkpoint, the shape in current model is torch.Size([8, 4096, 128]).\n\tsize mismatch for blocks.3.attn._W_V: copying a param with shape torch.Size([2097152, 1]) from checkpoint, the shape in current model is torch.Size([8, 4096, 128]).\n\tsize mismatch for blocks.4.attn._W_K: copying a param with shape torch.Size([2097152, 1]) from checkpoint, the shape in current model is torch.Size([8, 4096, 128]).\n\tsize mismatch for blocks.4.attn._W_V: copying a param with shape torch.Size([2097152, 1]) from checkpoint, the shape in current model is torch.Size([8, 4096, 128]).\n\tsize mismatch for blocks.5.attn._W_K: copying a param with shape torch.Size([2097152, 1]) from checkpoint, the shape in current model is torch.Size([8, 4096, 128]).\n\tsize mismatch for blocks.5.attn._W_V: copying a param with shape torch.Size([2097152, 1]) from checkpoint, the shape in current model is torch.Size([8, 4096, 128]).\n\tsize mismatch for blocks.6.attn._W_K: copying a param with shape torch.Size([2097152, 1]) from checkpoint, the shape in current model is torch.Size([8, 4096, 128]).\n\tsize mismatch for blocks.6.attn._W_V: copying a param with shape torch.Size([2097152, 1]) from checkpoint, the shape in current model is torch.Size([8, 4096, 128]).\n\tsize mismatch for blocks.7.attn._W_K: copying a param with shape torch.Size([2097152, 1]) from checkpoint, the shape in current model is torch.Size([8, 4096, 128]).\n\tsize mismatch for blocks.7.attn._W_V: copying a param with shape torch.Size([2097152, 1]) from checkpoint, the shape in current model is torch.Size([8, 4096, 128]).\n\tsize mismatch for blocks.8.attn._W_K: copying a param with shape torch.Size([2097152, 1]) from checkpoint, the shape in current model is torch.Size([8, 4096, 128]).\n\tsize mismatch for blocks.8.attn._W_V: copying a param with shape torch.Size([2097152, 1]) from checkpoint, the shape in current model is torch.Size([8, 4096, 128]).\n\tsize mismatch for blocks.9.attn._W_K: copying a param with shape torch.Size([2097152, 1]) from checkpoint, the shape in current model is torch.Size([8, 4096, 128]).\n\tsize mismatch for blocks.9.attn._W_V: copying a param with shape torch.Size([2097152, 1]) from checkpoint, the shape in current model is torch.Size([8, 4096, 128]).\n\tsize mismatch for blocks.10.attn._W_K: copying a param with shape torch.Size([2097152, 1]) from checkpoint, the shape in current model is torch.Size([8, 4096, 128]).\n\tsize mismatch for blocks.10.attn._W_V: copying a param with shape torch.Size([2097152, 1]) from checkpoint, the shape in current model is torch.Size([8, 4096, 128]).\n\tsize mismatch for blocks.11.attn._W_K: copying a param with shape torch.Size([2097152, 1]) from checkpoint, the shape in current model is torch.Size([8, 4096, 128]).\n\tsize mismatch for blocks.11.attn._W_V: copying a param with shape torch.Size([2097152, 1]) from checkpoint, the shape in current model is torch.Size([8, 4096, 128]).\n\tsize mismatch for blocks.12.attn._W_K: copying a param with shape torch.Size([2097152, 1]) from checkpoint, the shape in current model is torch.Size([8, 4096, 128]).\n\tsize mismatch for blocks.12.attn._W_V: copying a param with shape torch.Size([2097152, 1]) from checkpoint, the shape in current model is torch.Size([8, 4096, 128]).\n\tsize mismatch for blocks.13.attn._W_K: copying a param with shape torch.Size([2097152, 1]) from checkpoint, the shape in current model is torch.Size([8, 4096, 128]).\n\tsize mismatch for blocks.13.attn._W_V: copying a param with shape torch.Size([2097152, 1]) from checkpoint, the shape in current model is torch.Size([8, 4096, 128]).\n\tsize mismatch for blocks.14.attn._W_K: copying a param with shape torch.Size([2097152, 1]) from checkpoint, the shape in current model is torch.Size([8, 4096, 128]).\n\tsize mismatch for blocks.14.attn._W_V: copying a param with shape torch.Size([2097152, 1]) from checkpoint, the shape in current model is torch.Size([8, 4096, 128]).\n\tsize mismatch for blocks.15.attn._W_K: copying a param with shape torch.Size([2097152, 1]) from checkpoint, the shape in current model is torch.Size([8, 4096, 128]).\n\tsize mismatch for blocks.15.attn._W_V: copying a param with shape torch.Size([2097152, 1]) from checkpoint, the shape in current model is torch.Size([8, 4096, 128]).\n\tsize mismatch for blocks.16.attn._W_K: copying a param with shape torch.Size([2097152, 1]) from checkpoint, the shape in current model is torch.Size([8, 4096, 128]).\n\tsize mismatch for blocks.16.attn._W_V: copying a param with shape torch.Size([2097152, 1]) from checkpoint, the shape in current model is torch.Size([8, 4096, 128]).\n\tsize mismatch for blocks.17.attn._W_K: copying a param with shape torch.Size([2097152, 1]) from checkpoint, the shape in current model is torch.Size([8, 4096, 128]).\n\tsize mismatch for blocks.17.attn._W_V: copying a param with shape torch.Size([2097152, 1]) from checkpoint, the shape in current model is torch.Size([8, 4096, 128]).\n\tsize mismatch for blocks.18.attn._W_K: copying a param with shape torch.Size([2097152, 1]) from checkpoint, the shape in current model is torch.Size([8, 4096, 128]).\n\tsize mismatch for blocks.18.attn._W_V: copying a param with shape torch.Size([2097152, 1]) from checkpoint, the shape in current model is torch.Size([8, 4096, 128]).\n\tsize mismatch for blocks.19.attn._W_K: copying a param with shape torch.Size([2097152, 1]) from checkpoint, the shape in current model is torch.Size([8, 4096, 128]).\n\tsize mismatch for blocks.19.attn._W_V: copying a param with shape torch.Size([2097152, 1]) from checkpoint, the shape in current model is torch.Size([8, 4096, 128]).\n\tsize mismatch for blocks.20.attn._W_K: copying a param with shape torch.Size([2097152, 1]) from checkpoint, the shape in current model is torch.Size([8, 4096, 128]).\n\tsize mismatch for blocks.20.attn._W_V: copying a param with shape torch.Size([2097152, 1]) from checkpoint, the shape in current model is torch.Size([8, 4096, 128]).\n\tsize mismatch for blocks.21.attn._W_K: copying a param with shape torch.Size([2097152, 1]) from checkpoint, the shape in current model is torch.Size([8, 4096, 128]).\n\tsize mismatch for blocks.21.attn._W_V: copying a param with shape torch.Size([2097152, 1]) from checkpoint, the shape in current model is torch.Size([8, 4096, 128]).\n\tsize mismatch for blocks.22.attn._W_K: copying a param with shape torch.Size([2097152, 1]) from checkpoint, the shape in current model is torch.Size([8, 4096, 128]).\n\tsize mismatch for blocks.22.attn._W_V: copying a param with shape torch.Size([2097152, 1]) from checkpoint, the shape in current model is torch.Size([8, 4096, 128]).\n\tsize mismatch for blocks.23.attn._W_K: copying a param with shape torch.Size([2097152, 1]) from checkpoint, the shape in current model is torch.Size([8, 4096, 128]).\n\tsize mismatch for blocks.23.attn._W_V: copying a param with shape torch.Size([2097152, 1]) from checkpoint, the shape in current model is torch.Size([8, 4096, 128]).\n\tsize mismatch for blocks.24.attn._W_K: copying a param with shape torch.Size([2097152, 1]) from checkpoint, the shape in current model is torch.Size([8, 4096, 128]).\n\tsize mismatch for blocks.24.attn._W_V: copying a param with shape torch.Size([2097152, 1]) from checkpoint, the shape in current model is torch.Size([8, 4096, 128]).\n\tsize mismatch for blocks.25.attn._W_K: copying a param with shape torch.Size([2097152, 1]) from checkpoint, the shape in current model is torch.Size([8, 4096, 128]).\n\tsize mismatch for blocks.25.attn._W_V: copying a param with shape torch.Size([2097152, 1]) from checkpoint, the shape in current model is torch.Size([8, 4096, 128]).\n\tsize mismatch for blocks.26.attn._W_K: copying a param with shape torch.Size([2097152, 1]) from checkpoint, the shape in current model is torch.Size([8, 4096, 128]).\n\tsize mismatch for blocks.26.attn._W_V: copying a param with shape torch.Size([2097152, 1]) from checkpoint, the shape in current model is torch.Size([8, 4096, 128]).\n\tsize mismatch for blocks.27.attn._W_K: copying a param with shape torch.Size([2097152, 1]) from checkpoint, the shape in current model is torch.Size([8, 4096, 128]).\n\tsize mismatch for blocks.27.attn._W_V: copying a param with shape torch.Size([2097152, 1]) from checkpoint, the shape in current model is torch.Size([8, 4096, 128]).\n\tsize mismatch for blocks.28.attn._W_K: copying a param with shape torch.Size([2097152, 1]) from checkpoint, the shape in current model is torch.Size([8, 4096, 128]).\n\tsize mismatch for blocks.28.attn._W_V: copying a param with shape torch.Size([2097152, 1]) from checkpoint, the shape in current model is torch.Size([8, 4096, 128]).\n\tsize mismatch for blocks.29.attn._W_K: copying a param with shape torch.Size([2097152, 1]) from checkpoint, the shape in current model is torch.Size([8, 4096, 128]).\n\tsize mismatch for blocks.29.attn._W_V: copying a param with shape torch.Size([2097152, 1]) from checkpoint, the shape in current model is torch.Size([8, 4096, 128]).\n\tsize mismatch for blocks.30.attn._W_K: copying a param with shape torch.Size([2097152, 1]) from checkpoint, the shape in current model is torch.Size([8, 4096, 128]).\n\tsize mismatch for blocks.30.attn._W_V: copying a param with shape torch.Size([2097152, 1]) from checkpoint, the shape in current model is torch.Size([8, 4096, 128]).\n\tsize mismatch for blocks.31.attn._W_K: copying a param with shape torch.Size([2097152, 1]) from checkpoint, the shape in current model is torch.Size([8, 4096, 128]).\n\tsize mismatch for blocks.31.attn._W_V: copying a param with shape torch.Size([2097152, 1]) from checkpoint, the shape in current model is torch.Size([8, 4096, 128]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformer_lens\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m HookedTransformer\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# quantized_model = HookedTransformer.from_pretrained_no_processing(\"gemma-2-2b\", hf_model=model, device=\"cuda:5\", dtype=\"float16\")\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m quantized_model \u001b[38;5;241m=\u001b[39m \u001b[43mHookedTransformer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained_no_processing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhf_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda:5\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfloat16\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/SAE/lib/python3.10/site-packages/transformer_lens/HookedTransformer.py:1372\u001b[0m, in \u001b[0;36mHookedTransformer.from_pretrained_no_processing\u001b[0;34m(cls, model_name, fold_ln, center_writing_weights, center_unembed, refactor_factored_attn_matrices, fold_value_biases, dtype, default_prepend_bos, default_padding_side, **from_pretrained_kwargs)\u001b[0m\n\u001b[1;32m   1353\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m   1354\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfrom_pretrained_no_processing\u001b[39m(\n\u001b[1;32m   1355\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1365\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfrom_pretrained_kwargs,\n\u001b[1;32m   1366\u001b[0m ):\n\u001b[1;32m   1367\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Wrapper for from_pretrained.\u001b[39;00m\n\u001b[1;32m   1368\u001b[0m \n\u001b[1;32m   1369\u001b[0m \u001b[38;5;124;03m    Wrapper for from_pretrained with all boolean flags related to simplifying the model set to\u001b[39;00m\n\u001b[1;32m   1370\u001b[0m \u001b[38;5;124;03m    False. Refer to from_pretrained for details.\u001b[39;00m\n\u001b[1;32m   1371\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1372\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1373\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1374\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfold_ln\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfold_ln\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1375\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcenter_writing_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcenter_writing_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1376\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcenter_unembed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcenter_unembed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1377\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfold_value_biases\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfold_value_biases\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1378\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrefactor_factored_attn_matrices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrefactor_factored_attn_matrices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1379\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1380\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdefault_prepend_bos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_prepend_bos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1381\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdefault_padding_side\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_padding_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1382\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfrom_pretrained_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1383\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/SAE/lib/python3.10/site-packages/transformer_lens/HookedTransformer.py:1337\u001b[0m, in \u001b[0;36mHookedTransformer.from_pretrained\u001b[0;34m(cls, model_name, fold_ln, center_writing_weights, center_unembed, refactor_factored_attn_matrices, checkpoint_index, checkpoint_value, hf_model, device, n_devices, tokenizer, move_to_device, fold_value_biases, default_prepend_bos, default_padding_side, dtype, first_n_layers, **from_pretrained_kwargs)\u001b[0m\n\u001b[1;32m   1329\u001b[0m \u001b[38;5;66;03m# Create the HookedTransformer object\u001b[39;00m\n\u001b[1;32m   1330\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m(\n\u001b[1;32m   1331\u001b[0m     cfg,\n\u001b[1;32m   1332\u001b[0m     tokenizer,\n\u001b[1;32m   1333\u001b[0m     move_to_device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1334\u001b[0m     default_padding_side\u001b[38;5;241m=\u001b[39mdefault_padding_side,\n\u001b[1;32m   1335\u001b[0m )\n\u001b[0;32m-> 1337\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_and_process_state_dict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1338\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1339\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfold_ln\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfold_ln\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1340\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcenter_writing_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcenter_writing_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1341\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcenter_unembed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcenter_unembed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1342\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfold_value_biases\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfold_value_biases\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1343\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrefactor_factored_attn_matrices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrefactor_factored_attn_matrices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1344\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1346\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m move_to_device:\n\u001b[1;32m   1347\u001b[0m     model\u001b[38;5;241m.\u001b[39mmove_model_modules_to_device()\n",
      "File \u001b[0;32m~/.conda/envs/SAE/lib/python3.10/site-packages/transformer_lens/HookedTransformer.py:1601\u001b[0m, in \u001b[0;36mHookedTransformer.load_and_process_state_dict\u001b[0;34m(self, state_dict, fold_ln, center_writing_weights, center_unembed, fold_value_biases, refactor_factored_attn_matrices)\u001b[0m\n\u001b[1;32m   1595\u001b[0m     state_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrefactor_factored_attn_matrices(state_dict)\n\u001b[1;32m   1598\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mload_in_4bit:\n\u001b[1;32m   1599\u001b[0m     \u001b[38;5;66;03m# with quantization, parameters should be assigned\u001b[39;00m\n\u001b[1;32m   1600\u001b[0m     \u001b[38;5;66;03m# so that quantization settings are not lost\u001b[39;00m\n\u001b[0;32m-> 1601\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43massign\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   1602\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1603\u001b[0m     state_dict_keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(state_dict\u001b[38;5;241m.\u001b[39mkeys())\n",
      "File \u001b[0;32m~/.conda/envs/SAE/lib/python3.10/site-packages/torch/nn/modules/module.py:2581\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2573\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2574\u001b[0m             \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m   2575\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2576\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)\n\u001b[1;32m   2577\u001b[0m             ),\n\u001b[1;32m   2578\u001b[0m         )\n\u001b[1;32m   2580\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2581\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   2582\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2583\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)\n\u001b[1;32m   2584\u001b[0m         )\n\u001b[1;32m   2585\u001b[0m     )\n\u001b[1;32m   2586\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for HookedTransformer:\n\tsize mismatch for blocks.0.attn._W_K: copying a param with shape torch.Size([2097152, 1]) from checkpoint, the shape in current model is torch.Size([8, 4096, 128]).\n\tsize mismatch for blocks.0.attn._W_V: copying a param with shape torch.Size([2097152, 1]) from checkpoint, the shape in current model is torch.Size([8, 4096, 128]).\n\tsize mismatch for blocks.1.attn._W_K: copying a param with shape torch.Size([2097152, 1]) from checkpoint, the shape in current model is torch.Size([8, 4096, 128]).\n\tsize mismatch for blocks.1.attn._W_V: copying a param with shape torch.Size([2097152, 1]) from checkpoint, the shape in current model is torch.Size([8, 4096, 128]).\n\tsize mismatch for blocks.2.attn._W_K: copying a param with shape torch.Size([2097152, 1]) from checkpoint, the shape in current model is torch.Size([8, 4096, 128]).\n\tsize mismatch for blocks.2.attn._W_V: copying a param with shape torch.Size([2097152, 1]) from checkpoint, the shape in current model is torch.Size([8, 4096, 128]).\n\tsize mismatch for blocks.3.attn._W_K: copying a param with shape torch.Size([2097152, 1]) from checkpoint, the shape in current model is torch.Size([8, 4096, 128]).\n\tsize mismatch for blocks.3.attn._W_V: copying a param with shape torch.Size([2097152, 1]) from checkpoint, the shape in current model is torch.Size([8, 4096, 128]).\n\tsize mismatch for blocks.4.attn._W_K: copying a param with shape torch.Size([2097152, 1]) from checkpoint, the shape in current model is torch.Size([8, 4096, 128]).\n\tsize mismatch for blocks.4.attn._W_V: copying a param with shape torch.Size([2097152, 1]) from checkpoint, the shape in current model is torch.Size([8, 4096, 128]).\n\tsize mismatch for blocks.5.attn._W_K: copying a param with shape torch.Size([2097152, 1]) from checkpoint, the shape in current model is torch.Size([8, 4096, 128]).\n\tsize mismatch for blocks.5.attn._W_V: copying a param with shape torch.Size([2097152, 1]) from checkpoint, the shape in current model is torch.Size([8, 4096, 128]).\n\tsize mismatch for blocks.6.attn._W_K: copying a param with shape torch.Size([2097152, 1]) from checkpoint, the shape in current model is torch.Size([8, 4096, 128]).\n\tsize mismatch for blocks.6.attn._W_V: copying a param with shape torch.Size([2097152, 1]) from checkpoint, the shape in current model is torch.Size([8, 4096, 128]).\n\tsize mismatch for blocks.7.attn._W_K: copying a param with shape torch.Size([2097152, 1]) from checkpoint, the shape in current model is torch.Size([8, 4096, 128]).\n\tsize mismatch for blocks.7.attn._W_V: copying a param with shape torch.Size([2097152, 1]) from checkpoint, the shape in current model is torch.Size([8, 4096, 128]).\n\tsize mismatch for blocks.8.attn._W_K: copying a param with shape torch.Size([2097152, 1]) from checkpoint, the shape in current model is torch.Size([8, 4096, 128]).\n\tsize mismatch for blocks.8.attn._W_V: copying a param with shape torch.Size([2097152, 1]) from checkpoint, the shape in current model is torch.Size([8, 4096, 128]).\n\tsize mismatch for blocks.9.attn._W_K: copying a param with shape torch.Size([2097152, 1]) from checkpoint, the shape in current model is torch.Size([8, 4096, 128]).\n\tsize mismatch for blocks.9.attn._W_V: copying a param with shape torch.Size([2097152, 1]) from checkpoint, the shape in current model is torch.Size([8, 4096, 128]).\n\tsize mismatch for blocks.10.attn._W_K: copying a param with shape torch.Size([2097152, 1]) from checkpoint, the shape in current model is torch.Size([8, 4096, 128]).\n\tsize mismatch for blocks.10.attn._W_V: copying a param with shape torch.Size([2097152, 1]) from checkpoint, the shape in current model is torch.Size([8, 4096, 128]).\n\tsize mismatch for blocks.11.attn._W_K: copying a param with shape torch.Size([2097152, 1]) from checkpoint, the shape in current model is torch.Size([8, 4096, 128]).\n\tsize mismatch for blocks.11.attn._W_V: copying a param with shape torch.Size([2097152, 1]) from checkpoint, the shape in current model is torch.Size([8, 4096, 128]).\n\tsize mismatch for blocks.12.attn._W_K: copying a param with shape torch.Size([2097152, 1]) from checkpoint, the shape in current model is torch.Size([8, 4096, 128]).\n\tsize mismatch for blocks.12.attn._W_V: copying a param with shape torch.Size([2097152, 1]) from checkpoint, the shape in current model is torch.Size([8, 4096, 128]).\n\tsize mismatch for blocks.13.attn._W_K: copying a param with shape torch.Size([2097152, 1]) from checkpoint, the shape in current model is torch.Size([8, 4096, 128]).\n\tsize mismatch for blocks.13.attn._W_V: copying a param with shape torch.Size([2097152, 1]) from checkpoint, the shape in current model is torch.Size([8, 4096, 128]).\n\tsize mismatch for blocks.14.attn._W_K: copying a param with shape torch.Size([2097152, 1]) from checkpoint, the shape in current model is torch.Size([8, 4096, 128]).\n\tsize mismatch for blocks.14.attn._W_V: copying a param with shape torch.Size([2097152, 1]) from checkpoint, the shape in current model is torch.Size([8, 4096, 128]).\n\tsize mismatch for blocks.15.attn._W_K: copying a param with shape torch.Size([2097152, 1]) from checkpoint, the shape in current model is torch.Size([8, 4096, 128]).\n\tsize mismatch for blocks.15.attn._W_V: copying a param with shape torch.Size([2097152, 1]) from checkpoint, the shape in current model is torch.Size([8, 4096, 128]).\n\tsize mismatch for blocks.16.attn._W_K: copying a param with shape torch.Size([2097152, 1]) from checkpoint, the shape in current model is torch.Size([8, 4096, 128]).\n\tsize mismatch for blocks.16.attn._W_V: copying a param with shape torch.Size([2097152, 1]) from checkpoint, the shape in current model is torch.Size([8, 4096, 128]).\n\tsize mismatch for blocks.17.attn._W_K: copying a param with shape torch.Size([2097152, 1]) from checkpoint, the shape in current model is torch.Size([8, 4096, 128]).\n\tsize mismatch for blocks.17.attn._W_V: copying a param with shape torch.Size([2097152, 1]) from checkpoint, the shape in current model is torch.Size([8, 4096, 128]).\n\tsize mismatch for blocks.18.attn._W_K: copying a param with shape torch.Size([2097152, 1]) from checkpoint, the shape in current model is torch.Size([8, 4096, 128]).\n\tsize mismatch for blocks.18.attn._W_V: copying a param with shape torch.Size([2097152, 1]) from checkpoint, the shape in current model is torch.Size([8, 4096, 128]).\n\tsize mismatch for blocks.19.attn._W_K: copying a param with shape torch.Size([2097152, 1]) from checkpoint, the shape in current model is torch.Size([8, 4096, 128]).\n\tsize mismatch for blocks.19.attn._W_V: copying a param with shape torch.Size([2097152, 1]) from checkpoint, the shape in current model is torch.Size([8, 4096, 128]).\n\tsize mismatch for blocks.20.attn._W_K: copying a param with shape torch.Size([2097152, 1]) from checkpoint, the shape in current model is torch.Size([8, 4096, 128]).\n\tsize mismatch for blocks.20.attn._W_V: copying a param with shape torch.Size([2097152, 1]) from checkpoint, the shape in current model is torch.Size([8, 4096, 128]).\n\tsize mismatch for blocks.21.attn._W_K: copying a param with shape torch.Size([2097152, 1]) from checkpoint, the shape in current model is torch.Size([8, 4096, 128]).\n\tsize mismatch for blocks.21.attn._W_V: copying a param with shape torch.Size([2097152, 1]) from checkpoint, the shape in current model is torch.Size([8, 4096, 128]).\n\tsize mismatch for blocks.22.attn._W_K: copying a param with shape torch.Size([2097152, 1]) from checkpoint, the shape in current model is torch.Size([8, 4096, 128]).\n\tsize mismatch for blocks.22.attn._W_V: copying a param with shape torch.Size([2097152, 1]) from checkpoint, the shape in current model is torch.Size([8, 4096, 128]).\n\tsize mismatch for blocks.23.attn._W_K: copying a param with shape torch.Size([2097152, 1]) from checkpoint, the shape in current model is torch.Size([8, 4096, 128]).\n\tsize mismatch for blocks.23.attn._W_V: copying a param with shape torch.Size([2097152, 1]) from checkpoint, the shape in current model is torch.Size([8, 4096, 128]).\n\tsize mismatch for blocks.24.attn._W_K: copying a param with shape torch.Size([2097152, 1]) from checkpoint, the shape in current model is torch.Size([8, 4096, 128]).\n\tsize mismatch for blocks.24.attn._W_V: copying a param with shape torch.Size([2097152, 1]) from checkpoint, the shape in current model is torch.Size([8, 4096, 128]).\n\tsize mismatch for blocks.25.attn._W_K: copying a param with shape torch.Size([2097152, 1]) from checkpoint, the shape in current model is torch.Size([8, 4096, 128]).\n\tsize mismatch for blocks.25.attn._W_V: copying a param with shape torch.Size([2097152, 1]) from checkpoint, the shape in current model is torch.Size([8, 4096, 128]).\n\tsize mismatch for blocks.26.attn._W_K: copying a param with shape torch.Size([2097152, 1]) from checkpoint, the shape in current model is torch.Size([8, 4096, 128]).\n\tsize mismatch for blocks.26.attn._W_V: copying a param with shape torch.Size([2097152, 1]) from checkpoint, the shape in current model is torch.Size([8, 4096, 128]).\n\tsize mismatch for blocks.27.attn._W_K: copying a param with shape torch.Size([2097152, 1]) from checkpoint, the shape in current model is torch.Size([8, 4096, 128]).\n\tsize mismatch for blocks.27.attn._W_V: copying a param with shape torch.Size([2097152, 1]) from checkpoint, the shape in current model is torch.Size([8, 4096, 128]).\n\tsize mismatch for blocks.28.attn._W_K: copying a param with shape torch.Size([2097152, 1]) from checkpoint, the shape in current model is torch.Size([8, 4096, 128]).\n\tsize mismatch for blocks.28.attn._W_V: copying a param with shape torch.Size([2097152, 1]) from checkpoint, the shape in current model is torch.Size([8, 4096, 128]).\n\tsize mismatch for blocks.29.attn._W_K: copying a param with shape torch.Size([2097152, 1]) from checkpoint, the shape in current model is torch.Size([8, 4096, 128]).\n\tsize mismatch for blocks.29.attn._W_V: copying a param with shape torch.Size([2097152, 1]) from checkpoint, the shape in current model is torch.Size([8, 4096, 128]).\n\tsize mismatch for blocks.30.attn._W_K: copying a param with shape torch.Size([2097152, 1]) from checkpoint, the shape in current model is torch.Size([8, 4096, 128]).\n\tsize mismatch for blocks.30.attn._W_V: copying a param with shape torch.Size([2097152, 1]) from checkpoint, the shape in current model is torch.Size([8, 4096, 128]).\n\tsize mismatch for blocks.31.attn._W_K: copying a param with shape torch.Size([2097152, 1]) from checkpoint, the shape in current model is torch.Size([8, 4096, 128]).\n\tsize mismatch for blocks.31.attn._W_V: copying a param with shape torch.Size([2097152, 1]) from checkpoint, the shape in current model is torch.Size([8, 4096, 128])."
     ]
    }
   ],
   "source": [
    "from transformer_lens import HookedTransformer\n",
    "\n",
    "# quantized_model = HookedTransformer.from_pretrained_no_processing(\"gemma-2-2b\", hf_model=model, device=\"cuda:5\", dtype=\"float16\")\n",
    "quantized_model = HookedTransformer.from_pretrained_no_processing(model_id, hf_model=model, device=\"cuda:5\", dtype=\"float16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hookedmodel.cfg.load_in_4bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hookedmodel.cfg.load_in_4bit = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hookedmodel.cfg.load_in_4bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer_lens.pretrained.weight_conversions import convert_gemma_weights_q\n",
    "from transformer_lens.loading_from_pretrained import get_pretrained_state_dict\n",
    "\n",
    "state_dict1 = convert_gemma_weights_q(model, hookedmodel.cfg)\n",
    "# state_dict2 = get_pretrained_state_dict(\"gemma-2-2b\", hookedmodel.cfg, model, dtype=\"float16\")\n",
    "\n",
    "# assert state_dict1.keys() == state_dict2.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SAE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

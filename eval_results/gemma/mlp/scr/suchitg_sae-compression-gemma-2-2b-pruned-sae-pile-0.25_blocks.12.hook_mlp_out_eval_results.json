{
  "eval_type_id": "scr",
  "eval_config": {
    "random_seed": 42,
    "dataset_names": [
      "LabHC/bias_in_bios_class_set1",
      "canrager/amazon_reviews_mcauley_1and5"
    ],
    "perform_scr": true,
    "early_stopping_patience": 20,
    "train_set_size": 4000,
    "test_set_size": 1000,
    "context_length": 128,
    "probe_train_batch_size": 16,
    "probe_test_batch_size": 500,
    "probe_epochs": 20,
    "probe_lr": 0.001,
    "probe_l1_penalty": 0.001,
    "sae_batch_size": 125,
    "llm_batch_size": 32,
    "llm_dtype": "bfloat16",
    "lower_vram_usage": false,
    "model_name": "gemma-2-2b",
    "n_values": [
      2,
      5,
      10,
      20,
      50,
      100,
      500
    ],
    "column1_vals_lookup": {
      "LabHC/bias_in_bios_class_set1": [
        [
          "professor",
          "nurse"
        ],
        [
          "architect",
          "journalist"
        ],
        [
          "surgeon",
          "psychologist"
        ],
        [
          "attorney",
          "teacher"
        ]
      ],
      "canrager/amazon_reviews_mcauley_1and5": [
        [
          "Books",
          "CDs_and_Vinyl"
        ],
        [
          "Software",
          "Electronics"
        ],
        [
          "Pet_Supplies",
          "Office_Products"
        ],
        [
          "Industrial_and_Scientific",
          "Toys_and_Games"
        ]
      ]
    }
  },
  "eval_id": "7da83605-d9b7-41ff-ad6c-466dcf497722",
  "datetime_epoch_millis": 1742867176500,
  "eval_result_metrics": {
    "scr_metrics": {
      "scr_dir1_threshold_2": 0.13364148536648982,
      "scr_metric_threshold_2": 0.024848592164386275,
      "scr_dir2_threshold_2": 0.0351164865077336,
      "scr_dir1_threshold_5": 0.12351536249385314,
      "scr_metric_threshold_5": 0.03886165502304283,
      "scr_dir2_threshold_5": 0.04220085800641912,
      "scr_dir1_threshold_10": 0.11458310425848393,
      "scr_metric_threshold_10": 0.0462603315590897,
      "scr_dir2_threshold_10": 0.06437620876019114,
      "scr_dir1_threshold_20": 0.12394889826849008,
      "scr_metric_threshold_20": 0.05740207460863865,
      "scr_dir2_threshold_20": 0.08200227021447024,
      "scr_dir1_threshold_50": 0.15659338111602955,
      "scr_metric_threshold_50": 0.09136668383947542,
      "scr_dir2_threshold_50": 0.1293171796731213,
      "scr_dir1_threshold_100": 0.13961354278211743,
      "scr_metric_threshold_100": 0.10050567775055222,
      "scr_dir2_threshold_100": 0.18330942622918667,
      "scr_dir1_threshold_500": 0.04298189425577417,
      "scr_metric_threshold_500": 0.1511615335646396,
      "scr_dir2_threshold_500": 0.2040216591752385
    }
  },
  "eval_result_details": [
    {
      "dataset_name": "LabHC/bias_in_bios_class_set1_scr_professor_nurse_results",
      "scr_dir1_threshold_2": 0.1250009313230002,
      "scr_metric_threshold_2": 0.01913879282354446,
      "scr_dir2_threshold_2": 0.01913879282354446,
      "scr_dir1_threshold_5": 0.1250009313230002,
      "scr_metric_threshold_5": 0.033492816143788066,
      "scr_dir2_threshold_5": 0.033492816143788066,
      "scr_dir1_threshold_10": 0.0,
      "scr_metric_threshold_10": 0.038277443052259434,
      "scr_dir2_threshold_10": 0.038277443052259434,
      "scr_dir1_threshold_20": 0.08333229852999978,
      "scr_metric_threshold_20": 0.040669899101324604,
      "scr_dir2_threshold_20": 0.040669899101324604,
      "scr_dir1_threshold_50": 0.24999937911799985,
      "scr_metric_threshold_50": 0.0574162358758039,
      "scr_dir2_threshold_50": 0.0574162358758039,
      "scr_dir1_threshold_100": 0.1666670805880001,
      "scr_metric_threshold_100": 0.05980854933003958,
      "scr_dir2_threshold_100": 0.05980854933003958,
      "scr_dir1_threshold_500": 0.0,
      "scr_metric_threshold_500": 0.10526321793466505,
      "scr_dir2_threshold_500": 0.10526321793466505
    },
    {
      "dataset_name": "LabHC/bias_in_bios_class_set1_scr_architect_journalist_results",
      "scr_dir1_threshold_2": 0.32653110874314545,
      "scr_metric_threshold_2": 0.010695208472019047,
      "scr_dir2_threshold_2": 0.010695208472019047,
      "scr_dir1_threshold_5": 0.32653110874314545,
      "scr_metric_threshold_5": 0.005347683921360758,
      "scr_dir2_threshold_5": 0.005347683921360758,
      "scr_dir1_threshold_10": 0.2448983315573591,
      "scr_metric_threshold_10": 0.013368970747348191,
      "scr_dir2_threshold_10": 0.013368970747348191,
      "scr_dir1_threshold_20": 0.3061220021311691,
      "scr_metric_threshold_20": 0.024064179219367238,
      "scr_dir2_threshold_20": 0.024064179219367238,
      "scr_dir1_threshold_50": 0.32653110874314545,
      "scr_metric_threshold_50": 0.03208562541605714,
      "scr_dir2_threshold_50": 0.03208562541605714,
      "scr_dir1_threshold_100": 0.40816266950822544,
      "scr_metric_threshold_100": 0.029411863140727995,
      "scr_dir2_threshold_100": 0.029411863140727995,
      "scr_dir1_threshold_500": 0.3469389989344155,
      "scr_metric_threshold_500": 0.016042733022677336,
      "scr_dir2_threshold_500": 0.016042733022677336
    },
    {
      "dataset_name": "LabHC/bias_in_bios_class_set1_scr_surgeon_psychologist_results",
      "scr_dir1_threshold_2": 0.2666669315759253,
      "scr_metric_threshold_2": 0.005025060907560086,
      "scr_dir2_threshold_2": 0.005025060907560086,
      "scr_dir1_threshold_5": 0.2333340618337946,
      "scr_metric_threshold_5": 0.012562802029313504,
      "scr_dir2_threshold_5": 0.012562802029313504,
      "scr_dir1_threshold_10": 0.29999980131805604,
      "scr_metric_threshold_10": 0.005025060907560086,
      "scr_dir2_threshold_10": 0.005025060907560086,
      "scr_dir1_threshold_20": 0.29999980131805604,
      "scr_metric_threshold_20": 0.03266334518038043,
      "scr_dir2_threshold_20": 0.03266334518038043,
      "scr_dir1_threshold_50": 0.2666669315759253,
      "scr_metric_threshold_50": 0.027638134512407053,
      "scr_dir2_threshold_50": 0.027638134512407053,
      "scr_dir1_threshold_100": 0.19999920527222406,
      "scr_metric_threshold_100": 0.022613073604846965,
      "scr_dir2_threshold_100": 0.022613073604846965,
      "scr_dir1_threshold_500": -0.7333330684240746,
      "scr_metric_threshold_500": 0.030150664966187093,
      "scr_dir2_threshold_500": 0.030150664966187093
    },
    {
      "dataset_name": "LabHC/bias_in_bios_class_set1_scr_attorney_teacher_results",
      "scr_dir1_threshold_2": 0.1896557216834399,
      "scr_metric_threshold_2": 0.0026524855055588045,
      "scr_dir2_threshold_2": 0.0026524855055588045,
      "scr_dir1_threshold_5": 0.05172447458017283,
      "scr_metric_threshold_5": 0.007957614619168251,
      "scr_dir2_threshold_5": 0.007957614619168251,
      "scr_dir1_threshold_10": 0.06896562355163352,
      "scr_metric_threshold_10": 0.010610100124727056,
      "scr_dir2_threshold_10": 0.010610100124727056,
      "scr_dir1_threshold_20": -0.05172447458017283,
      "scr_metric_threshold_20": 0.007957614619168251,
      "scr_dir2_threshold_20": 0.007957614619168251,
      "scr_dir1_threshold_50": -0.17241354504618844,
      "scr_metric_threshold_50": 0.031830300374181164,
      "scr_dir2_threshold_50": 0.031830300374181164,
      "scr_dir1_threshold_100": -0.3103447921494555,
      "scr_metric_threshold_100": 0.03978775689085758,
      "scr_dir2_threshold_100": 0.03978775689085758,
      "scr_dir1_threshold_500": -0.2931036431779948,
      "scr_metric_threshold_500": 0.03448278587973997,
      "scr_dir2_threshold_500": 0.03448278587973997
    },
    {
      "dataset_name": "canrager/amazon_reviews_mcauley_1and5_scr_Books_CDs_and_Vinyl_results",
      "scr_dir1_threshold_2": 0.06557388902829207,
      "scr_metric_threshold_2": 0.06557388902829207,
      "scr_dir2_threshold_2": 0.09774443503318589,
      "scr_dir1_threshold_5": 0.08524595802426867,
      "scr_metric_threshold_5": 0.08524595802426867,
      "scr_dir2_threshold_5": 0.12781968028610227,
      "scr_dir1_threshold_10": 0.10163938142508616,
      "scr_metric_threshold_10": 0.10163938142508616,
      "scr_dir2_threshold_10": 0.24060151386816173,
      "scr_dir1_threshold_20": 0.11147551363558549,
      "scr_metric_threshold_20": 0.11147551363558549,
      "scr_dir2_threshold_20": 0.315789402922868,
      "scr_dir1_threshold_50": 0.13770506924690232,
      "scr_metric_threshold_50": 0.13770506924690232,
      "scr_dir2_threshold_50": 0.4285716846600967,
      "scr_dir1_threshold_100": 0.13770506924690232,
      "scr_metric_threshold_100": 0.13770506924690232,
      "scr_dir2_threshold_100": 0.5037595737148031,
      "scr_dir1_threshold_500": 0.226229477441308,
      "scr_metric_threshold_500": 0.226229477441308,
      "scr_dir2_threshold_500": 0.5563909167910297
    },
    {
      "dataset_name": "canrager/amazon_reviews_mcauley_1and5_scr_Software_Electronics_results",
      "scr_dir1_threshold_2": 0.0311418763622444,
      "scr_metric_threshold_2": 0.0311418763622444,
      "scr_dir2_threshold_2": 0.0,
      "scr_dir1_threshold_5": 0.06574412162151014,
      "scr_metric_threshold_5": 0.06574412162151014,
      "scr_dir2_threshold_5": -0.046874854480871565,
      "scr_dir1_threshold_10": 0.08996546643413296,
      "scr_metric_threshold_10": 0.08996546643413296,
      "scr_dir2_threshold_10": 0.03906245634426147,
      "scr_dir1_threshold_20": 0.10726648594155529,
      "scr_metric_threshold_20": 0.10726648594155529,
      "scr_dir2_threshold_20": 0.04687532014208255,
      "scr_dir1_threshold_50": 0.15570938181122201,
      "scr_metric_threshold_50": 0.15570938181122201,
      "scr_dir2_threshold_50": 0.1171878346939954,
      "scr_dir1_threshold_100": 0.1626299133608436,
      "scr_metric_threshold_100": 0.1626299133608436,
      "scr_dir2_threshold_100": 0.18749988358469724,
      "scr_dir1_threshold_500": 0.259515705100177,
      "scr_metric_threshold_500": 0.259515705100177,
      "scr_dir2_threshold_500": 0.15625029103825686
    },
    {
      "dataset_name": "canrager/amazon_reviews_mcauley_1and5_scr_Pet_Supplies_Office_Products_results",
      "scr_dir1_threshold_2": 0.02456129416939053,
      "scr_metric_threshold_2": 0.02456129416939053,
      "scr_dir2_threshold_2": 0.12328769360278982,
      "scr_dir1_threshold_5": 0.03508768260717803,
      "scr_metric_threshold_5": 0.03508768260717803,
      "scr_dir2_threshold_5": 0.13013691919163053,
      "scr_dir1_threshold_10": 0.042105135472979546,
      "scr_metric_threshold_10": 0.042105135472979546,
      "scr_dir2_threshold_10": 0.12328769360278982,
      "scr_dir1_threshold_20": 0.08421048008504432,
      "scr_metric_threshold_20": 0.08421048008504432,
      "scr_dir2_threshold_20": 0.15068500420906691,
      "scr_dir1_threshold_50": 0.16491223373718789,
      "scr_metric_threshold_50": 0.16491223373718789,
      "scr_dir2_threshold_50": 0.20547921717070686,
      "scr_dir1_threshold_100": 0.21754375764795492,
      "scr_metric_threshold_100": 0.21754375764795492,
      "scr_dir2_threshold_100": 0.3698630808083695,
      "scr_dir1_threshold_500": 0.3157893526036875,
      "scr_metric_threshold_500": 0.3157893526036875,
      "scr_dir2_threshold_500": 0.30821923400697454
    },
    {
      "dataset_name": "canrager/amazon_reviews_mcauley_1and5_scr_Industrial_and_Scientific_Toys_and_Games_results",
      "scr_dir1_threshold_2": 0.04000013004648078,
      "scr_metric_threshold_2": 0.04000013004648078,
      "scr_dir2_threshold_2": 0.022388215717210662,
      "scr_dir1_threshold_5": 0.06545456121775525,
      "scr_metric_threshold_5": 0.06545456121775525,
      "scr_dir2_threshold_5": 0.06716420234086116,
      "scr_dir1_threshold_10": 0.06909109430862415,
      "scr_metric_threshold_10": 0.06909109430862415,
      "scr_dir2_threshold_10": 0.044776431434421324,
      "scr_dir1_threshold_20": 0.050909079086683566,
      "scr_metric_threshold_20": 0.050909079086683566,
      "scr_dir2_threshold_20": 0.037313396321503885,
      "scr_dir1_threshold_50": 0.12363648974204197,
      "scr_metric_threshold_50": 0.12363648974204197,
      "scr_dir2_threshold_50": 0.13432840468172233,
      "scr_dir1_threshold_100": 0.13454543878224476,
      "scr_metric_threshold_100": 0.13454543878224476,
      "scr_dir2_threshold_100": 0.25373162875915145,
      "scr_dir1_threshold_500": 0.22181833156867484,
      "scr_metric_threshold_500": 0.22181833156867484,
      "scr_dir2_threshold_500": 0.42537342976237763
    }
  ],
  "sae_bench_commit_hash": "c5c90a136c8db8e704e401328a49b19932ad56c9",
  "sae_lens_id": "blocks.12.hook_mlp_out",
  "sae_lens_release_id": "suchitg/sae-compression-gemma-2-2b-pruned-sae-pile-0.25",
  "sae_lens_version": "5.6.1",
  "sae_cfg_dict": {
    "architecture": "jumprelu",
    "d_in": 2304,
    "d_sae": 16384,
    "activation_fn_str": "relu",
    "apply_b_dec_to_input": false,
    "finetuning_scaling_factor": false,
    "context_size": 1024,
    "model_name": "gemma-2-2b",
    "hook_name": "blocks.12.hook_mlp_out",
    "hook_layer": 12,
    "hook_head_index": null,
    "prepend_bos": true,
    "dataset_path": "monology/pile-uncopyrighted",
    "dataset_trust_remote_code": true,
    "normalize_activations": null,
    "dtype": "torch.bfloat16",
    "device": "cuda",
    "sae_lens_training_version": null,
    "activation_fn_kwargs": {},
    "neuronpedia_id": "gemma-2-2b/12-gemmascope-mlp-16k",
    "model_from_pretrained_kwargs": {},
    "seqpos_slice": [
      null
    ]
  },
  "eval_result_unstructured": null
}
{
  "eval_type_id": "scr",
  "eval_config": {
    "random_seed": 42,
    "dataset_names": [
      "LabHC/bias_in_bios_class_set1",
      "canrager/amazon_reviews_mcauley_1and5"
    ],
    "perform_scr": true,
    "early_stopping_patience": 20,
    "train_set_size": 4000,
    "test_set_size": 1000,
    "context_length": 128,
    "probe_train_batch_size": 16,
    "probe_test_batch_size": 500,
    "probe_epochs": 20,
    "probe_lr": 0.001,
    "probe_l1_penalty": 0.001,
    "sae_batch_size": 125,
    "llm_batch_size": 32,
    "llm_dtype": "bfloat16",
    "lower_vram_usage": false,
    "model_name": "gemma-2-2b",
    "n_values": [
      2,
      5,
      10,
      20,
      50,
      100,
      500
    ],
    "column1_vals_lookup": {
      "LabHC/bias_in_bios_class_set1": [
        [
          "professor",
          "nurse"
        ],
        [
          "architect",
          "journalist"
        ],
        [
          "surgeon",
          "psychologist"
        ],
        [
          "attorney",
          "teacher"
        ]
      ],
      "canrager/amazon_reviews_mcauley_1and5": [
        [
          "Books",
          "CDs_and_Vinyl"
        ],
        [
          "Software",
          "Electronics"
        ],
        [
          "Pet_Supplies",
          "Office_Products"
        ],
        [
          "Industrial_and_Scientific",
          "Toys_and_Games"
        ]
      ]
    }
  },
  "eval_id": "5d66ff5e-04b2-4984-a151-c4d2c3a40a70",
  "datetime_epoch_millis": 1742852618216,
  "eval_result_metrics": {
    "scr_metrics": {
      "scr_dir1_threshold_2": 0.29468861097133986,
      "scr_metric_threshold_2": 0.1105223258959027,
      "scr_dir2_threshold_2": 0.1013597939980053,
      "scr_dir1_threshold_5": 0.32871740939198935,
      "scr_metric_threshold_5": 0.16656613601385387,
      "scr_dir2_threshold_5": 0.15514804981117591,
      "scr_dir1_threshold_10": 0.3409456620892568,
      "scr_metric_threshold_10": 0.24486899340379784,
      "scr_dir2_threshold_10": 0.23384748430722874,
      "scr_dir1_threshold_20": 0.29883853243192704,
      "scr_metric_threshold_20": 0.30618498194060473,
      "scr_dir2_threshold_20": 0.295120312295959,
      "scr_dir1_threshold_50": 0.2997043836302853,
      "scr_metric_threshold_50": 0.36799520757262844,
      "scr_dir2_threshold_50": 0.3555113676949642,
      "scr_dir1_threshold_100": 0.2222187580616761,
      "scr_metric_threshold_100": 0.31597614191264844,
      "scr_dir2_threshold_100": 0.3154176533701414,
      "scr_dir1_threshold_500": 0.15679473407455702,
      "scr_metric_threshold_500": 0.3378606082755206,
      "scr_dir2_threshold_500": 0.329672041299865
    }
  },
  "eval_result_details": [
    {
      "dataset_name": "LabHC/bias_in_bios_class_set1_scr_professor_nurse_results",
      "scr_dir1_threshold_2": 0.5272734563211802,
      "scr_metric_threshold_2": 0.036764671508278036,
      "scr_dir2_threshold_2": 0.036764671508278036,
      "scr_dir1_threshold_5": 0.5636366197885228,
      "scr_metric_threshold_5": 0.05392145977621377,
      "scr_dir2_threshold_5": 0.05392145977621377,
      "scr_dir1_threshold_10": 0.5818182015221941,
      "scr_metric_threshold_10": 0.05882350363120852,
      "scr_dir2_threshold_10": 0.05882350363120852,
      "scr_dir1_threshold_20": 0.5090907908668356,
      "scr_metric_threshold_20": 0.10294116787706951,
      "scr_dir2_threshold_20": 0.10294116787706951,
      "scr_dir1_threshold_50": 0.4000002167441346,
      "scr_metric_threshold_50": 0.125,
      "scr_dir2_threshold_50": 0.125,
      "scr_dir1_threshold_100": 0.4363644639321504,
      "scr_metric_threshold_100": 0.21323518240190362,
      "scr_dir2_threshold_100": 0.21323518240190362,
      "scr_dir1_threshold_500": -0.5636355360678496,
      "scr_metric_threshold_500": 0.2965686131284492,
      "scr_dir2_threshold_500": 0.2965686131284492
    },
    {
      "dataset_name": "LabHC/bias_in_bios_class_set1_scr_architect_journalist_results",
      "scr_dir1_threshold_2": 0.26829280702927605,
      "scr_metric_threshold_2": 0.18784533024838174,
      "scr_dir2_threshold_2": 0.18784533024838174,
      "scr_dir1_threshold_5": 0.28048776942020687,
      "scr_metric_threshold_5": 0.21270721461439834,
      "scr_dir2_threshold_5": 0.21270721461439834,
      "scr_dir1_threshold_10": 0.28048776942020687,
      "scr_metric_threshold_10": 0.2817679130457261,
      "scr_dir2_threshold_10": 0.2817679130457261,
      "scr_dir1_threshold_20": 0.18292661652124106,
      "scr_metric_threshold_20": 0.3370166364444813,
      "scr_dir2_threshold_20": 0.3370166364444813,
      "scr_dir1_threshold_50": -0.414633809491965,
      "scr_metric_threshold_50": 0.44198896392157677,
      "scr_dir2_threshold_50": 0.44198896392157677,
      "scr_dir1_threshold_100": -0.5121949623909308,
      "scr_metric_threshold_100": 0.13812156151634855,
      "scr_dir2_threshold_100": 0.13812156151634855,
      "scr_dir1_threshold_500": 0.09756115289896579,
      "scr_metric_threshold_500": 0.19060772525489628,
      "scr_dir2_threshold_500": 0.19060772525489628
    },
    {
      "dataset_name": "LabHC/bias_in_bios_class_set1_scr_surgeon_psychologist_results",
      "scr_dir1_threshold_2": 0.49206359718628334,
      "scr_metric_threshold_2": 0.03608246947566349,
      "scr_dir2_threshold_2": 0.03608246947566349,
      "scr_dir1_threshold_5": 0.5396820140685832,
      "scr_metric_threshold_5": 0.07731951161061577,
      "scr_dir2_threshold_5": 0.07731951161061577,
      "scr_dir1_threshold_10": 0.5079364028137167,
      "scr_metric_threshold_10": 0.11855670736578201,
      "scr_dir2_threshold_10": 0.11855670736578201,
      "scr_dir1_threshold_20": 0.4126976768388736,
      "scr_metric_threshold_20": 0.14432987790265395,
      "scr_dir2_threshold_20": 0.14432987790265395,
      "scr_dir1_threshold_50": 0.3492064543291405,
      "scr_metric_threshold_50": 0.2577320126091472,
      "scr_dir2_threshold_50": 0.2577320126091472,
      "scr_dir1_threshold_100": -0.14285714285714285,
      "scr_metric_threshold_100": 0.015463871598080362,
      "scr_dir2_threshold_100": 0.015463871598080362,
      "scr_dir1_threshold_500": 0.1111105854971546,
      "scr_metric_threshold_500": -0.028350456866516324,
      "scr_dir2_threshold_500": -0.028350456866516324
    },
    {
      "dataset_name": "LabHC/bias_in_bios_class_set1_scr_attorney_teacher_results",
      "scr_dir1_threshold_2": 0.25,
      "scr_metric_threshold_2": 0.08123245043882042,
      "scr_dir2_threshold_2": 0.08123245043882042,
      "scr_dir1_threshold_5": 0.1851854917933265,
      "scr_metric_threshold_5": 0.14845931173163016,
      "scr_dir2_threshold_5": 0.14845931173163016,
      "scr_dir1_threshold_10": 0.15740753005066394,
      "scr_metric_threshold_10": 0.21848742442147598,
      "scr_dir2_threshold_10": 0.21848742442147598,
      "scr_dir1_threshold_20": -0.04629623497466803,
      "scr_metric_threshold_20": 0.3053222106945761,
      "scr_dir2_threshold_20": 0.3053222106945761,
      "scr_dir1_threshold_50": 0.5925924699493361,
      "scr_metric_threshold_50": 0.35854348284137516,
      "scr_dir2_threshold_50": 0.35854348284137516,
      "scr_dir1_threshold_100": 0.5648150601013279,
      "scr_metric_threshold_100": 0.08123245043882042,
      "scr_dir2_threshold_100": 0.08123245043882042,
      "scr_dir1_threshold_500": 0.5277779617426626,
      "scr_metric_threshold_500": 0.08683478627310014,
      "scr_dir2_threshold_500": 0.08683478627310014
    },
    {
      "dataset_name": "canrager/amazon_reviews_mcauley_1and5_scr_Books_CDs_and_Vinyl_results",
      "scr_dir1_threshold_2": 0.08133980980128552,
      "scr_metric_threshold_2": 0.06926420777301313,
      "scr_dir2_threshold_2": 0.06926420777301313,
      "scr_dir1_threshold_5": 0.10047860535392326,
      "scr_metric_threshold_5": 0.16450236444652702,
      "scr_dir2_threshold_5": 0.16450236444652702,
      "scr_dir1_threshold_10": 0.11483248812612686,
      "scr_metric_threshold_10": 0.43290052449505706,
      "scr_dir2_threshold_10": 0.43290052449505706,
      "scr_dir1_threshold_20": 0.0669856418393823,
      "scr_metric_threshold_20": 0.549783681590448,
      "scr_dir2_threshold_20": 0.549783681590448,
      "scr_dir1_threshold_50": 0.07655518221055099,
      "scr_metric_threshold_50": 0.6623378902072151,
      "scr_dir2_threshold_50": 0.6623378902072151,
      "scr_dir1_threshold_100": 0.07177026943011683,
      "scr_metric_threshold_100": 0.7229439429942225,
      "scr_dir2_threshold_100": 0.7229439429942225,
      "scr_dir1_threshold_500": 0.057416386657913245,
      "scr_metric_threshold_500": 0.7835499957812297,
      "scr_dir2_threshold_500": 0.7835499957812297
    },
    {
      "dataset_name": "canrager/amazon_reviews_mcauley_1and5_scr_Software_Electronics_results",
      "scr_dir1_threshold_2": 0.2445652878126652,
      "scr_metric_threshold_2": 0.0693878335488021,
      "scr_dir2_threshold_2": 0.0693878335488021,
      "scr_dir1_threshold_5": 0.3043480936881278,
      "scr_metric_threshold_5": 0.11836752369223763,
      "scr_dir2_threshold_5": 0.11836752369223763,
      "scr_dir1_threshold_10": 0.3315219785630617,
      "scr_metric_threshold_10": 0.15102040717026832,
      "scr_dir2_threshold_10": 0.15102040717026832,
      "scr_dir1_threshold_20": 0.39130446050026435,
      "scr_metric_threshold_20": 0.22040824071907042,
      "scr_dir2_threshold_20": 0.22040824071907042,
      "scr_dir1_threshold_50": 0.40217388487493394,
      "scr_metric_threshold_50": 0.34693892445975083,
      "scr_dir2_threshold_50": 0.34693892445975083,
      "scr_dir1_threshold_100": 0.45108694243746694,
      "scr_metric_threshold_100": 0.4653062048677288,
      "scr_dir2_threshold_100": 0.4653062048677288,
      "scr_dir1_threshold_500": 0.2445652878126652,
      "scr_metric_threshold_500": 0.6122450320137758,
      "scr_dir2_threshold_500": 0.6122450320137758
    },
    {
      "dataset_name": "canrager/amazon_reviews_mcauley_1and5_scr_Pet_Supplies_Office_Products_results",
      "scr_dir1_threshold_2": 0.3523809929283339,
      "scr_metric_threshold_2": 0.26200870748256827,
      "scr_dir2_threshold_2": 0.26200870748256827,
      "scr_dir1_threshold_5": 0.45238085101249853,
      "scr_metric_threshold_5": 0.353711872228482,
      "scr_dir2_threshold_5": 0.353711872228482,
      "scr_dir1_threshold_10": 0.5190475176791652,
      "scr_metric_threshold_10": 0.46288205856194575,
      "scr_dir2_threshold_10": 0.46288205856194575,
      "scr_dir1_threshold_20": 0.5952381560591675,
      "scr_metric_threshold_20": 0.5109168884922187,
      "scr_dir2_threshold_20": 0.5109168884922187,
      "scr_dir1_threshold_50": 0.6333334752491687,
      "scr_metric_threshold_50": 0.39301319136498003,
      "scr_dir2_threshold_50": 0.39301319136498003,
      "scr_dir1_threshold_100": 0.5238096454516684,
      "scr_metric_threshold_100": 0.5065501330953313,
      "scr_dir2_threshold_100": 0.5065501330953313,
      "scr_dir1_threshold_500": 0.32380964545166846,
      "scr_metric_threshold_500": 0.3056767820159537,
      "scr_dir2_threshold_500": 0.3056767820159537
    },
    {
      "dataset_name": "canrager/amazon_reviews_mcauley_1and5_scr_Industrial_and_Scientific_Toys_and_Games_results",
      "scr_dir1_threshold_2": 0.14159293669169445,
      "scr_metric_threshold_2": 0.14159293669169445,
      "scr_dir2_threshold_2": 0.06829268150851518,
      "scr_dir1_threshold_5": 0.2035398300107262,
      "scr_metric_threshold_5": 0.2035398300107262,
      "scr_dir2_threshold_5": 0.11219514038930269,
      "scr_dir1_threshold_10": 0.2345134085389188,
      "scr_metric_threshold_10": 0.2345134085389188,
      "scr_dir2_threshold_10": 0.14634133576636604,
      "scr_dir1_threshold_20": 0.27876115180431954,
      "scr_metric_threshold_20": 0.27876115180431954,
      "scr_dir2_threshold_20": 0.19024379464715355,
      "scr_dir1_threshold_50": 0.3584071951769823,
      "scr_metric_threshold_50": 0.3584071951769823,
      "scr_dir2_threshold_50": 0.2585364761556687,
      "scr_dir1_threshold_100": 0.38495578838875205,
      "scr_metric_threshold_100": 0.38495578838875205,
      "scr_dir2_threshold_100": 0.38048788004869555,
      "scr_dir1_threshold_500": 0.455752388603276,
      "scr_metric_threshold_500": 0.455752388603276,
      "scr_dir2_threshold_500": 0.39024385279803125
    }
  ],
  "sae_bench_commit_hash": "c5c90a136c8db8e704e401328a49b19932ad56c9",
  "sae_lens_id": "blocks.12.hook_resid_post",
  "sae_lens_release_id": "suchitg/sae-compression-gemma-2-2b-pruned-sae-pile-0.25",
  "sae_lens_version": "5.6.1",
  "sae_cfg_dict": {
    "architecture": "jumprelu",
    "d_in": 2304,
    "d_sae": 16384,
    "activation_fn_str": "relu",
    "apply_b_dec_to_input": false,
    "finetuning_scaling_factor": false,
    "context_size": 1024,
    "model_name": "gemma-2-2b",
    "hook_name": "blocks.12.hook_resid_post",
    "hook_layer": 12,
    "hook_head_index": null,
    "prepend_bos": true,
    "dataset_path": "monology/pile-uncopyrighted",
    "dataset_trust_remote_code": true,
    "normalize_activations": null,
    "dtype": "torch.bfloat16",
    "device": "cuda",
    "sae_lens_training_version": null,
    "activation_fn_kwargs": {},
    "neuronpedia_id": "gemma-2-2b/12-gemmascope-res-16k",
    "model_from_pretrained_kwargs": {},
    "seqpos_slice": [
      null
    ]
  },
  "eval_result_unstructured": null
}
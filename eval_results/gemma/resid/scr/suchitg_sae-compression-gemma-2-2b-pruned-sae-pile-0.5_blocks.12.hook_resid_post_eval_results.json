{
  "eval_type_id": "scr",
  "eval_config": {
    "random_seed": 42,
    "dataset_names": [
      "LabHC/bias_in_bios_class_set1",
      "canrager/amazon_reviews_mcauley_1and5"
    ],
    "perform_scr": true,
    "early_stopping_patience": 20,
    "train_set_size": 4000,
    "test_set_size": 1000,
    "context_length": 128,
    "probe_train_batch_size": 16,
    "probe_test_batch_size": 500,
    "probe_epochs": 20,
    "probe_lr": 0.001,
    "probe_l1_penalty": 0.001,
    "sae_batch_size": 125,
    "llm_batch_size": 32,
    "llm_dtype": "bfloat16",
    "lower_vram_usage": false,
    "model_name": "gemma-2-2b",
    "n_values": [
      2,
      5,
      10,
      20,
      50,
      100,
      500
    ],
    "column1_vals_lookup": {
      "LabHC/bias_in_bios_class_set1": [
        [
          "professor",
          "nurse"
        ],
        [
          "architect",
          "journalist"
        ],
        [
          "surgeon",
          "psychologist"
        ],
        [
          "attorney",
          "teacher"
        ]
      ],
      "canrager/amazon_reviews_mcauley_1and5": [
        [
          "Books",
          "CDs_and_Vinyl"
        ],
        [
          "Software",
          "Electronics"
        ],
        [
          "Pet_Supplies",
          "Office_Products"
        ],
        [
          "Industrial_and_Scientific",
          "Toys_and_Games"
        ]
      ]
    }
  },
  "eval_id": "31a011cf-768c-4ec3-9a40-d701c9e6faab",
  "datetime_epoch_millis": 1742891236712,
  "eval_result_metrics": {
    "scr_metrics": {
      "scr_dir1_threshold_2": 0.26233905687395837,
      "scr_metric_threshold_2": 0.09999267056426757,
      "scr_dir2_threshold_2": 0.0896106420727032,
      "scr_dir1_threshold_5": 0.28696148799055093,
      "scr_metric_threshold_5": 0.14331707721989526,
      "scr_dir2_threshold_5": 0.1361537678879771,
      "scr_dir1_threshold_10": 0.29034101423239606,
      "scr_metric_threshold_10": 0.22186740435477392,
      "scr_dir2_threshold_10": 0.2127749951405303,
      "scr_dir1_threshold_20": 0.2189859882230869,
      "scr_metric_threshold_20": 0.2686686186800615,
      "scr_dir2_threshold_20": 0.26683663996126544,
      "scr_dir1_threshold_50": 0.16561856774382372,
      "scr_metric_threshold_50": 0.3396621649135318,
      "scr_dir2_threshold_50": 0.34155886797085816,
      "scr_dir1_threshold_100": 0.145522097631668,
      "scr_metric_threshold_100": 0.305846605726821,
      "scr_dir2_threshold_100": 0.3194851519252834,
      "scr_dir1_threshold_500": 0.13192378945722621,
      "scr_metric_threshold_500": 0.30399162368620264,
      "scr_dir2_threshold_500": 0.31558778677060917
    }
  },
  "eval_result_details": [
    {
      "dataset_name": "LabHC/bias_in_bios_class_set1_scr_professor_nurse_results",
      "scr_dir1_threshold_2": 0.45454604566582174,
      "scr_metric_threshold_2": 0.03186262765328328,
      "scr_dir2_threshold_2": 0.03186262765328328,
      "scr_dir1_threshold_5": 0.4909092091331643,
      "scr_metric_threshold_5": 0.049019562011037375,
      "scr_dir2_threshold_5": 0.049019562011037375,
      "scr_dir1_threshold_10": 0.5272734563211802,
      "scr_metric_threshold_10": 0.044117518156042625,
      "scr_dir2_threshold_10": 0.044117518156042625,
      "scr_dir1_threshold_20": 0.3818186350104633,
      "scr_metric_threshold_20": 0.049019562011037375,
      "scr_dir2_threshold_20": 0.049019562011037375,
      "scr_dir1_threshold_50": 0.32727280608877624,
      "scr_metric_threshold_50": 0.08088233575413901,
      "scr_dir2_threshold_50": 0.08088233575413901,
      "scr_dir1_threshold_100": 0.3818186350104633,
      "scr_metric_threshold_100": 0.1127449634074223,
      "scr_dir2_threshold_100": 0.1127449634074223,
      "scr_dir1_threshold_500": 0.05454582892168709,
      "scr_metric_threshold_500": 0.19607839413396788,
      "scr_dir2_threshold_500": 0.19607839413396788
    },
    {
      "dataset_name": "LabHC/bias_in_bios_class_set1_scr_architect_journalist_results",
      "scr_dir1_threshold_2": 0.24390288224741447,
      "scr_metric_threshold_2": 0.17127079555560165,
      "scr_dir2_threshold_2": 0.17127079555560165,
      "scr_dir1_threshold_5": 0.24390288224741447,
      "scr_metric_threshold_5": 0.2071824246013693,
      "scr_dir2_threshold_5": 0.2071824246013693,
      "scr_dir1_threshold_10": 0.2560978446383453,
      "scr_metric_threshold_10": 0.25414363367319504,
      "scr_dir2_threshold_10": 0.25414363367319504,
      "scr_dir1_threshold_20": 0.23170719297072395,
      "scr_metric_threshold_20": 0.30939235707195023,
      "scr_dir2_threshold_20": 0.30939235707195023,
      "scr_dir1_threshold_50": -0.4390244611595863,
      "scr_metric_threshold_50": 0.4254144292287967,
      "scr_dir2_threshold_50": 0.4254144292287967,
      "scr_dir1_threshold_100": -0.5609755388404137,
      "scr_metric_threshold_100": 0.18784533024838174,
      "scr_dir2_threshold_100": 0.18784533024838174,
      "scr_dir1_threshold_500": 0.048780576449482894,
      "scr_metric_threshold_500": 0.2872928677124481,
      "scr_dir2_threshold_500": 0.2872928677124481
    },
    {
      "dataset_name": "LabHC/bias_in_bios_class_set1_scr_surgeon_psychologist_results",
      "scr_dir1_threshold_2": 0.42857142857142855,
      "scr_metric_threshold_2": 0.025773170536871923,
      "scr_dir2_threshold_2": 0.025773170536871923,
      "scr_dir1_threshold_5": 0.42857142857142855,
      "scr_metric_threshold_5": 0.05927835368289101,
      "scr_dir2_threshold_5": 0.05927835368289101,
      "scr_dir1_threshold_10": 0.5079364028137167,
      "scr_metric_threshold_10": 0.10824740842699046,
      "scr_dir2_threshold_10": 0.10824740842699046,
      "scr_dir1_threshold_20": 0.20634931147199764,
      "scr_metric_threshold_20": 0.13144329263421797,
      "scr_dir2_threshold_20": 0.13144329263421797,
      "scr_dir1_threshold_50": 0.20634931147199764,
      "scr_metric_threshold_50": 0.21907210318362533,
      "scr_dir2_threshold_50": 0.21907210318362533,
      "scr_dir1_threshold_100": -0.04761936298742153,
      "scr_metric_threshold_100": 0.08247423789011854,
      "scr_dir2_threshold_100": 0.08247423789011854,
      "scr_dir1_threshold_500": -0.1269843372297096,
      "scr_metric_threshold_500": 0.06701036629203817,
      "scr_dir2_threshold_500": 0.06701036629203817
    },
    {
      "dataset_name": "LabHC/bias_in_bios_class_set1_scr_attorney_teacher_results",
      "scr_dir1_threshold_2": 0.23148172676799453,
      "scr_metric_threshold_2": 0.06722686129280975,
      "scr_dir2_threshold_2": 0.06722686129280975,
      "scr_dir1_threshold_5": 0.19444462840932922,
      "scr_metric_threshold_5": 0.12605046927388852,
      "scr_dir2_threshold_5": 0.12605046927388852,
      "scr_dir1_threshold_10": 0.018518825126659824,
      "scr_metric_threshold_10": 0.16526615227467692,
      "scr_dir2_threshold_10": 0.16526615227467692,
      "scr_dir1_threshold_20": -0.018518273232005476,
      "scr_metric_threshold_20": 0.2100840041499526,
      "scr_dir2_threshold_20": 0.2100840041499526,
      "scr_dir1_threshold_50": 0.12037043169199863,
      "scr_metric_threshold_50": 0.24649860271349733,
      "scr_dir2_threshold_50": 0.24649860271349733,
      "scr_dir1_threshold_100": 0.5555553715906708,
      "scr_metric_threshold_100": 0.29691879042305275,
      "scr_dir2_threshold_100": 0.29691879042305275,
      "scr_dir1_threshold_500": 0.34259246994933606,
      "scr_metric_threshold_500": 0.04481785187527569,
      "scr_dir2_threshold_500": 0.04481785187527569
    },
    {
      "dataset_name": "canrager/amazon_reviews_mcauley_1and5_scr_Books_CDs_and_Vinyl_results",
      "scr_dir1_threshold_2": 0.057416386657913245,
      "scr_metric_threshold_2": 0.06926420777301313,
      "scr_dir2_threshold_2": 0.06926420777301313,
      "scr_dir1_threshold_5": 0.08133980980128552,
      "scr_metric_threshold_5": 0.16017315793914494,
      "scr_dir2_threshold_5": 0.16017315793914494,
      "scr_dir1_threshold_10": 0.05263147387747908,
      "scr_metric_threshold_10": 0.4848486803248169,
      "scr_dir2_threshold_10": 0.4848486803248169,
      "scr_dir1_threshold_20": 0.01435416796190322,
      "scr_metric_threshold_20": 0.5974026309128257,
      "scr_dir2_threshold_20": 0.5974026309128257,
      "scr_dir1_threshold_50": 0.07655518221055099,
      "scr_metric_threshold_50": 0.6623378902072151,
      "scr_dir2_threshold_50": 0.6623378902072151,
      "scr_dir1_threshold_100": 0.0,
      "scr_metric_threshold_100": 0.6839828906290921,
      "scr_dir2_threshold_100": 0.6839828906290921,
      "scr_dir1_threshold_500": -0.04306221869601003,
      "scr_metric_threshold_500": 0.774891840795224,
      "scr_dir2_threshold_500": 0.774891840795224
    },
    {
      "dataset_name": "canrager/amazon_reviews_mcauley_1and5_scr_Software_Electronics_results",
      "scr_dir1_threshold_2": 0.2173914029377313,
      "scr_metric_threshold_2": 0.05714285019187829,
      "scr_dir2_threshold_2": 0.05714285019187829,
      "scr_dir1_threshold_5": 0.28260892100052865,
      "scr_metric_threshold_5": 0.10612254033531383,
      "scr_dir2_threshold_5": 0.10612254033531383,
      "scr_dir1_threshold_10": 0.29891305756253306,
      "scr_metric_threshold_10": 0.13061226376490176,
      "scr_dir2_threshold_10": 0.13061226376490176,
      "scr_dir1_threshold_20": 0.25543503612559476,
      "scr_metric_threshold_20": 0.21632666069484904,
      "scr_dir2_threshold_20": 0.21632666069484904,
      "scr_dir1_threshold_50": 0.3043480936881278,
      "scr_metric_threshold_50": 0.2938776542920939,
      "scr_dir2_threshold_50": 0.2938776542920939,
      "scr_dir1_threshold_100": 0.3369566907503965,
      "scr_metric_threshold_100": 0.4163265147242933,
      "scr_dir2_threshold_100": 0.4163265147242933,
      "scr_dir1_threshold_500": 0.17391305756253303,
      "scr_metric_threshold_500": 0.5346940384165308,
      "scr_dir2_threshold_500": 0.5346940384165308
    },
    {
      "dataset_name": "canrager/amazon_reviews_mcauley_1and5_scr_Pet_Supplies_Office_Products_results",
      "scr_dir1_threshold_2": 0.32380964545166846,
      "scr_metric_threshold_2": 0.235807914818988,
      "scr_dir2_threshold_2": 0.235807914818988,
      "scr_dir1_threshold_5": 0.41904765959500057,
      "scr_metric_threshold_5": 0.28384274474926097,
      "scr_dir2_threshold_5": 0.28384274474926097,
      "scr_dir1_threshold_10": 0.4666666666666667,
      "scr_metric_threshold_10": 0.39301319136498003,
      "scr_dir2_threshold_10": 0.39301319136498003,
      "scr_dir1_threshold_20": 0.49047617020249973,
      "scr_metric_threshold_20": 0.44541477669214047,
      "scr_dir2_threshold_20": 0.44541477669214047,
      "scr_dir1_threshold_50": 0.4857143262616673,
      "scr_metric_threshold_50": 0.5458514522318293,
      "scr_dir2_threshold_50": 0.5458514522318293,
      "scr_dir1_threshold_100": 0.2904761702024997,
      "scr_metric_threshold_100": 0.45851530316505823,
      "scr_dir2_threshold_100": 0.45851530316505823,
      "scr_dir1_threshold_500": 0.36666680858250206,
      "scr_metric_threshold_500": 0.2882095001461485,
      "scr_dir2_threshold_500": 0.2882095001461485
    },
    {
      "dataset_name": "canrager/amazon_reviews_mcauley_1and5_scr_Industrial_and_Scientific_Toys_and_Games_results",
      "scr_dir1_threshold_2": 0.14159293669169445,
      "scr_metric_threshold_2": 0.14159293669169445,
      "scr_dir2_threshold_2": 0.058536708759179476,
      "scr_dir1_threshold_5": 0.15486736516625607,
      "scr_metric_threshold_5": 0.15486736516625607,
      "scr_dir2_threshold_5": 0.0975608905109107,
      "scr_dir1_threshold_10": 0.19469038685258744,
      "scr_metric_threshold_10": 0.19469038685258744,
      "scr_dir2_threshold_10": 0.12195111313863838,
      "scr_dir1_threshold_20": 0.19026566527351804,
      "scr_metric_threshold_20": 0.19026566527351804,
      "scr_dir2_threshold_20": 0.17560983552315002,
      "scr_dir1_threshold_50": 0.24336285169705757,
      "scr_metric_threshold_50": 0.24336285169705757,
      "scr_dir2_threshold_50": 0.2585364761556687,
      "scr_dir1_threshold_100": 0.20796481532714906,
      "scr_metric_threshold_100": 0.20796481532714906,
      "scr_dir2_threshold_100": 0.3170731849148482,
      "scr_dir1_threshold_500": 0.23893813011798817,
      "scr_metric_threshold_500": 0.23893813011798817,
      "scr_dir2_threshold_500": 0.3317074347932402
    }
  ],
  "sae_bench_commit_hash": "c5c90a136c8db8e704e401328a49b19932ad56c9",
  "sae_lens_id": "blocks.12.hook_resid_post",
  "sae_lens_release_id": "suchitg/sae-compression-gemma-2-2b-pruned-sae-pile-0.5",
  "sae_lens_version": "5.6.1",
  "sae_cfg_dict": {
    "architecture": "jumprelu",
    "d_in": 2304,
    "d_sae": 16384,
    "activation_fn_str": "relu",
    "apply_b_dec_to_input": false,
    "finetuning_scaling_factor": false,
    "context_size": 1024,
    "model_name": "gemma-2-2b",
    "hook_name": "blocks.12.hook_resid_post",
    "hook_layer": 12,
    "hook_head_index": null,
    "prepend_bos": true,
    "dataset_path": "monology/pile-uncopyrighted",
    "dataset_trust_remote_code": true,
    "normalize_activations": null,
    "dtype": "torch.bfloat16",
    "device": "cuda",
    "sae_lens_training_version": null,
    "activation_fn_kwargs": {},
    "neuronpedia_id": "gemma-2-2b/12-gemmascope-res-16k",
    "model_from_pretrained_kwargs": {},
    "seqpos_slice": [
      null
    ]
  },
  "eval_result_unstructured": null
}
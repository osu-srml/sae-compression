\begin{table}
\caption{mlp output - core}
\label{tab:mlp_core}
\begin{tabular}{llllll}
\toprule
metric & prunedBest & pretrained & trained & pruned50 & pruned25 \\
\midrule
metric & core & core & core & core & core \\
kl_div_score & -1.000 & -1.000 & -1.000 & -1.000 & -1.000 \\
kl_div_with_ablation & -1.000 & -1.000 & -1.000 & -1.000 & -1.000 \\
kl_div_with_sae & -1.000 & -1.000 & -1.000 & -1.000 & -1.000 \\
ce_loss_score & 0.717 & 0.774 & 0.796 & 0.682 & 0.705 \\
ce_loss_with_ablation & 3.947 & 3.947 & 3.941 & 3.654 & 3.654 \\
ce_loss_with_sae & 3.873 & 3.867 & 3.857 & 3.596 & 3.594 \\
ce_loss_without_sae & 3.844 & 3.844 & 3.836 & 3.568 & 3.568 \\
explained_variance & 0.653 & 0.747 & 0.774 & 0.597 & 0.659 \\
explained_variance_legacy & 0.587 & 0.698 & 0.718 & 0.543 & 0.614 \\
mse & 0.146 & 0.107 & 0.097 & 0.191 & 0.162 \\
cossim & 0.818 & 0.858 & 0.878 & 0.780 & 0.823 \\
l2_norm_in & 28.969 & 28.969 & 29.094 & 30.688 & 30.688 \\
l2_norm_out & 28.406 & 24.969 & 22.219 & 28.391 & 30.172 \\
l2_ratio & 0.980 & 0.859 & 0.751 & 0.923 & 0.982 \\
relative_reconstruction_bias & 1.193 & 1.003 & 0.883 & 1.183 & 1.191 \\
l0 & 32.000 & 32.000 & 155.704 & 32.000 & 32.000 \\
l1 & 99.312 & 77.562 & 68.250 & 93.500 & 101.500 \\
total_tokens_eval_reconstruction & 409600 & 409600 & 409600 & 409600 & 409600 \\
total_tokens_eval_sparsity_variance & 4096000 & 4096000 & 4096000 & 4096000 & 4096000 \\
freq_over_1_percent & 0.011 & 0.009 & 0.243 & 0.005 & 0.012 \\
freq_over_10_percent & 0.000 & 0.000 & 0.001 & 0.000 & 0.000 \\
normalized_freq_over_1_percent & 0.200 & 0.175 & 0.819 & 0.109 & 0.216 \\
normalized_freq_over_10_percent & 0.030 & 0.045 & 0.027 & 0.027 & 0.032 \\
average_max_encoder_cosine_sim & 0.387 & 0.438 & 0.705 & 0.342 & 0.398 \\
average_max_decoder_cosine_sim & 0.286 & 0.290 & 0.391 & 0.273 & 0.287 \\
frac_alive & 0.999 & 1.000 & 1.000 & 0.991 & 0.999 \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}
\caption{mlp output - core}
\label{tab:mlp_core}
\begin{tabular}{llllll}
\toprule
metric & pretrained & pruned50 & pruned25 & trained & prunedBest \\
\midrule
metric & core & core & core & core & core \\
kl_div_score & -1.000 & -1.000 & -1.000 & -1.000 & -1.000 \\
kl_div_with_ablation & -1.000 & -1.000 & -1.000 & -1.000 & -1.000 \\
kl_div_with_sae & -1.000 & -1.000 & -1.000 & -1.000 & -1.000 \\
ce_loss_score & 0.745 & 0.655 & 0.793 & 0.796 & 0.673 \\
ce_loss_with_ablation & 3.951 & 3.682 & 3.682 & 3.941 & 3.951 \\
ce_loss_with_sae & 3.871 & 3.607 & 3.592 & 3.857 & 3.879 \\
ce_loss_without_sae & 3.844 & 3.568 & 3.568 & 3.836 & 3.844 \\
explained_variance & 0.919 & 0.860 & 0.901 & 0.937 & 0.909 \\
explained_variance_legacy & 0.780 & 0.667 & 0.760 & 0.818 & 0.756 \\
mse & 0.239 & 0.465 & 0.327 & 0.190 & 0.269 \\
cossim & 0.951 & 0.915 & 0.940 & 0.964 & 0.945 \\
l2_norm_in & 75.750 & 80.125 & 80.125 & 76.188 & 75.750 \\
l2_norm_out & 72.250 & 75.000 & 75.688 & 68.438 & 71.625 \\
l2_ratio & 0.950 & 0.931 & 0.940 & 0.891 & 0.942 \\
relative_reconstruction_bias & 1.000 & 1.014 & 1.000 & 0.935 & 0.996 \\
l0 & 32.000 & 32.000 & 32.000 & 379.207 & 32.000 \\
l1 & 51.750 & 52.438 & 55.406 & 190.875 & 52.219 \\
total_tokens_eval_reconstruction & 409600 & 409600 & 409600 & 409600 & 409600 \\
total_tokens_eval_sparsity_variance & 4096000 & 4096000 & 4096000 & 4096000 & 4096000 \\
freq_over_1_percent & 0.010 & 0.011 & 0.014 & 0.293 & 0.014 \\
freq_over_10_percent & 0.000 & 0.000 & 0.000 & 0.016 & 0.000 \\
normalized_freq_over_1_percent & 0.211 & 0.268 & 0.287 & 0.985 & 0.294 \\
normalized_freq_over_10_percent & 0.029 & 0.044 & 0.029 & 0.150 & 0.029 \\
average_max_encoder_cosine_sim & 0.499 & -1.000 & 0.490 & 0.730 & 0.490 \\
average_max_decoder_cosine_sim & 0.297 & 0.285 & 0.295 & 0.228 & 0.295 \\
frac_alive & 1.000 & 0.969 & 1.000 & 0.566 & 1.000 \\
\bottomrule
\end{tabular}
\end{table}

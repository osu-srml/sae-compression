\begin{table}
\caption{mlp output - core}
\label{tab:mlp_core}
\begin{tabular}{llllll}
\toprule
metric & prunedBest & trained & pruned50 & pruned25 & pretrained \\
\midrule
metric & core & core & core & core & core \\
kl_div_score & -1.000 & -1.000 & -1.000 & -1.000 & -1.000 \\
kl_div_with_ablation & -1.000 & -1.000 & -1.000 & -1.000 & -1.000 \\
kl_div_with_sae & -1.000 & -1.000 & -1.000 & -1.000 & -1.000 \\
ce_loss_score & 0.766 & 0.676 & 0.697 & 0.758 & 0.805 \\
ce_loss_with_ablation & 3.994 & 3.980 & 3.633 & 3.633 & 3.994 \\
ce_loss_with_sae & 3.879 & 3.883 & 3.588 & 3.584 & 3.873 \\
ce_loss_without_sae & 3.844 & 3.836 & 3.568 & 3.568 & 3.844 \\
explained_variance & 0.868 & 0.575 & 0.809 & 0.869 & 0.896 \\
explained_variance_legacy & 0.661 & 0.505 & 0.654 & 0.739 & 0.691 \\
mse & 0.029 & 0.035 & 0.046 & 0.032 & 0.023 \\
cossim & 0.822 & 0.750 & 0.810 & 0.861 & 0.838 \\
l2_norm_in & 13.773 & 12.602 & 15.672 & 15.672 & 13.773 \\
l2_norm_out & 12.266 & 7.551 & 13.414 & 14.492 & 11.773 \\
l2_ratio & 0.871 & 0.583 & 0.845 & 0.916 & 0.835 \\
relative_reconstruction_bias & 1.052 & 0.815 & 1.042 & 1.054 & 1.000 \\
l0 & 32.000 & 28.180 & 32.000 & 32.000 & 31.949 \\
l1 & 91.312 & 15.250 & 87.812 & 93.562 & 84.500 \\
total_tokens_eval_reconstruction & 409600 & 409600 & 409600 & 409600 & 409600 \\
total_tokens_eval_sparsity_variance & 4096000 & 4096000 & 4096000 & 4096000 & 4096000 \\
freq_over_1_percent & 0.011 & 0.013 & 0.004 & 0.009 & 0.010 \\
freq_over_10_percent & 0.000 & 0.001 & 0.000 & 0.000 & 0.000 \\
normalized_freq_over_1_percent & 0.239 & 0.457 & 0.108 & 0.181 & 0.212 \\
normalized_freq_over_10_percent & 0.056 & 0.198 & 0.043 & 0.041 & 0.061 \\
average_max_encoder_cosine_sim & 0.373 & 0.752 & -1.000 & 0.373 & 0.386 \\
average_max_decoder_cosine_sim & 0.315 & 0.482 & 0.299 & 0.315 & 0.318 \\
frac_alive & 1.000 & 0.998 & 0.990 & 1.000 & 1.000 \\
\bottomrule
\end{tabular}
\end{table}

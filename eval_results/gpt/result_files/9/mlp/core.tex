\begin{table}
\caption{mlp output - core}
\label{tab:mlp_core}
\begin{tabular}{llllll}
\toprule
metric & prunedBest & pretrained & pruned25 & pruned50 & trained \\
\midrule
metric & core & core & core & core & core \\
kl_div_score & -1.000 & -1.000 & -1.000 & -1.000 & -1.000 \\
kl_div_with_ablation & -1.000 & -1.000 & -1.000 & -1.000 & -1.000 \\
kl_div_with_sae & -1.000 & -1.000 & -1.000 & -1.000 & -1.000 \\
ce_loss_score & 0.731 & 0.788 & 0.717 & 0.674 & 0.804 \\
ce_loss_with_ablation & 3.945 & 3.945 & 3.658 & 3.658 & 3.936 \\
ce_loss_with_sae & 3.871 & 3.865 & 3.594 & 3.598 & 3.855 \\
ce_loss_without_sae & 3.844 & 3.844 & 3.568 & 3.568 & 3.836 \\
explained_variance & 0.715 & 0.793 & 0.719 & 0.654 & 0.835 \\
explained_variance_legacy & 0.622 & 0.723 & 0.643 & 0.562 & 0.766 \\
mse & 0.213 & 0.155 & 0.237 & 0.292 & 0.125 \\
cossim & 0.842 & 0.879 & 0.845 & 0.803 & 0.907 \\
l2_norm_in & 38.156 & 38.156 & 40.531 & 40.531 & 38.344 \\
l2_norm_out & 36.750 & 33.719 & 39.031 & 37.438 & 31.094 \\
l2_ratio & 0.961 & 0.878 & 0.961 & 0.920 & 0.795 \\
relative_reconstruction_bias & 1.133 & 1.001 & 1.130 & 1.138 & 0.908 \\
l0 & 32.000 & 32.000 & 32.000 & 32.000 & 228.514 \\
l1 & 91.125 & 74.125 & 93.062 & 87.000 & 104.062 \\
total_tokens_eval_reconstruction & 409600 & 409600 & 409600 & 409600 & 409600 \\
total_tokens_eval_sparsity_variance & 4096000 & 4096000 & 4096000 & 4096000 & 4096000 \\
freq_over_1_percent & 0.011 & 0.011 & 0.011 & 0.005 & 0.292 \\
freq_over_10_percent & 0.000 & 0.000 & 0.000 & 0.000 & 0.001 \\
normalized_freq_over_1_percent & 0.196 & 0.216 & 0.206 & 0.112 & 0.881 \\
normalized_freq_over_10_percent & 0.023 & 0.026 & 0.023 & 0.023 & 0.025 \\
average_max_encoder_cosine_sim & 0.432 & 0.481 & 0.442 & -1.000 & 0.719 \\
average_max_decoder_cosine_sim & 0.284 & 0.288 & 0.285 & 0.271 & 0.348 \\
frac_alive & 0.999 & 1.000 & 1.000 & 0.990 & 0.944 \\
\bottomrule
\end{tabular}
\end{table}

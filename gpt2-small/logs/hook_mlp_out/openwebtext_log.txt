Using device: cuda:0
Model loaded
Loading pretrained SAE: gpt2-small-mlp-out-v5-32k/blocks.0.hook_mlp_out
Before pruning layer 0 - Val loss: 2.426, Test loss: 2.496
Layer 0 | Sparse Ratio: 0.99 | Val Loss: 9.055
Layer 0 | Sparse Ratio: 0.95 | Val Loss: 8.425
Layer 0 | Sparse Ratio: 0.85 | Val Loss: 5.445
Layer 0 | Sparse Ratio: 0.8 | Val Loss: 5.403
Layer 0 | Sparse Ratio: 0.75 | Val Loss: 4.523
Layer 0 | Sparse Ratio: 0.7 | Val Loss: 3.777
Layer 0 | Sparse Ratio: 0.65 | Val Loss: 3.517
Layer 0 | Sparse Ratio: 0.6 | Val Loss: 3.259
Layer 0 | Sparse Ratio: 0.55 | Val Loss: 3.127
Layer 0 | Sparse Ratio: 0.5 | Val Loss: 3.013
Layer 0 | Sparse Ratio: 0.45 | Val Loss: 2.929
Layer 0 | Sparse Ratio: 0.4 | Val Loss: 2.805
Layer 0 | Sparse Ratio: 0.35 | Val Loss: 2.837
Layer 0 | Sparse Ratio: 0.3 | Val Loss: 2.79
Layer 0 | Sparse Ratio: 0.25 | Val Loss: 2.764
Layer 0 best sparse ratio: 0.25 with val loss: 2.764
Layer 0 final test loss: 2.852
Loading pretrained SAE: gpt2-small-mlp-out-v5-32k/blocks.1.hook_mlp_out
Before pruning layer 1 - Val loss: 2.296, Test loss: 2.356
Layer 1 | Sparse Ratio: 0.99 | Val Loss: 2.543
Layer 1 | Sparse Ratio: 0.95 | Val Loss: 2.481
Layer 1 | Sparse Ratio: 0.85 | Val Loss: 2.545
Layer 1 | Sparse Ratio: 0.8 | Val Loss: 2.512
Layer 1 | Sparse Ratio: 0.75 | Val Loss: 2.485
Layer 1 | Sparse Ratio: 0.7 | Val Loss: 2.51
Layer 1 | Sparse Ratio: 0.65 | Val Loss: 2.447
Layer 1 | Sparse Ratio: 0.6 | Val Loss: 2.442
Layer 1 | Sparse Ratio: 0.55 | Val Loss: 2.414
Layer 1 | Sparse Ratio: 0.5 | Val Loss: 2.464
Layer 1 | Sparse Ratio: 0.45 | Val Loss: 2.401
Layer 1 | Sparse Ratio: 0.4 | Val Loss: 2.453
Layer 1 | Sparse Ratio: 0.35 | Val Loss: 2.438
Layer 1 | Sparse Ratio: 0.3 | Val Loss: 2.413
Layer 1 | Sparse Ratio: 0.25 | Val Loss: 2.385
Layer 1 best sparse ratio: 0.25 with val loss: 2.385
Layer 1 final test loss: 2.452
Loading pretrained SAE: gpt2-small-mlp-out-v5-32k/blocks.2.hook_mlp_out
Before pruning layer 2 - Val loss: 2.286, Test loss: 2.345
Layer 2 | Sparse Ratio: 0.99 | Val Loss: 2.668
Layer 2 | Sparse Ratio: 0.95 | Val Loss: 2.812
Layer 2 | Sparse Ratio: 0.85 | Val Loss: 2.677
Layer 2 | Sparse Ratio: 0.8 | Val Loss: 2.684
Layer 2 | Sparse Ratio: 0.75 | Val Loss: 2.659
Layer 2 | Sparse Ratio: 0.7 | Val Loss: 2.554
Layer 2 | Sparse Ratio: 0.65 | Val Loss: 2.594
Layer 2 | Sparse Ratio: 0.6 | Val Loss: 2.503
Layer 2 | Sparse Ratio: 0.55 | Val Loss: 2.443
Layer 2 | Sparse Ratio: 0.5 | Val Loss: 2.447
Layer 2 | Sparse Ratio: 0.45 | Val Loss: 2.416
Layer 2 | Sparse Ratio: 0.4 | Val Loss: 2.385
Layer 2 | Sparse Ratio: 0.35 | Val Loss: 2.36
Layer 2 | Sparse Ratio: 0.3 | Val Loss: 2.349
Layer 2 | Sparse Ratio: 0.25 | Val Loss: 2.297
Layer 2 best sparse ratio: 0.25 with val loss: 2.297
Layer 2 final test loss: 2.356
Loading pretrained SAE: gpt2-small-mlp-out-v5-32k/blocks.3.hook_mlp_out
Before pruning layer 3 - Val loss: 2.274, Test loss: 2.331
Layer 3 | Sparse Ratio: 0.99 | Val Loss: 2.436
Layer 3 | Sparse Ratio: 0.95 | Val Loss: 2.419
Layer 3 | Sparse Ratio: 0.85 | Val Loss: 2.376
Layer 3 | Sparse Ratio: 0.8 | Val Loss: 2.351
Layer 3 | Sparse Ratio: 0.75 | Val Loss: 2.339
Layer 3 | Sparse Ratio: 0.7 | Val Loss: 2.328
Layer 3 | Sparse Ratio: 0.65 | Val Loss: 2.314
Layer 3 | Sparse Ratio: 0.6 | Val Loss: 2.307
Layer 3 | Sparse Ratio: 0.55 | Val Loss: 2.297
Layer 3 | Sparse Ratio: 0.5 | Val Loss: 2.293
Layer 3 | Sparse Ratio: 0.45 | Val Loss: 2.286
Layer 3 | Sparse Ratio: 0.4 | Val Loss: 2.282
Layer 3 | Sparse Ratio: 0.35 | Val Loss: 2.28
Layer 3 | Sparse Ratio: 0.3 | Val Loss: 2.278
Layer 3 | Sparse Ratio: 0.25 | Val Loss: 2.274
Layer 3 best sparse ratio: 0.25 with val loss: 2.274
Layer 3 final test loss: 2.332
Loading pretrained SAE: gpt2-small-mlp-out-v5-32k/blocks.4.hook_mlp_out
Before pruning layer 4 - Val loss: 2.31, Test loss: 2.37
Layer 4 | Sparse Ratio: 0.99 | Val Loss: 2.455
Layer 4 | Sparse Ratio: 0.95 | Val Loss: 2.439
Layer 4 | Sparse Ratio: 0.85 | Val Loss: 2.41
Layer 4 | Sparse Ratio: 0.8 | Val Loss: 2.406
Layer 4 | Sparse Ratio: 0.75 | Val Loss: 2.398
Layer 4 | Sparse Ratio: 0.7 | Val Loss: 2.386
Layer 4 | Sparse Ratio: 0.65 | Val Loss: 2.37
Layer 4 | Sparse Ratio: 0.6 | Val Loss: 2.36
Layer 4 | Sparse Ratio: 0.55 | Val Loss: 2.349
Layer 4 | Sparse Ratio: 0.5 | Val Loss: 2.34
Layer 4 | Sparse Ratio: 0.45 | Val Loss: 2.329
Layer 4 | Sparse Ratio: 0.4 | Val Loss: 2.328
Layer 4 | Sparse Ratio: 0.35 | Val Loss: 2.323
Layer 4 | Sparse Ratio: 0.3 | Val Loss: 2.318
Layer 4 | Sparse Ratio: 0.25 | Val Loss: 2.312
Layer 4 best sparse ratio: 0.25 with val loss: 2.312
Layer 4 final test loss: 2.37
Loading pretrained SAE: gpt2-small-mlp-out-v5-32k/blocks.5.hook_mlp_out
Before pruning layer 5 - Val loss: 2.277, Test loss: 2.333
Layer 5 | Sparse Ratio: 0.99 | Val Loss: 2.384
Layer 5 | Sparse Ratio: 0.95 | Val Loss: 2.369
Layer 5 | Sparse Ratio: 0.85 | Val Loss: 2.349
Layer 5 | Sparse Ratio: 0.8 | Val Loss: 2.346
Layer 5 | Sparse Ratio: 0.75 | Val Loss: 2.338
Layer 5 | Sparse Ratio: 0.7 | Val Loss: 2.327
Layer 5 | Sparse Ratio: 0.65 | Val Loss: 2.318
Layer 5 | Sparse Ratio: 0.6 | Val Loss: 2.313
Layer 5 | Sparse Ratio: 0.55 | Val Loss: 2.307
Layer 5 | Sparse Ratio: 0.5 | Val Loss: 2.298
Layer 5 | Sparse Ratio: 0.45 | Val Loss: 2.293
Layer 5 | Sparse Ratio: 0.4 | Val Loss: 2.292
Layer 5 | Sparse Ratio: 0.35 | Val Loss: 2.286
Layer 5 | Sparse Ratio: 0.3 | Val Loss: 2.282
Layer 5 | Sparse Ratio: 0.25 | Val Loss: 2.281
Layer 5 best sparse ratio: 0.25 with val loss: 2.281
Layer 5 final test loss: 2.337
Loading pretrained SAE: gpt2-small-mlp-out-v5-32k/blocks.6.hook_mlp_out
Before pruning layer 6 - Val loss: 2.268, Test loss: 2.323
Layer 6 | Sparse Ratio: 0.99 | Val Loss: 2.345
Layer 6 | Sparse Ratio: 0.95 | Val Loss: 2.342
Layer 6 | Sparse Ratio: 0.85 | Val Loss: 2.317
Layer 6 | Sparse Ratio: 0.8 | Val Loss: 2.309
Layer 6 | Sparse Ratio: 0.75 | Val Loss: 2.306
Layer 6 | Sparse Ratio: 0.7 | Val Loss: 2.301
Layer 6 | Sparse Ratio: 0.65 | Val Loss: 2.293
Layer 6 | Sparse Ratio: 0.6 | Val Loss: 2.289
Layer 6 | Sparse Ratio: 0.55 | Val Loss: 2.287
Layer 6 | Sparse Ratio: 0.5 | Val Loss: 2.283
Layer 6 | Sparse Ratio: 0.45 | Val Loss: 2.281
Layer 6 | Sparse Ratio: 0.4 | Val Loss: 2.278
Layer 6 | Sparse Ratio: 0.35 | Val Loss: 2.271
Layer 6 | Sparse Ratio: 0.3 | Val Loss: 2.269
Layer 6 | Sparse Ratio: 0.25 | Val Loss: 2.271
Layer 6 best sparse ratio: 0.3 with val loss: 2.269
Layer 6 final test loss: 2.326
Loading pretrained SAE: gpt2-small-mlp-out-v5-32k/blocks.7.hook_mlp_out
Before pruning layer 7 - Val loss: 2.267, Test loss: 2.325
Layer 7 | Sparse Ratio: 0.99 | Val Loss: 2.352
Layer 7 | Sparse Ratio: 0.95 | Val Loss: 2.349
Layer 7 | Sparse Ratio: 0.85 | Val Loss: 2.325
Layer 7 | Sparse Ratio: 0.8 | Val Loss: 2.309
Layer 7 | Sparse Ratio: 0.75 | Val Loss: 2.305
Layer 7 | Sparse Ratio: 0.7 | Val Loss: 2.299
Layer 7 | Sparse Ratio: 0.65 | Val Loss: 2.291
Layer 7 | Sparse Ratio: 0.6 | Val Loss: 2.283
Layer 7 | Sparse Ratio: 0.55 | Val Loss: 2.276
Layer 7 | Sparse Ratio: 0.5 | Val Loss: 2.273
Layer 7 | Sparse Ratio: 0.45 | Val Loss: 2.27
Layer 7 | Sparse Ratio: 0.4 | Val Loss: 2.266
Layer 7 | Sparse Ratio: 0.35 | Val Loss: 2.265
Layer 7 | Sparse Ratio: 0.3 | Val Loss: 2.265
Layer 7 | Sparse Ratio: 0.25 | Val Loss: 2.266
Layer 7 best sparse ratio: 0.35 with val loss: 2.265
Layer 7 final test loss: 2.323
Loading pretrained SAE: gpt2-small-mlp-out-v5-32k/blocks.8.hook_mlp_out
Before pruning layer 8 - Val loss: 2.267, Test loss: 2.324
Layer 8 | Sparse Ratio: 0.99 | Val Loss: 2.343
Layer 8 | Sparse Ratio: 0.95 | Val Loss: 2.333
Layer 8 | Sparse Ratio: 0.85 | Val Loss: 2.313
Layer 8 | Sparse Ratio: 0.8 | Val Loss: 2.306
Layer 8 | Sparse Ratio: 0.75 | Val Loss: 2.301
Layer 8 | Sparse Ratio: 0.7 | Val Loss: 2.298
Layer 8 | Sparse Ratio: 0.65 | Val Loss: 2.295
Layer 8 | Sparse Ratio: 0.6 | Val Loss: 2.291
Layer 8 | Sparse Ratio: 0.55 | Val Loss: 2.286
Layer 8 | Sparse Ratio: 0.5 | Val Loss: 2.279
Layer 8 | Sparse Ratio: 0.45 | Val Loss: 2.274
Layer 8 | Sparse Ratio: 0.4 | Val Loss: 2.271
Layer 8 | Sparse Ratio: 0.35 | Val Loss: 2.27
Layer 8 | Sparse Ratio: 0.3 | Val Loss: 2.269
Layer 8 | Sparse Ratio: 0.25 | Val Loss: 2.27
Layer 8 best sparse ratio: 0.3 with val loss: 2.269
Layer 8 final test loss: 2.327
Loading pretrained SAE: gpt2-small-mlp-out-v5-32k/blocks.9.hook_mlp_out
Before pruning layer 9 - Val loss: 2.266, Test loss: 2.323
Layer 9 | Sparse Ratio: 0.99 | Val Loss: 2.352
Layer 9 | Sparse Ratio: 0.95 | Val Loss: 2.341
Layer 9 | Sparse Ratio: 0.85 | Val Loss: 2.322
Layer 9 | Sparse Ratio: 0.8 | Val Loss: 2.315
Layer 9 | Sparse Ratio: 0.75 | Val Loss: 2.314
Layer 9 | Sparse Ratio: 0.7 | Val Loss: 2.301
Layer 9 | Sparse Ratio: 0.65 | Val Loss: 2.297
Layer 9 | Sparse Ratio: 0.6 | Val Loss: 2.292
Layer 9 | Sparse Ratio: 0.55 | Val Loss: 2.285
Layer 9 | Sparse Ratio: 0.5 | Val Loss: 2.282
Layer 9 | Sparse Ratio: 0.45 | Val Loss: 2.278
Layer 9 | Sparse Ratio: 0.4 | Val Loss: 2.274
Layer 9 | Sparse Ratio: 0.35 | Val Loss: 2.268
Layer 9 | Sparse Ratio: 0.3 | Val Loss: 2.267
Layer 9 | Sparse Ratio: 0.25 | Val Loss: 2.267
Layer 9 best sparse ratio: 0.3 with val loss: 2.267
Layer 9 final test loss: 2.327
Loading pretrained SAE: gpt2-small-mlp-out-v5-32k/blocks.10.hook_mlp_out
Before pruning layer 10 - Val loss: 2.265, Test loss: 2.323
Layer 10 | Sparse Ratio: 0.99 | Val Loss: 2.309
Layer 10 | Sparse Ratio: 0.95 | Val Loss: 2.307
Layer 10 | Sparse Ratio: 0.85 | Val Loss: 2.302
Layer 10 | Sparse Ratio: 0.8 | Val Loss: 2.303
Layer 10 | Sparse Ratio: 0.75 | Val Loss: 2.305
Layer 10 | Sparse Ratio: 0.7 | Val Loss: 2.316
Layer 10 | Sparse Ratio: 0.65 | Val Loss: 2.307
Layer 10 | Sparse Ratio: 0.6 | Val Loss: 2.298
Layer 10 | Sparse Ratio: 0.55 | Val Loss: 2.291
Layer 10 | Sparse Ratio: 0.5 | Val Loss: 2.288
Layer 10 | Sparse Ratio: 0.45 | Val Loss: 2.285
Layer 10 | Sparse Ratio: 0.4 | Val Loss: 2.277
Layer 10 | Sparse Ratio: 0.35 | Val Loss: 2.274
Layer 10 | Sparse Ratio: 0.3 | Val Loss: 2.271
Layer 10 | Sparse Ratio: 0.25 | Val Loss: 2.27
Layer 10 best sparse ratio: 0.25 with val loss: 2.27
Layer 10 final test loss: 2.328
Loading pretrained SAE: gpt2-small-mlp-out-v5-32k/blocks.11.hook_mlp_out
Before pruning layer 11 - Val loss: 2.279, Test loss: 2.336
Layer 11 | Sparse Ratio: 0.99 | Val Loss: 2.425
Layer 11 | Sparse Ratio: 0.95 | Val Loss: 2.469
Layer 11 | Sparse Ratio: 0.85 | Val Loss: 2.404
Layer 11 | Sparse Ratio: 0.8 | Val Loss: 2.392
Layer 11 | Sparse Ratio: 0.75 | Val Loss: 2.387
Layer 11 | Sparse Ratio: 0.7 | Val Loss: 2.34
Layer 11 | Sparse Ratio: 0.65 | Val Loss: 2.332
Layer 11 | Sparse Ratio: 0.6 | Val Loss: 2.316
Layer 11 | Sparse Ratio: 0.55 | Val Loss: 2.314
Layer 11 | Sparse Ratio: 0.5 | Val Loss: 2.31
Layer 11 | Sparse Ratio: 0.45 | Val Loss: 2.3
Layer 11 | Sparse Ratio: 0.4 | Val Loss: 2.296
Layer 11 | Sparse Ratio: 0.35 | Val Loss: 2.292
Layer 11 | Sparse Ratio: 0.3 | Val Loss: 2.289
Layer 11 | Sparse Ratio: 0.25 | Val Loss: 2.286
Layer 11 best sparse ratio: 0.25 with val loss: 2.286
Layer 11 final test loss: 2.343

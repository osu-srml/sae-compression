Using device: cuda:3
Model loaded
Loading pretrained SAE: gpt2-small-mlp-out-v5-32k/blocks.0.hook_mlp_out
Before pruning layer 0 - Val loss: 1.698, Test loss: 1.687
Layer 0 | Sparse Ratio: 0.99 | Val Loss: 6.421
Layer 0 | Sparse Ratio: 0.95 | Val Loss: 7.202
Layer 0 | Sparse Ratio: 0.85 | Val Loss: 4.983
Layer 0 | Sparse Ratio: 0.8 | Val Loss: 3.218
Layer 0 | Sparse Ratio: 0.75 | Val Loss: 2.469
Layer 0 | Sparse Ratio: 0.7 | Val Loss: 2.26
Layer 0 | Sparse Ratio: 0.65 | Val Loss: 2.156
Layer 0 | Sparse Ratio: 0.6 | Val Loss: 2.085
Layer 0 | Sparse Ratio: 0.55 | Val Loss: 2.02
Layer 0 | Sparse Ratio: 0.5 | Val Loss: 1.964
Layer 0 | Sparse Ratio: 0.45 | Val Loss: 1.915
Layer 0 | Sparse Ratio: 0.4 | Val Loss: 1.893
Layer 0 | Sparse Ratio: 0.35 | Val Loss: 1.901
Layer 0 | Sparse Ratio: 0.3 | Val Loss: 1.862
Layer 0 | Sparse Ratio: 0.25 | Val Loss: 1.825
Layer 0 best sparse ratio: 0.25 with val loss: 1.825
Layer 0 final test loss: 1.802
Loading pretrained SAE: gpt2-small-mlp-out-v5-32k/blocks.1.hook_mlp_out
Before pruning layer 1 - Val loss: 1.681, Test loss: 1.672
Layer 1 | Sparse Ratio: 0.99 | Val Loss: 1.71
Layer 1 | Sparse Ratio: 0.95 | Val Loss: 1.716
Layer 1 | Sparse Ratio: 0.85 | Val Loss: 1.757
Layer 1 | Sparse Ratio: 0.8 | Val Loss: 1.757
Layer 1 | Sparse Ratio: 0.75 | Val Loss: 1.76
Layer 1 | Sparse Ratio: 0.7 | Val Loss: 1.736
Layer 1 | Sparse Ratio: 0.65 | Val Loss: 1.745
Layer 1 | Sparse Ratio: 0.6 | Val Loss: 1.758
Layer 1 | Sparse Ratio: 0.55 | Val Loss: 1.758
Layer 1 | Sparse Ratio: 0.5 | Val Loss: 1.752
Layer 1 | Sparse Ratio: 0.45 | Val Loss: 1.733
Layer 1 | Sparse Ratio: 0.4 | Val Loss: 1.728
Layer 1 | Sparse Ratio: 0.35 | Val Loss: 1.715
Layer 1 | Sparse Ratio: 0.3 | Val Loss: 1.715
Layer 1 | Sparse Ratio: 0.25 | Val Loss: 1.722
Layer 1 best sparse ratio: 0.99 with val loss: 1.71
Layer 1 final test loss: 1.695
Loading pretrained SAE: gpt2-small-mlp-out-v5-32k/blocks.2.hook_mlp_out
Before pruning layer 2 - Val loss: 1.681, Test loss: 1.679
Layer 2 | Sparse Ratio: 0.99 | Val Loss: 1.82
Layer 2 | Sparse Ratio: 0.95 | Val Loss: 1.92
Layer 2 | Sparse Ratio: 0.85 | Val Loss: 1.894
Layer 2 | Sparse Ratio: 0.8 | Val Loss: 1.912
Layer 2 | Sparse Ratio: 0.75 | Val Loss: 1.896
Layer 2 | Sparse Ratio: 0.7 | Val Loss: 1.853
Layer 2 | Sparse Ratio: 0.65 | Val Loss: 1.797
Layer 2 | Sparse Ratio: 0.6 | Val Loss: 1.794
Layer 2 | Sparse Ratio: 0.55 | Val Loss: 1.765
Layer 2 | Sparse Ratio: 0.5 | Val Loss: 1.75
Layer 2 | Sparse Ratio: 0.45 | Val Loss: 1.729
Layer 2 | Sparse Ratio: 0.4 | Val Loss: 1.744
Layer 2 | Sparse Ratio: 0.35 | Val Loss: 1.728
Layer 2 | Sparse Ratio: 0.3 | Val Loss: 1.713
Layer 2 | Sparse Ratio: 0.25 | Val Loss: 1.702
Layer 2 best sparse ratio: 0.25 with val loss: 1.702
Layer 2 final test loss: 1.699
Loading pretrained SAE: gpt2-small-mlp-out-v5-32k/blocks.3.hook_mlp_out
Before pruning layer 3 - Val loss: 1.678, Test loss: 1.673
Layer 3 | Sparse Ratio: 0.99 | Val Loss: 1.703
Layer 3 | Sparse Ratio: 0.95 | Val Loss: 1.702
Layer 3 | Sparse Ratio: 0.85 | Val Loss: 1.699
Layer 3 | Sparse Ratio: 0.8 | Val Loss: 1.696
Layer 3 | Sparse Ratio: 0.75 | Val Loss: 1.691
Layer 3 | Sparse Ratio: 0.7 | Val Loss: 1.687
Layer 3 | Sparse Ratio: 0.65 | Val Loss: 1.684
Layer 3 | Sparse Ratio: 0.6 | Val Loss: 1.682
Layer 3 | Sparse Ratio: 0.55 | Val Loss: 1.683
Layer 3 | Sparse Ratio: 0.5 | Val Loss: 1.683
Layer 3 | Sparse Ratio: 0.45 | Val Loss: 1.682
Layer 3 | Sparse Ratio: 0.4 | Val Loss: 1.681
Layer 3 | Sparse Ratio: 0.35 | Val Loss: 1.679
Layer 3 | Sparse Ratio: 0.3 | Val Loss: 1.681
Layer 3 | Sparse Ratio: 0.25 | Val Loss: 1.681
Layer 3 best sparse ratio: 0.35 with val loss: 1.679
Layer 3 final test loss: 1.674
Loading pretrained SAE: gpt2-small-mlp-out-v5-32k/blocks.4.hook_mlp_out
Before pruning layer 4 - Val loss: 1.684, Test loss: 1.682
Layer 4 | Sparse Ratio: 0.99 | Val Loss: 1.711
Layer 4 | Sparse Ratio: 0.95 | Val Loss: 1.706
Layer 4 | Sparse Ratio: 0.85 | Val Loss: 1.701
Layer 4 | Sparse Ratio: 0.8 | Val Loss: 1.701
Layer 4 | Sparse Ratio: 0.75 | Val Loss: 1.699
Layer 4 | Sparse Ratio: 0.7 | Val Loss: 1.696
Layer 4 | Sparse Ratio: 0.65 | Val Loss: 1.696
Layer 4 | Sparse Ratio: 0.6 | Val Loss: 1.69
Layer 4 | Sparse Ratio: 0.55 | Val Loss: 1.689
Layer 4 | Sparse Ratio: 0.5 | Val Loss: 1.688
Layer 4 | Sparse Ratio: 0.45 | Val Loss: 1.685
Layer 4 | Sparse Ratio: 0.4 | Val Loss: 1.683
Layer 4 | Sparse Ratio: 0.35 | Val Loss: 1.683
Layer 4 | Sparse Ratio: 0.3 | Val Loss: 1.684
Layer 4 | Sparse Ratio: 0.25 | Val Loss: 1.684
Layer 4 best sparse ratio: 0.4 with val loss: 1.683
Layer 4 final test loss: 1.679
Loading pretrained SAE: gpt2-small-mlp-out-v5-32k/blocks.5.hook_mlp_out
Before pruning layer 5 - Val loss: 1.681, Test loss: 1.671
Layer 5 | Sparse Ratio: 0.99 | Val Loss: 1.705
Layer 5 | Sparse Ratio: 0.95 | Val Loss: 1.702
Layer 5 | Sparse Ratio: 0.85 | Val Loss: 1.698
Layer 5 | Sparse Ratio: 0.8 | Val Loss: 1.693
Layer 5 | Sparse Ratio: 0.75 | Val Loss: 1.691
Layer 5 | Sparse Ratio: 0.7 | Val Loss: 1.688
Layer 5 | Sparse Ratio: 0.65 | Val Loss: 1.685
Layer 5 | Sparse Ratio: 0.6 | Val Loss: 1.685
Layer 5 | Sparse Ratio: 0.55 | Val Loss: 1.683
Layer 5 | Sparse Ratio: 0.5 | Val Loss: 1.685
Layer 5 | Sparse Ratio: 0.45 | Val Loss: 1.682
Layer 5 | Sparse Ratio: 0.4 | Val Loss: 1.68
Layer 5 | Sparse Ratio: 0.35 | Val Loss: 1.681
Layer 5 | Sparse Ratio: 0.3 | Val Loss: 1.681
Layer 5 | Sparse Ratio: 0.25 | Val Loss: 1.682
Layer 5 best sparse ratio: 0.4 with val loss: 1.68
Layer 5 final test loss: 1.669
Loading pretrained SAE: gpt2-small-mlp-out-v5-32k/blocks.6.hook_mlp_out
Before pruning layer 6 - Val loss: 1.679, Test loss: 1.676
Layer 6 | Sparse Ratio: 0.99 | Val Loss: 1.682
Layer 6 | Sparse Ratio: 0.95 | Val Loss: 1.68
Layer 6 | Sparse Ratio: 0.85 | Val Loss: 1.677
Layer 6 | Sparse Ratio: 0.8 | Val Loss: 1.679
Layer 6 | Sparse Ratio: 0.75 | Val Loss: 1.679
Layer 6 | Sparse Ratio: 0.7 | Val Loss: 1.681
Layer 6 | Sparse Ratio: 0.65 | Val Loss: 1.683
Layer 6 | Sparse Ratio: 0.6 | Val Loss: 1.682
Layer 6 | Sparse Ratio: 0.55 | Val Loss: 1.684
Layer 6 | Sparse Ratio: 0.5 | Val Loss: 1.681
Layer 6 | Sparse Ratio: 0.45 | Val Loss: 1.682
Layer 6 | Sparse Ratio: 0.4 | Val Loss: 1.683
Layer 6 | Sparse Ratio: 0.35 | Val Loss: 1.684
Layer 6 | Sparse Ratio: 0.3 | Val Loss: 1.682
Layer 6 | Sparse Ratio: 0.25 | Val Loss: 1.682
Layer 6 best sparse ratio: 0.85 with val loss: 1.677
Layer 6 final test loss: 1.665
Loading pretrained SAE: gpt2-small-mlp-out-v5-32k/blocks.7.hook_mlp_out
Before pruning layer 7 - Val loss: 1.68, Test loss: 1.674
Layer 7 | Sparse Ratio: 0.99 | Val Loss: 1.708
Layer 7 | Sparse Ratio: 0.95 | Val Loss: 1.703
Layer 7 | Sparse Ratio: 0.85 | Val Loss: 1.697
Layer 7 | Sparse Ratio: 0.8 | Val Loss: 1.695
Layer 7 | Sparse Ratio: 0.75 | Val Loss: 1.695
Layer 7 | Sparse Ratio: 0.7 | Val Loss: 1.693
Layer 7 | Sparse Ratio: 0.65 | Val Loss: 1.689
Layer 7 | Sparse Ratio: 0.6 | Val Loss: 1.69
Layer 7 | Sparse Ratio: 0.55 | Val Loss: 1.686
Layer 7 | Sparse Ratio: 0.5 | Val Loss: 1.683
Layer 7 | Sparse Ratio: 0.45 | Val Loss: 1.682
Layer 7 | Sparse Ratio: 0.4 | Val Loss: 1.681
Layer 7 | Sparse Ratio: 0.35 | Val Loss: 1.681
Layer 7 | Sparse Ratio: 0.3 | Val Loss: 1.68
Layer 7 | Sparse Ratio: 0.25 | Val Loss: 1.679
Layer 7 best sparse ratio: 0.25 with val loss: 1.679
Layer 7 final test loss: 1.673
Loading pretrained SAE: gpt2-small-mlp-out-v5-32k/blocks.8.hook_mlp_out
Before pruning layer 8 - Val loss: 1.681, Test loss: 1.676
Layer 8 | Sparse Ratio: 0.99 | Val Loss: 1.699
Layer 8 | Sparse Ratio: 0.95 | Val Loss: 1.693
Layer 8 | Sparse Ratio: 0.85 | Val Loss: 1.689
Layer 8 | Sparse Ratio: 0.8 | Val Loss: 1.689
Layer 8 | Sparse Ratio: 0.75 | Val Loss: 1.687
Layer 8 | Sparse Ratio: 0.7 | Val Loss: 1.688
Layer 8 | Sparse Ratio: 0.65 | Val Loss: 1.689
Layer 8 | Sparse Ratio: 0.6 | Val Loss: 1.686
Layer 8 | Sparse Ratio: 0.55 | Val Loss: 1.687
Layer 8 | Sparse Ratio: 0.5 | Val Loss: 1.686
Layer 8 | Sparse Ratio: 0.45 | Val Loss: 1.686
Layer 8 | Sparse Ratio: 0.4 | Val Loss: 1.688
Layer 8 | Sparse Ratio: 0.35 | Val Loss: 1.688
Layer 8 | Sparse Ratio: 0.3 | Val Loss: 1.686
Layer 8 | Sparse Ratio: 0.25 | Val Loss: 1.685
Layer 8 best sparse ratio: 0.25 with val loss: 1.685
Layer 8 final test loss: 1.68
Loading pretrained SAE: gpt2-small-mlp-out-v5-32k/blocks.9.hook_mlp_out
Before pruning layer 9 - Val loss: 1.681, Test loss: 1.674
Layer 9 | Sparse Ratio: 0.99 | Val Loss: 1.714
Layer 9 | Sparse Ratio: 0.95 | Val Loss: 1.709
Layer 9 | Sparse Ratio: 0.85 | Val Loss: 1.703
Layer 9 | Sparse Ratio: 0.8 | Val Loss: 1.698
Layer 9 | Sparse Ratio: 0.75 | Val Loss: 1.697
Layer 9 | Sparse Ratio: 0.7 | Val Loss: 1.693
Layer 9 | Sparse Ratio: 0.65 | Val Loss: 1.693
Layer 9 | Sparse Ratio: 0.6 | Val Loss: 1.689
Layer 9 | Sparse Ratio: 0.55 | Val Loss: 1.689
Layer 9 | Sparse Ratio: 0.5 | Val Loss: 1.688
Layer 9 | Sparse Ratio: 0.45 | Val Loss: 1.687
Layer 9 | Sparse Ratio: 0.4 | Val Loss: 1.686
Layer 9 | Sparse Ratio: 0.35 | Val Loss: 1.687
Layer 9 | Sparse Ratio: 0.3 | Val Loss: 1.686
Layer 9 | Sparse Ratio: 0.25 | Val Loss: 1.684
Layer 9 best sparse ratio: 0.25 with val loss: 1.684
Layer 9 final test loss: 1.675
Loading pretrained SAE: gpt2-small-mlp-out-v5-32k/blocks.10.hook_mlp_out
Before pruning layer 10 - Val loss: 1.68, Test loss: 1.675
Layer 10 | Sparse Ratio: 0.99 | Val Loss: 1.681
Layer 10 | Sparse Ratio: 0.95 | Val Loss: 1.689
Layer 10 | Sparse Ratio: 0.85 | Val Loss: 1.695
Layer 10 | Sparse Ratio: 0.8 | Val Loss: 1.695
Layer 10 | Sparse Ratio: 0.75 | Val Loss: 1.696
Layer 10 | Sparse Ratio: 0.7 | Val Loss: 1.698
Layer 10 | Sparse Ratio: 0.65 | Val Loss: 1.697
Layer 10 | Sparse Ratio: 0.6 | Val Loss: 1.693
Layer 10 | Sparse Ratio: 0.55 | Val Loss: 1.689
Layer 10 | Sparse Ratio: 0.5 | Val Loss: 1.688
Layer 10 | Sparse Ratio: 0.45 | Val Loss: 1.685
Layer 10 | Sparse Ratio: 0.4 | Val Loss: 1.682
Layer 10 | Sparse Ratio: 0.35 | Val Loss: 1.682
Layer 10 | Sparse Ratio: 0.3 | Val Loss: 1.682
Layer 10 | Sparse Ratio: 0.25 | Val Loss: 1.681
Layer 10 best sparse ratio: 0.99 with val loss: 1.681
Layer 10 final test loss: 1.662
Loading pretrained SAE: gpt2-small-mlp-out-v5-32k/blocks.11.hook_mlp_out
Before pruning layer 11 - Val loss: 1.688, Test loss: 1.681
Layer 11 | Sparse Ratio: 0.99 | Val Loss: 1.76
Layer 11 | Sparse Ratio: 0.95 | Val Loss: 1.796
Layer 11 | Sparse Ratio: 0.85 | Val Loss: 1.729
Layer 11 | Sparse Ratio: 0.8 | Val Loss: 1.712
Layer 11 | Sparse Ratio: 0.75 | Val Loss: 1.707
Layer 11 | Sparse Ratio: 0.7 | Val Loss: 1.7
Layer 11 | Sparse Ratio: 0.65 | Val Loss: 1.698
Layer 11 | Sparse Ratio: 0.6 | Val Loss: 1.694
Layer 11 | Sparse Ratio: 0.55 | Val Loss: 1.693
Layer 11 | Sparse Ratio: 0.5 | Val Loss: 1.698
Layer 11 | Sparse Ratio: 0.45 | Val Loss: 1.697
Layer 11 | Sparse Ratio: 0.4 | Val Loss: 1.691
Layer 11 | Sparse Ratio: 0.35 | Val Loss: 1.687
Layer 11 | Sparse Ratio: 0.3 | Val Loss: 1.688
Layer 11 | Sparse Ratio: 0.25 | Val Loss: 1.691
Layer 11 best sparse ratio: 0.35 with val loss: 1.687
Layer 11 final test loss: 1.677

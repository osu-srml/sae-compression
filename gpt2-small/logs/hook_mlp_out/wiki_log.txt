Using device: cuda:1
Model loaded
Loading pretrained SAE: gpt2-small-mlp-out-v5-32k/blocks.0.hook_mlp_out
Before pruning layer 0 - Val loss: 3.496, Test loss: 3.515
Layer 0 | Sparse Ratio: 0.99 | Val Loss: 10.468
Layer 0 | Sparse Ratio: 0.95 | Val Loss: 9.821
Layer 0 | Sparse Ratio: 0.85 | Val Loss: 6.536
Layer 0 | Sparse Ratio: 0.8 | Val Loss: 6.198
Layer 0 | Sparse Ratio: 0.75 | Val Loss: 7.385
Layer 0 | Sparse Ratio: 0.7 | Val Loss: 5.403
Layer 0 | Sparse Ratio: 0.65 | Val Loss: 5.033
Layer 0 | Sparse Ratio: 0.6 | Val Loss: 4.761
Layer 0 | Sparse Ratio: 0.55 | Val Loss: 4.588
Layer 0 | Sparse Ratio: 0.5 | Val Loss: 4.313
Layer 0 | Sparse Ratio: 0.45 | Val Loss: 4.267
Layer 0 | Sparse Ratio: 0.4 | Val Loss: 4.136
Layer 0 | Sparse Ratio: 0.35 | Val Loss: 4.108
Layer 0 | Sparse Ratio: 0.3 | Val Loss: 4.113
Layer 0 | Sparse Ratio: 0.25 | Val Loss: 3.96
Layer 0 best sparse ratio: 0.25 with val loss: 3.96
Layer 0 final test loss: 3.977
Loading pretrained SAE: gpt2-small-mlp-out-v5-32k/blocks.1.hook_mlp_out
Before pruning layer 1 - Val loss: 3.284, Test loss: 3.306
Layer 1 | Sparse Ratio: 0.99 | Val Loss: 3.556
Layer 1 | Sparse Ratio: 0.95 | Val Loss: 3.502
Layer 1 | Sparse Ratio: 0.85 | Val Loss: 3.583
Layer 1 | Sparse Ratio: 0.8 | Val Loss: 3.521
Layer 1 | Sparse Ratio: 0.75 | Val Loss: 3.525
Layer 1 | Sparse Ratio: 0.7 | Val Loss: 3.522
Layer 1 | Sparse Ratio: 0.65 | Val Loss: 3.464
Layer 1 | Sparse Ratio: 0.6 | Val Loss: 3.416
Layer 1 | Sparse Ratio: 0.55 | Val Loss: 3.461
Layer 1 | Sparse Ratio: 0.5 | Val Loss: 3.449
Layer 1 | Sparse Ratio: 0.45 | Val Loss: 3.415
Layer 1 | Sparse Ratio: 0.4 | Val Loss: 3.427
Layer 1 | Sparse Ratio: 0.35 | Val Loss: 3.44
Layer 1 | Sparse Ratio: 0.3 | Val Loss: 3.432
Layer 1 | Sparse Ratio: 0.25 | Val Loss: 3.385
Layer 1 best sparse ratio: 0.25 with val loss: 3.385
Layer 1 final test loss: 3.405
Loading pretrained SAE: gpt2-small-mlp-out-v5-32k/blocks.2.hook_mlp_out
Before pruning layer 2 - Val loss: 3.286, Test loss: 3.305
Layer 2 | Sparse Ratio: 0.99 | Val Loss: 3.722
Layer 2 | Sparse Ratio: 0.95 | Val Loss: 3.79
Layer 2 | Sparse Ratio: 0.85 | Val Loss: 3.874
Layer 2 | Sparse Ratio: 0.8 | Val Loss: 3.834
Layer 2 | Sparse Ratio: 0.75 | Val Loss: 3.667
Layer 2 | Sparse Ratio: 0.7 | Val Loss: 3.604
Layer 2 | Sparse Ratio: 0.65 | Val Loss: 3.616
Layer 2 | Sparse Ratio: 0.6 | Val Loss: 3.553
Layer 2 | Sparse Ratio: 0.55 | Val Loss: 3.527
Layer 2 | Sparse Ratio: 0.5 | Val Loss: 3.466
Layer 2 | Sparse Ratio: 0.45 | Val Loss: 3.444
Layer 2 | Sparse Ratio: 0.4 | Val Loss: 3.404
Layer 2 | Sparse Ratio: 0.35 | Val Loss: 3.37
Layer 2 | Sparse Ratio: 0.3 | Val Loss: 3.338
Layer 2 | Sparse Ratio: 0.25 | Val Loss: 3.311
Layer 2 best sparse ratio: 0.25 with val loss: 3.311
Layer 2 final test loss: 3.328
Loading pretrained SAE: gpt2-small-mlp-out-v5-32k/blocks.3.hook_mlp_out
Before pruning layer 3 - Val loss: 3.266, Test loss: 3.287
Layer 3 | Sparse Ratio: 0.99 | Val Loss: 3.393
Layer 3 | Sparse Ratio: 0.95 | Val Loss: 3.389
Layer 3 | Sparse Ratio: 0.85 | Val Loss: 3.355
Layer 3 | Sparse Ratio: 0.8 | Val Loss: 3.343
Layer 3 | Sparse Ratio: 0.75 | Val Loss: 3.327
Layer 3 | Sparse Ratio: 0.7 | Val Loss: 3.316
Layer 3 | Sparse Ratio: 0.65 | Val Loss: 3.304
Layer 3 | Sparse Ratio: 0.6 | Val Loss: 3.299
Layer 3 | Sparse Ratio: 0.55 | Val Loss: 3.288
Layer 3 | Sparse Ratio: 0.5 | Val Loss: 3.284
Layer 3 | Sparse Ratio: 0.45 | Val Loss: 3.277
Layer 3 | Sparse Ratio: 0.4 | Val Loss: 3.276
Layer 3 | Sparse Ratio: 0.35 | Val Loss: 3.269
Layer 3 | Sparse Ratio: 0.3 | Val Loss: 3.268
Layer 3 | Sparse Ratio: 0.25 | Val Loss: 3.268
Layer 3 best sparse ratio: 0.3 with val loss: 3.268
Layer 3 final test loss: 3.288
Loading pretrained SAE: gpt2-small-mlp-out-v5-32k/blocks.4.hook_mlp_out
Before pruning layer 4 - Val loss: 3.299, Test loss: 3.323
Layer 4 | Sparse Ratio: 0.99 | Val Loss: 3.411
Layer 4 | Sparse Ratio: 0.95 | Val Loss: 3.402
Layer 4 | Sparse Ratio: 0.85 | Val Loss: 3.385
Layer 4 | Sparse Ratio: 0.8 | Val Loss: 3.383
Layer 4 | Sparse Ratio: 0.75 | Val Loss: 3.374
Layer 4 | Sparse Ratio: 0.7 | Val Loss: 3.363
Layer 4 | Sparse Ratio: 0.65 | Val Loss: 3.358
Layer 4 | Sparse Ratio: 0.6 | Val Loss: 3.347
Layer 4 | Sparse Ratio: 0.55 | Val Loss: 3.338
Layer 4 | Sparse Ratio: 0.5 | Val Loss: 3.331
Layer 4 | Sparse Ratio: 0.45 | Val Loss: 3.328
Layer 4 | Sparse Ratio: 0.4 | Val Loss: 3.32
Layer 4 | Sparse Ratio: 0.35 | Val Loss: 3.317
Layer 4 | Sparse Ratio: 0.3 | Val Loss: 3.31
Layer 4 | Sparse Ratio: 0.25 | Val Loss: 3.304
Layer 4 best sparse ratio: 0.25 with val loss: 3.304
Layer 4 final test loss: 3.328
Loading pretrained SAE: gpt2-small-mlp-out-v5-32k/blocks.5.hook_mlp_out
Before pruning layer 5 - Val loss: 3.271, Test loss: 3.293
Layer 5 | Sparse Ratio: 0.99 | Val Loss: 3.366
Layer 5 | Sparse Ratio: 0.95 | Val Loss: 3.359
Layer 5 | Sparse Ratio: 0.85 | Val Loss: 3.352
Layer 5 | Sparse Ratio: 0.8 | Val Loss: 3.345
Layer 5 | Sparse Ratio: 0.75 | Val Loss: 3.339
Layer 5 | Sparse Ratio: 0.7 | Val Loss: 3.323
Layer 5 | Sparse Ratio: 0.65 | Val Loss: 3.315
Layer 5 | Sparse Ratio: 0.6 | Val Loss: 3.307
Layer 5 | Sparse Ratio: 0.55 | Val Loss: 3.3
Layer 5 | Sparse Ratio: 0.5 | Val Loss: 3.293
Layer 5 | Sparse Ratio: 0.45 | Val Loss: 3.288
Layer 5 | Sparse Ratio: 0.4 | Val Loss: 3.284
Layer 5 | Sparse Ratio: 0.35 | Val Loss: 3.279
Layer 5 | Sparse Ratio: 0.3 | Val Loss: 3.274
Layer 5 | Sparse Ratio: 0.25 | Val Loss: 3.273
Layer 5 best sparse ratio: 0.25 with val loss: 3.273
Layer 5 final test loss: 3.295
Loading pretrained SAE: gpt2-small-mlp-out-v5-32k/blocks.6.hook_mlp_out
Before pruning layer 6 - Val loss: 3.259, Test loss: 3.28
Layer 6 | Sparse Ratio: 0.99 | Val Loss: 3.307
Layer 6 | Sparse Ratio: 0.95 | Val Loss: 3.3
Layer 6 | Sparse Ratio: 0.85 | Val Loss: 3.291
Layer 6 | Sparse Ratio: 0.8 | Val Loss: 3.287
Layer 6 | Sparse Ratio: 0.75 | Val Loss: 3.285
Layer 6 | Sparse Ratio: 0.7 | Val Loss: 3.278
Layer 6 | Sparse Ratio: 0.65 | Val Loss: 3.274
Layer 6 | Sparse Ratio: 0.6 | Val Loss: 3.271
Layer 6 | Sparse Ratio: 0.55 | Val Loss: 3.269
Layer 6 | Sparse Ratio: 0.5 | Val Loss: 3.267
Layer 6 | Sparse Ratio: 0.45 | Val Loss: 3.264
Layer 6 | Sparse Ratio: 0.4 | Val Loss: 3.262
Layer 6 | Sparse Ratio: 0.35 | Val Loss: 3.262
Layer 6 | Sparse Ratio: 0.3 | Val Loss: 3.26
Layer 6 | Sparse Ratio: 0.25 | Val Loss: 3.261
Layer 6 best sparse ratio: 0.3 with val loss: 3.26
Layer 6 final test loss: 3.279
Loading pretrained SAE: gpt2-small-mlp-out-v5-32k/blocks.7.hook_mlp_out
Before pruning layer 7 - Val loss: 3.263, Test loss: 3.282
Layer 7 | Sparse Ratio: 0.99 | Val Loss: 3.338
Layer 7 | Sparse Ratio: 0.95 | Val Loss: 3.33
Layer 7 | Sparse Ratio: 0.85 | Val Loss: 3.312
Layer 7 | Sparse Ratio: 0.8 | Val Loss: 3.306
Layer 7 | Sparse Ratio: 0.75 | Val Loss: 3.301
Layer 7 | Sparse Ratio: 0.7 | Val Loss: 3.294
Layer 7 | Sparse Ratio: 0.65 | Val Loss: 3.289
Layer 7 | Sparse Ratio: 0.6 | Val Loss: 3.283
Layer 7 | Sparse Ratio: 0.55 | Val Loss: 3.278
Layer 7 | Sparse Ratio: 0.5 | Val Loss: 3.274
Layer 7 | Sparse Ratio: 0.45 | Val Loss: 3.269
Layer 7 | Sparse Ratio: 0.4 | Val Loss: 3.266
Layer 7 | Sparse Ratio: 0.35 | Val Loss: 3.262
Layer 7 | Sparse Ratio: 0.3 | Val Loss: 3.261
Layer 7 | Sparse Ratio: 0.25 | Val Loss: 3.26
Layer 7 best sparse ratio: 0.25 with val loss: 3.26
Layer 7 final test loss: 3.279
Loading pretrained SAE: gpt2-small-mlp-out-v5-32k/blocks.8.hook_mlp_out
Before pruning layer 8 - Val loss: 3.264, Test loss: 3.282
Layer 8 | Sparse Ratio: 0.99 | Val Loss: 3.33
Layer 8 | Sparse Ratio: 0.95 | Val Loss: 3.319
Layer 8 | Sparse Ratio: 0.85 | Val Loss: 3.307
Layer 8 | Sparse Ratio: 0.8 | Val Loss: 3.304
Layer 8 | Sparse Ratio: 0.75 | Val Loss: 3.3
Layer 8 | Sparse Ratio: 0.7 | Val Loss: 3.296
Layer 8 | Sparse Ratio: 0.65 | Val Loss: 3.294
Layer 8 | Sparse Ratio: 0.6 | Val Loss: 3.289
Layer 8 | Sparse Ratio: 0.55 | Val Loss: 3.285
Layer 8 | Sparse Ratio: 0.5 | Val Loss: 3.28
Layer 8 | Sparse Ratio: 0.45 | Val Loss: 3.277
Layer 8 | Sparse Ratio: 0.4 | Val Loss: 3.272
Layer 8 | Sparse Ratio: 0.35 | Val Loss: 3.27
Layer 8 | Sparse Ratio: 0.3 | Val Loss: 3.267
Layer 8 | Sparse Ratio: 0.25 | Val Loss: 3.266
Layer 8 best sparse ratio: 0.25 with val loss: 3.266
Layer 8 final test loss: 3.284
Loading pretrained SAE: gpt2-small-mlp-out-v5-32k/blocks.9.hook_mlp_out
Before pruning layer 9 - Val loss: 3.262, Test loss: 3.282
Layer 9 | Sparse Ratio: 0.99 | Val Loss: 3.343
Layer 9 | Sparse Ratio: 0.95 | Val Loss: 3.332
Layer 9 | Sparse Ratio: 0.85 | Val Loss: 3.317
Layer 9 | Sparse Ratio: 0.8 | Val Loss: 3.311
Layer 9 | Sparse Ratio: 0.75 | Val Loss: 3.305
Layer 9 | Sparse Ratio: 0.7 | Val Loss: 3.3
Layer 9 | Sparse Ratio: 0.65 | Val Loss: 3.296
Layer 9 | Sparse Ratio: 0.6 | Val Loss: 3.291
Layer 9 | Sparse Ratio: 0.55 | Val Loss: 3.285
Layer 9 | Sparse Ratio: 0.5 | Val Loss: 3.28
Layer 9 | Sparse Ratio: 0.45 | Val Loss: 3.275
Layer 9 | Sparse Ratio: 0.4 | Val Loss: 3.272
Layer 9 | Sparse Ratio: 0.35 | Val Loss: 3.269
Layer 9 | Sparse Ratio: 0.3 | Val Loss: 3.268
Layer 9 | Sparse Ratio: 0.25 | Val Loss: 3.267
Layer 9 best sparse ratio: 0.25 with val loss: 3.267
Layer 9 final test loss: 3.287
Loading pretrained SAE: gpt2-small-mlp-out-v5-32k/blocks.10.hook_mlp_out
Before pruning layer 10 - Val loss: 3.269, Test loss: 3.289
Layer 10 | Sparse Ratio: 0.99 | Val Loss: 3.327
Layer 10 | Sparse Ratio: 0.95 | Val Loss: 3.332
Layer 10 | Sparse Ratio: 0.85 | Val Loss: 3.33
Layer 10 | Sparse Ratio: 0.8 | Val Loss: 3.331
Layer 10 | Sparse Ratio: 0.75 | Val Loss: 3.329
Layer 10 | Sparse Ratio: 0.7 | Val Loss: 3.323
Layer 10 | Sparse Ratio: 0.65 | Val Loss: 3.319
Layer 10 | Sparse Ratio: 0.6 | Val Loss: 3.315
Layer 10 | Sparse Ratio: 0.55 | Val Loss: 3.31
Layer 10 | Sparse Ratio: 0.5 | Val Loss: 3.306
Layer 10 | Sparse Ratio: 0.45 | Val Loss: 3.299
Layer 10 | Sparse Ratio: 0.4 | Val Loss: 3.289
Layer 10 | Sparse Ratio: 0.35 | Val Loss: 3.285
Layer 10 | Sparse Ratio: 0.3 | Val Loss: 3.282
Layer 10 | Sparse Ratio: 0.25 | Val Loss: 3.278
Layer 10 best sparse ratio: 0.25 with val loss: 3.278
Layer 10 final test loss: 3.299
Loading pretrained SAE: gpt2-small-mlp-out-v5-32k/blocks.11.hook_mlp_out
Before pruning layer 11 - Val loss: 3.282, Test loss: 3.302
Layer 11 | Sparse Ratio: 0.99 | Val Loss: 3.378
Layer 11 | Sparse Ratio: 0.95 | Val Loss: 3.386
Layer 11 | Sparse Ratio: 0.85 | Val Loss: 3.359
Layer 11 | Sparse Ratio: 0.8 | Val Loss: 3.351
Layer 11 | Sparse Ratio: 0.75 | Val Loss: 3.336
Layer 11 | Sparse Ratio: 0.7 | Val Loss: 3.337
Layer 11 | Sparse Ratio: 0.65 | Val Loss: 3.319
Layer 11 | Sparse Ratio: 0.6 | Val Loss: 3.311
Layer 11 | Sparse Ratio: 0.55 | Val Loss: 3.308
Layer 11 | Sparse Ratio: 0.5 | Val Loss: 3.309
Layer 11 | Sparse Ratio: 0.45 | Val Loss: 3.303
Layer 11 | Sparse Ratio: 0.4 | Val Loss: 3.296
Layer 11 | Sparse Ratio: 0.35 | Val Loss: 3.29
Layer 11 | Sparse Ratio: 0.3 | Val Loss: 3.289
Layer 11 | Sparse Ratio: 0.25 | Val Loss: 3.29
Layer 11 best sparse ratio: 0.3 with val loss: 3.289
Layer 11 final test loss: 3.309

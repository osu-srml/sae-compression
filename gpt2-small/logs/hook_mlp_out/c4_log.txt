Using device: cuda:2
Model loaded
Loading pretrained SAE: gpt2-small-mlp-out-v5-32k/blocks.0.hook_mlp_out
Before pruning layer 0 - Val loss: 1.669, Test loss: 1.649
Layer 0 | Sparse Ratio: 0.99 | Val Loss: 7.394
Layer 0 | Sparse Ratio: 0.95 | Val Loss: 5.902
Layer 0 | Sparse Ratio: 0.85 | Val Loss: 4.424
Layer 0 | Sparse Ratio: 0.8 | Val Loss: 2.822
Layer 0 | Sparse Ratio: 0.75 | Val Loss: 2.242
Layer 0 | Sparse Ratio: 0.7 | Val Loss: 2.009
Layer 0 | Sparse Ratio: 0.65 | Val Loss: 1.927
Layer 0 | Sparse Ratio: 0.6 | Val Loss: 1.863
Layer 0 | Sparse Ratio: 0.55 | Val Loss: 1.852
Layer 0 | Sparse Ratio: 0.5 | Val Loss: 1.821
Layer 0 | Sparse Ratio: 0.45 | Val Loss: 1.785
Layer 0 | Sparse Ratio: 0.4 | Val Loss: 1.783
Layer 0 | Sparse Ratio: 0.35 | Val Loss: 1.785
Layer 0 | Sparse Ratio: 0.3 | Val Loss: 1.762
Layer 0 | Sparse Ratio: 0.25 | Val Loss: 1.737
Layer 0 best sparse ratio: 0.25 with val loss: 1.737
Layer 0 final test loss: 1.724
Loading pretrained SAE: gpt2-small-mlp-out-v5-32k/blocks.1.hook_mlp_out
Before pruning layer 1 - Val loss: 1.66, Test loss: 1.64
Layer 1 | Sparse Ratio: 0.99 | Val Loss: 1.676
Layer 1 | Sparse Ratio: 0.95 | Val Loss: 1.699
Layer 1 | Sparse Ratio: 0.85 | Val Loss: 1.719
Layer 1 | Sparse Ratio: 0.8 | Val Loss: 1.719
Layer 1 | Sparse Ratio: 0.75 | Val Loss: 1.736
Layer 1 | Sparse Ratio: 0.7 | Val Loss: 1.707
Layer 1 | Sparse Ratio: 0.65 | Val Loss: 1.717
Layer 1 | Sparse Ratio: 0.6 | Val Loss: 1.714
Layer 1 | Sparse Ratio: 0.55 | Val Loss: 1.742
Layer 1 | Sparse Ratio: 0.5 | Val Loss: 1.734
Layer 1 | Sparse Ratio: 0.45 | Val Loss: 1.726
Layer 1 | Sparse Ratio: 0.4 | Val Loss: 1.716
Layer 1 | Sparse Ratio: 0.35 | Val Loss: 1.705
Layer 1 | Sparse Ratio: 0.3 | Val Loss: 1.707
Layer 1 | Sparse Ratio: 0.25 | Val Loss: 1.713
Layer 1 best sparse ratio: 0.99 with val loss: 1.676
Layer 1 final test loss: 1.658
Loading pretrained SAE: gpt2-small-mlp-out-v5-32k/blocks.2.hook_mlp_out
Before pruning layer 2 - Val loss: 1.657, Test loss: 1.637
Layer 2 | Sparse Ratio: 0.99 | Val Loss: 1.79
Layer 2 | Sparse Ratio: 0.95 | Val Loss: 1.854
Layer 2 | Sparse Ratio: 0.85 | Val Loss: 1.844
Layer 2 | Sparse Ratio: 0.8 | Val Loss: 1.837
Layer 2 | Sparse Ratio: 0.75 | Val Loss: 1.847
Layer 2 | Sparse Ratio: 0.7 | Val Loss: 1.834
Layer 2 | Sparse Ratio: 0.65 | Val Loss: 1.758
Layer 2 | Sparse Ratio: 0.6 | Val Loss: 1.749
Layer 2 | Sparse Ratio: 0.55 | Val Loss: 1.754
Layer 2 | Sparse Ratio: 0.5 | Val Loss: 1.721
Layer 2 | Sparse Ratio: 0.45 | Val Loss: 1.7
Layer 2 | Sparse Ratio: 0.4 | Val Loss: 1.708
Layer 2 | Sparse Ratio: 0.35 | Val Loss: 1.714
Layer 2 | Sparse Ratio: 0.3 | Val Loss: 1.689
Layer 2 | Sparse Ratio: 0.25 | Val Loss: 1.676
Layer 2 best sparse ratio: 0.25 with val loss: 1.676
Layer 2 final test loss: 1.658
Loading pretrained SAE: gpt2-small-mlp-out-v5-32k/blocks.3.hook_mlp_out
Before pruning layer 3 - Val loss: 1.66, Test loss: 1.641
Layer 3 | Sparse Ratio: 0.99 | Val Loss: 1.689
Layer 3 | Sparse Ratio: 0.95 | Val Loss: 1.681
Layer 3 | Sparse Ratio: 0.85 | Val Loss: 1.675
Layer 3 | Sparse Ratio: 0.8 | Val Loss: 1.676
Layer 3 | Sparse Ratio: 0.75 | Val Loss: 1.668
Layer 3 | Sparse Ratio: 0.7 | Val Loss: 1.663
Layer 3 | Sparse Ratio: 0.65 | Val Loss: 1.661
Layer 3 | Sparse Ratio: 0.6 | Val Loss: 1.661
Layer 3 | Sparse Ratio: 0.55 | Val Loss: 1.659
Layer 3 | Sparse Ratio: 0.5 | Val Loss: 1.659
Layer 3 | Sparse Ratio: 0.45 | Val Loss: 1.659
Layer 3 | Sparse Ratio: 0.4 | Val Loss: 1.657
Layer 3 | Sparse Ratio: 0.35 | Val Loss: 1.657
Layer 3 | Sparse Ratio: 0.3 | Val Loss: 1.658
Layer 3 | Sparse Ratio: 0.25 | Val Loss: 1.661
Layer 3 best sparse ratio: 0.4 with val loss: 1.657
Layer 3 final test loss: 1.638
Loading pretrained SAE: gpt2-small-mlp-out-v5-32k/blocks.4.hook_mlp_out
Before pruning layer 4 - Val loss: 1.666, Test loss: 1.645
Layer 4 | Sparse Ratio: 0.99 | Val Loss: 1.724
Layer 4 | Sparse Ratio: 0.95 | Val Loss: 1.712
Layer 4 | Sparse Ratio: 0.85 | Val Loss: 1.701
Layer 4 | Sparse Ratio: 0.8 | Val Loss: 1.704
Layer 4 | Sparse Ratio: 0.75 | Val Loss: 1.698
Layer 4 | Sparse Ratio: 0.7 | Val Loss: 1.691
Layer 4 | Sparse Ratio: 0.65 | Val Loss: 1.686
Layer 4 | Sparse Ratio: 0.6 | Val Loss: 1.679
Layer 4 | Sparse Ratio: 0.55 | Val Loss: 1.678
Layer 4 | Sparse Ratio: 0.5 | Val Loss: 1.673
Layer 4 | Sparse Ratio: 0.45 | Val Loss: 1.67
Layer 4 | Sparse Ratio: 0.4 | Val Loss: 1.664
Layer 4 | Sparse Ratio: 0.35 | Val Loss: 1.665
Layer 4 | Sparse Ratio: 0.3 | Val Loss: 1.668
Layer 4 | Sparse Ratio: 0.25 | Val Loss: 1.666
Layer 4 best sparse ratio: 0.4 with val loss: 1.664
Layer 4 final test loss: 1.644
Loading pretrained SAE: gpt2-small-mlp-out-v5-32k/blocks.5.hook_mlp_out
Before pruning layer 5 - Val loss: 1.666, Test loss: 1.646
Layer 5 | Sparse Ratio: 0.99 | Val Loss: 1.71
Layer 5 | Sparse Ratio: 0.95 | Val Loss: 1.701
Layer 5 | Sparse Ratio: 0.85 | Val Loss: 1.691
Layer 5 | Sparse Ratio: 0.8 | Val Loss: 1.682
Layer 5 | Sparse Ratio: 0.75 | Val Loss: 1.68
Layer 5 | Sparse Ratio: 0.7 | Val Loss: 1.675
Layer 5 | Sparse Ratio: 0.65 | Val Loss: 1.671
Layer 5 | Sparse Ratio: 0.6 | Val Loss: 1.669
Layer 5 | Sparse Ratio: 0.55 | Val Loss: 1.669
Layer 5 | Sparse Ratio: 0.5 | Val Loss: 1.671
Layer 5 | Sparse Ratio: 0.45 | Val Loss: 1.667
Layer 5 | Sparse Ratio: 0.4 | Val Loss: 1.666
Layer 5 | Sparse Ratio: 0.35 | Val Loss: 1.666
Layer 5 | Sparse Ratio: 0.3 | Val Loss: 1.668
Layer 5 | Sparse Ratio: 0.25 | Val Loss: 1.666
Layer 5 best sparse ratio: 0.4 with val loss: 1.666
Layer 5 final test loss: 1.645
Loading pretrained SAE: gpt2-small-mlp-out-v5-32k/blocks.6.hook_mlp_out
Before pruning layer 6 - Val loss: 1.66, Test loss: 1.64
Layer 6 | Sparse Ratio: 0.99 | Val Loss: 1.688
Layer 6 | Sparse Ratio: 0.95 | Val Loss: 1.682
Layer 6 | Sparse Ratio: 0.85 | Val Loss: 1.675
Layer 6 | Sparse Ratio: 0.8 | Val Loss: 1.675
Layer 6 | Sparse Ratio: 0.75 | Val Loss: 1.672
Layer 6 | Sparse Ratio: 0.7 | Val Loss: 1.671
Layer 6 | Sparse Ratio: 0.65 | Val Loss: 1.67
Layer 6 | Sparse Ratio: 0.6 | Val Loss: 1.671
Layer 6 | Sparse Ratio: 0.55 | Val Loss: 1.67
Layer 6 | Sparse Ratio: 0.5 | Val Loss: 1.669
Layer 6 | Sparse Ratio: 0.45 | Val Loss: 1.67
Layer 6 | Sparse Ratio: 0.4 | Val Loss: 1.671
Layer 6 | Sparse Ratio: 0.35 | Val Loss: 1.669
Layer 6 | Sparse Ratio: 0.3 | Val Loss: 1.666
Layer 6 | Sparse Ratio: 0.25 | Val Loss: 1.665
Layer 6 best sparse ratio: 0.25 with val loss: 1.665
Layer 6 final test loss: 1.645
Loading pretrained SAE: gpt2-small-mlp-out-v5-32k/blocks.7.hook_mlp_out
Before pruning layer 7 - Val loss: 1.662, Test loss: 1.643
Layer 7 | Sparse Ratio: 0.99 | Val Loss: 1.719
Layer 7 | Sparse Ratio: 0.95 | Val Loss: 1.707
Layer 7 | Sparse Ratio: 0.85 | Val Loss: 1.695
Layer 7 | Sparse Ratio: 0.8 | Val Loss: 1.695
Layer 7 | Sparse Ratio: 0.75 | Val Loss: 1.689
Layer 7 | Sparse Ratio: 0.7 | Val Loss: 1.688
Layer 7 | Sparse Ratio: 0.65 | Val Loss: 1.683
Layer 7 | Sparse Ratio: 0.6 | Val Loss: 1.678
Layer 7 | Sparse Ratio: 0.55 | Val Loss: 1.673
Layer 7 | Sparse Ratio: 0.5 | Val Loss: 1.668
Layer 7 | Sparse Ratio: 0.45 | Val Loss: 1.666
Layer 7 | Sparse Ratio: 0.4 | Val Loss: 1.662
Layer 7 | Sparse Ratio: 0.35 | Val Loss: 1.662
Layer 7 | Sparse Ratio: 0.3 | Val Loss: 1.661
Layer 7 | Sparse Ratio: 0.25 | Val Loss: 1.662
Layer 7 best sparse ratio: 0.3 with val loss: 1.661
Layer 7 final test loss: 1.642
Loading pretrained SAE: gpt2-small-mlp-out-v5-32k/blocks.8.hook_mlp_out
Before pruning layer 8 - Val loss: 1.661, Test loss: 1.641
Layer 8 | Sparse Ratio: 0.99 | Val Loss: 1.702
Layer 8 | Sparse Ratio: 0.95 | Val Loss: 1.693
Layer 8 | Sparse Ratio: 0.85 | Val Loss: 1.681
Layer 8 | Sparse Ratio: 0.8 | Val Loss: 1.68
Layer 8 | Sparse Ratio: 0.75 | Val Loss: 1.677
Layer 8 | Sparse Ratio: 0.7 | Val Loss: 1.673
Layer 8 | Sparse Ratio: 0.65 | Val Loss: 1.674
Layer 8 | Sparse Ratio: 0.6 | Val Loss: 1.671
Layer 8 | Sparse Ratio: 0.55 | Val Loss: 1.668
Layer 8 | Sparse Ratio: 0.5 | Val Loss: 1.669
Layer 8 | Sparse Ratio: 0.45 | Val Loss: 1.669
Layer 8 | Sparse Ratio: 0.4 | Val Loss: 1.667
Layer 8 | Sparse Ratio: 0.35 | Val Loss: 1.667
Layer 8 | Sparse Ratio: 0.3 | Val Loss: 1.665
Layer 8 | Sparse Ratio: 0.25 | Val Loss: 1.663
Layer 8 best sparse ratio: 0.25 with val loss: 1.663
Layer 8 final test loss: 1.644
Loading pretrained SAE: gpt2-small-mlp-out-v5-32k/blocks.9.hook_mlp_out
Before pruning layer 9 - Val loss: 1.662, Test loss: 1.643
Layer 9 | Sparse Ratio: 0.99 | Val Loss: 1.705
Layer 9 | Sparse Ratio: 0.95 | Val Loss: 1.698
Layer 9 | Sparse Ratio: 0.85 | Val Loss: 1.691
Layer 9 | Sparse Ratio: 0.8 | Val Loss: 1.687
Layer 9 | Sparse Ratio: 0.75 | Val Loss: 1.684
Layer 9 | Sparse Ratio: 0.7 | Val Loss: 1.68
Layer 9 | Sparse Ratio: 0.65 | Val Loss: 1.677
Layer 9 | Sparse Ratio: 0.6 | Val Loss: 1.672
Layer 9 | Sparse Ratio: 0.55 | Val Loss: 1.672
Layer 9 | Sparse Ratio: 0.5 | Val Loss: 1.67
Layer 9 | Sparse Ratio: 0.45 | Val Loss: 1.668
Layer 9 | Sparse Ratio: 0.4 | Val Loss: 1.667
Layer 9 | Sparse Ratio: 0.35 | Val Loss: 1.669
Layer 9 | Sparse Ratio: 0.3 | Val Loss: 1.668
Layer 9 | Sparse Ratio: 0.25 | Val Loss: 1.666
Layer 9 best sparse ratio: 0.25 with val loss: 1.666
Layer 9 final test loss: 1.646
Loading pretrained SAE: gpt2-small-mlp-out-v5-32k/blocks.10.hook_mlp_out
Before pruning layer 10 - Val loss: 1.659, Test loss: 1.639
Layer 10 | Sparse Ratio: 0.99 | Val Loss: 1.675
Layer 10 | Sparse Ratio: 0.95 | Val Loss: 1.675
Layer 10 | Sparse Ratio: 0.85 | Val Loss: 1.674
Layer 10 | Sparse Ratio: 0.8 | Val Loss: 1.673
Layer 10 | Sparse Ratio: 0.75 | Val Loss: 1.674
Layer 10 | Sparse Ratio: 0.7 | Val Loss: 1.674
Layer 10 | Sparse Ratio: 0.65 | Val Loss: 1.677
Layer 10 | Sparse Ratio: 0.6 | Val Loss: 1.673
Layer 10 | Sparse Ratio: 0.55 | Val Loss: 1.664
Layer 10 | Sparse Ratio: 0.5 | Val Loss: 1.663
Layer 10 | Sparse Ratio: 0.45 | Val Loss: 1.663
Layer 10 | Sparse Ratio: 0.4 | Val Loss: 1.662
Layer 10 | Sparse Ratio: 0.35 | Val Loss: 1.66
Layer 10 | Sparse Ratio: 0.3 | Val Loss: 1.661
Layer 10 | Sparse Ratio: 0.25 | Val Loss: 1.66
Layer 10 best sparse ratio: 0.35 with val loss: 1.66
Layer 10 final test loss: 1.64
Loading pretrained SAE: gpt2-small-mlp-out-v5-32k/blocks.11.hook_mlp_out
Before pruning layer 11 - Val loss: 1.667, Test loss: 1.647
Layer 11 | Sparse Ratio: 0.99 | Val Loss: 1.756
Layer 11 | Sparse Ratio: 0.95 | Val Loss: 1.805
Layer 11 | Sparse Ratio: 0.85 | Val Loss: 1.708
Layer 11 | Sparse Ratio: 0.8 | Val Loss: 1.705
Layer 11 | Sparse Ratio: 0.75 | Val Loss: 1.696
Layer 11 | Sparse Ratio: 0.7 | Val Loss: 1.692
Layer 11 | Sparse Ratio: 0.65 | Val Loss: 1.685
Layer 11 | Sparse Ratio: 0.6 | Val Loss: 1.685
Layer 11 | Sparse Ratio: 0.55 | Val Loss: 1.68
Layer 11 | Sparse Ratio: 0.5 | Val Loss: 1.679
Layer 11 | Sparse Ratio: 0.45 | Val Loss: 1.677
Layer 11 | Sparse Ratio: 0.4 | Val Loss: 1.678
Layer 11 | Sparse Ratio: 0.35 | Val Loss: 1.675
Layer 11 | Sparse Ratio: 0.3 | Val Loss: 1.67
Layer 11 | Sparse Ratio: 0.25 | Val Loss: 1.67
Layer 11 best sparse ratio: 0.3 with val loss: 1.67
Layer 11 final test loss: 1.651
